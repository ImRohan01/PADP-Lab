[ Team LiB ]
Table of Contents •
Introduction to Parallel Computing, Second Edition
r Vipin Kuma , s George Karypi , a Anshul Gupt , a Ananth Gram By
: Addison Wesley Publisher
: January 16, 2003 Pub Date
: 0-201-64865-2 ISBN
: 856 Pages
Increasingly, parallel processing is being seen as the only cost-effective method for the fast solution of computationally large and
data-intensive problems. The emergence of inexpensive parallel computers such as commodity desktop multiprocessors and clusters of
workstations or PCs has made such parallel methods generally applicable, as have software standards for portable parallel
programming. This sets the stage for substantial growth in parallel software.
Data-intensive applications such as transaction processing and information retrieval, data mining and analysis and multimedia services
have provided a new challenge for the modern generation of parallel platforms. Emerging areas such as computational biology and
nanotechnology have implications for algorithms and systems development, while changes in architectures, programming models and
applications have implications for how parallel platforms are made available to users in the form of grid-based services.
This book takes into account these new developments as well as covering the more traditional problems addressed by parallel
computers.Where possible it employs an architecture-independent view of the underlying platforms and designs algorithms for an
abstract model. Message Passing Interface (MPI), POSIX threads and OpenMP have been selected as programming models and the
evolving application mix of parallel computing is reflected in various examples throughout the book.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Table of Contents •
Introduction to Parallel Computing, Second Edition
r Vipin Kuma , s George Karypi , a Anshul Gupt , a Ananth Gram By
: Addison Wesley Publisher
: January 16, 2003 Pub Date
: 0-201-64865-2 ISBN
: 856 Pages
Copyright 
Pearson Education 
Preface 
Acknowledgments 
g Chapter 1. Introduction to Parallel Computin 
m Section 1.1. Motivating Parallelis 
g Section 1.2. Scope of Parallel Computin 
t Section 1.3. Organization and Contents of the Tex 
s Section 1.4. Bibliographic Remark 
Problems 
s Chapter 2. Parallel Programming Platform 
* Section 2.1. Implicit Parallelism: Trends in Microprocessor Architectures 
* Section 2.2. Limitations of Memory System Performance 
s Section 2.3. Dichotomy of Parallel Computing Platform 
s Section 2.4. Physical Organization of Parallel Platform 
s Section 2.5. Communication Costs in Parallel Machine 
s Section 2.6. Routing Mechanisms for Interconnection Network 
s Section 2.7. Impact of Process-Processor Mapping and Mapping Technique 
s Section 2.8. Bibliographic Remark 
Problems 
n Chapter 3. Principles of Parallel Algorithm Desig 
s Section 3.1. Preliminarie 
s Section 3.2. Decomposition Technique 
s Section 3.3. Characteristics of Tasks and Interaction 
g Section 3.4. Mapping Techniques for Load Balancin 
s Section 3.5. Methods for Containing Interaction Overhead 
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
s Section 3.6. Parallel Algorithm Model 
s Section 3.7. Bibliographic Remark 
Problems 
s Chapter 4. Basic Communication Operation 
n Section 4.1. One-to-All Broadcast and All-to-One Reductio 
n Section 4.2. All-to-All Broadcast and Reductio 
s Section 4.3. All-Reduce and Prefix-Sum Operation 
r Section 4.4. Scatter and Gathe 
n Section 4.5. All-to-All Personalized Communicatio 
t Section 4.6. Circular Shif 
s Section 4.7. Improving the Speed of Some Communication Operation 
y Section 4.8. Summar 
s Section 4.9. Bibliographic Remark 
Problems 
s Chapter 5. Analytical Modeling of Parallel Program 
s Section 5.1. Sources of Overhead in Parallel Program 
s Section 5.2. Performance Metrics for Parallel System 
e Section 5.3. The Effect of Granularity on Performanc 
s Section 5.4. Scalability of Parallel System 
e Section 5.5. Minimum Execution Time and Minimum Cost-Optimal Execution Tim 
s Section 5.6. Asymptotic Analysis of Parallel Program 
s Section 5.7. Other Scalability Metric 
s Section 5.8. Bibliographic Remark 
Problems 
m Chapter 6. Programming Using the Message-Passing Paradig 
g Section 6.1. Principles of Message-Passing Programmin 
s Section 6.2. The Building Blocks: Send and Receive Operation 
e Section 6.3. MPI: the Message Passing Interfac 
g Section 6.4. Topologies and Embeddin 
n Section 6.5. Overlapping Communication with Computatio 
s Section 6.6. Collective Communication and Computation Operation 
s Section 6.7. Groups and Communicator 
s Section 6.8. Bibliographic Remark 
Problems 
s Chapter 7. Programming Shared Address Space Platform 
s Section 7.1. Thread Basic 
? Section 7.2. Why Threads 
I Section 7.3. The POSIX Thread AP 
n Section 7.4. Thread Basics: Creation and Terminatio 
s Section 7.5. Synchronization Primitives in Pthread 
s Section 7.6. Controlling Thread and Synchronization Attribute 
n Section 7.7. Thread Cancellatio 
s Section 7.8. Composite Synchronization Construct 
s Section 7.9. Tips for Designing Asynchronous Program 
g Section 7.10. OpenMP: a Standard for Directive Based Parallel Programmin 
s Section 7.11. Bibliographic Remark 
Problems 
s Chapter 8. Dense Matrix Algorithm 
n Section 8.1. Matrix-Vector Multiplicatio 
n Section 8.2. Matrix-Matrix Multiplicatio 
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
s Section 8.3. Solving a System of Linear Equation 
s Section 8.4. Bibliographic Remark 
Problems 
g Chapter 9. Sortin 
s Section 9.1. Issues in Sorting on Parallel Computer 
s Section 9.2. Sorting Network 
s Section 9.3. Bubble Sort and its Variant 
t Section 9.4. Quicksor 
t Section 9.5. Bucket and Sample Sor 
s Section 9.6. Other Sorting Algorithm 
s Section 9.7. Bibliographic Remark 
Problems 
s Chapter 10. Graph Algorithm 
n Section 10.1. Definitions and Representatio 
m Section 10.2. Minimum Spanning Tree: Prim's Algorith 
m Section 10.3. Single-Source Shortest Paths: Dijkstra's Algorith 
s Section 10.4. All-Pairs Shortest Path 
e Section 10.5. Transitive Closur 
s Section 10.6. Connected Component 
s Section 10.7. Algorithms for Sparse Graph 
s Section 10.8. Bibliographic Remark 
Problems 
s Chapter 11. Search Algorithms for Discrete Optimization Problem 
s Section 11.1. Definitions and Example 
s Section 11.2. Sequential Search Algorithm 
r Section 11.3. Search Overhead Facto 
h Section 11.4. Parallel Depth-First Searc 
h Section 11.5. Parallel Best-First Searc 
s Section 11.6. Speedup Anomalies in Parallel Search Algorithm 
s Section 11.7. Bibliographic Remark 
Problems 
g Chapter 12. Dynamic Programmin 
g Section 12.1. Overview of Dynamic Programmin 
s Section 12.2. Serial Monadic DP Formulation 
s Section 12.3. Nonserial Monadic DP Formulation 
s Section 12.4. Serial Polyadic DP Formulation 
s Section 12.5. Nonserial Polyadic DP Formulation 
n Section 12.6. Summary and Discussio 
s Section 12.7. Bibliographic Remark 
Problems 
m Chapter 13. Fast Fourier Transfor 
m Section 13.1. The Serial Algorith 
m Section 13.2. The Binary-Exchange Algorith 
m Section 13.3. The Transpose Algorith 
s Section 13.4. Bibliographic Remark 
Problems 
s Appendix A. Complexity of Functions and Order Analysi 
s Section A.1. Complexity of Function 
s Section A.2. Order Analysis of Function 
Bibliography 
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Copyright
Pearson Education Limited
Edinburgh Gate
Harlow
Essex CM20 2JE
England
and Associated Companies throughout the world
www.pearsoneduc.com Visit us on the World Wide Web at:
First published by The Benjamin/Cummings Publishing Company, Inc. 1994
Second edition published 2003
4 © The Benjamin/Cummings Publishing Company, Inc. 199
3 © Pearson Education Limited 200
d The rights of Ananth Grama, Anshul Gupta, George Karypis and Vipin Kumar to be identified as authors of this work have been asserte
by them in accordance with the Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any
means, electronic, mechanical, photocopying, recording or otherwise, without either the prior written permission of the publisher or a
licence permitting restricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, 90 Tottenham Court Road,
London W1T 4LP.
The programs in this book have been included for their instructional value. They have been tested with care but are not guaranteed for
any particular purpose. The publisher does not offer any warranties or representations nor does it accept any liabilities with respect to
the programs.
All trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the author
or publisher any trademark ownership rights in such trademarks, nor does the use of such trademarks imply any affiliation with or
endorsement of this book by such owners.
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library
Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the Library of Congress
10 9 8 7 6 5 4 3 2 1
07 06 05 04 03
Printed and bound in the United States of America
Dedication
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
To Joanna, Rinku, Krista, and Renu
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Pearson Education
We work with leading authors to develop the strongest educational materials in computing, bringing cutting-edge thinking and best
learning practice to a global market.
Under a range of well-known imprints, including Addison-Wesley, we craft high-quality print and electronic publications which help
readers to understand and apply their content, whether studying or at work.
www.pearsoneduc.com To find out more about the complete range of our publishing, please visit us on the World Wide Web at:
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Preface
Since the 1994 release of the text "Introduction to Parallel Computing: Design and Analysis of Algorithms" by the same authors, the field
of parallel computing has undergone significant changes. Whereas tightly coupled scalable message-passing platforms were the norm a
decade ago, a significant portion of the current generation of platforms consists of inexpensive clusters of workstations, and
multiprocessor workstations and servers. Programming models for these platforms have also evolved over this time. Whereas most
machines a decade back relied on custom APIs for messaging and loop-based parallelism, current models standardize these APIs
across platforms. Message passing libraries such as PVM and MPI, thread libraries such as POSIX threads, and directive based models
such as OpenMP are widely accepted as standards, and have been ported to a variety of platforms.
With respect to applications, fluid dynamics, structural mechanics, and signal processing formed dominant applications a decade back.
These applications continue to challenge the current generation of parallel platforms. However, a variety of new applications have also
become important. These include data-intensive applications such as transaction processing and information retrieval, data mining and
analysis, and multimedia services. Applications in emerging areas of computational biology and nanotechnology pose tremendous
challenges for algorithms and systems development. Changes in architectures, programming models, and applications are also being
accompanied by changes in how parallel platforms are made available to the users in the form of grid-based services.
This evolution has a profound impact on the process of design, analysis, and implementation of parallel algorithms. Whereas the
emphasis of parallel algorithm design a decade back was on precise mapping of tasks to specific topologies such as meshes and
hypercubes, current emphasis is on programmability and portability, both from points of view of algorithm design and implementation. To
this effect, where possible, this book employs an architecture independent view of the underlying platforms and designs algorithms for an
abstract model. With respect to programming models, Message Passing Interface (MPI), POSIX threads, and OpenMP have been
selected. The evolving application mix for parallel computing is also reflected in various examples in the book.
This book forms the basis for a single concentrated course on parallel computing or a two-part sequence. Some suggestions for such a
two-part sequence are:
. This course would provide the basics of algorithm design and parallel 6 – 1 : Chapters Introduction to Parallel Computing
programming.
. 1
. This course would provide an 12 – 8  followed by Chapters 3  and 2 Design and Analysis of Parallel Algorithms: Chapters
in-depth coverage of design and analysis of various parallel algorithms.
. 2
The material in this book has been tested in Parallel Algorithms and Parallel Computing courses at the University of Minnesota and
Purdue University. These courses are taken primarily by graduate students and senior-level undergraduate students in Computer
Science. In addition, related courses in Scientific Computation, for which this material has also been tested, are taken by graduate
students in science and engineering, who are interested in solving computationally intensive problems.
Most chapters of the book include (i) examples and illustrations; (ii) problems that supplement the text and test students' understanding
of the material; and (iii) bibliographic remarks to aid researchers and students interested in learning more about related and advanced
topics. The comprehensive subject index helps the reader locate terms they might be interested in. The page number on which a term is
defined is highlighted in boldface in the index. Furthermore, the term itself appears in bold italics where it is defined. The sections that
deal with relatively complex material are preceded by a '*'. An instructors' manual containing slides of the figures and solutions to
). http://www.booksites.net/kumar selected problems is also available from the publisher (
As with our previous book, we view this book as a continually evolving resource. We thank all the readers who have kindly shared
critiques, opinions, problems, code, and other information relating to our first book. It is our sincere hope that we can continue this
interaction centered around this new book. We encourage readers to address communication relating to this book to
 with http://www.cs.umn.edu/~parbook . All relevant reader input will be added to the information archived at the site book-vk@cs.umn.edu
due credit to (and permission of) the sender(s). An on-line errata of the book will also be maintained at the site. We believe that in a
highly dynamic field such as ours, a lot is to be gained from a healthy exchange of ideas and material in this manner.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Acknowledgments
We would like to begin by acknowledging our spouses, Joanna, Rinku, Krista, and Renu to whom this book is dedicated. Without their
sacrifices this project would not have been seen completion. We also thank our parents, and family members, Akash, Avi, Chethan,
Eleni, Larry, Mary-Jo, Naina, Petros, Samir, Subhasish, Varun, Vibhav, and Vipasha for their affectionate support and encouragement
throughout this project.
Our respective institutions, Computer Sciences and Computing Research Institute (CRI) at Purdue University, Department of Computer
Science & Engineering, the Army High Performance Computing Research Center (AHPCRC), and the Digital Technology Center (DTC)
at the University of Minnesota, and the IBM T. J. Watson Research Center at Yorktown Heights, provided computing resources and
active and nurturing environments for the completion of this project.
This project evolved from our first book. We would therefore like to acknowledge all of the people who helped us with both editions. Many
people contributed to this project in different ways. We would like to thank Ahmed Sameh for his constant encouragement and support,
and Dan Challou, Michael Heath, Dinesh Mehta, Tom Nurkkala, Paul Saylor, and Shang-Hua Teng for the valuable input they provided
to the various versions of the book. We thank the students of the introduction to parallel computing classes at the University of
Minnesota and Purdue university for identifying and working through the errors in the early drafts of the book. In particular, we
acknowledge the patience and help of Jim Diehl and Rasit Eskicioglu, who worked through several early drafts of the manuscript to
identify numerous errors. Ramesh Agarwal, David Bailey, Rupak Biswas, Jim Bottum, Thomas Downar, Rudolf Eigenmann, Sonia
Fahmy, Greg Frederickson, John Gunnels, Fred Gustavson, Susanne Hambrusch, Bruce Hendrickson, Christoph Hoffmann, Kai Hwang,
Ioannis Ioannidis, Chandrika Kamath, David Keyes, Mehmet Koyuturk, Piyush Mehrotra, Zhiyuan Li, Jens Palsberg, Voicu Popescu, Alex
Pothen, Viktor Prasanna, Sanjay Ranka, Naren Ramakrishnan, Elisha Sacks, Vineet Singh, Sartaj Sahni, Vivek Sarin, Wojciech
Szpankowski, Srikanth Thirumalai, Jan Vitek, and David Yau have been great technical resources. It was a pleasure working with the
cooperative and helpful staff at Pearson Education. In particular, we would like to thank Keith Mansfield and Mary Lince for their
professional handling of the project.
The Army Research Laboratory, ARO, DOE, NASA, and NSF provided parallel computing research support for Ananth Grama, George
Karypis, and Vipin Kumar. In particular, Kamal Abdali, Michael Coyle, Jagdish Chandra, Frederica Darema, Stephen Davis, Wm
Randolph Franklin, Richard Hirsch, Charles Koelbel, Raju Namburu, N. Radhakrishnan, John Van Rosendale, Subhash Saini, and
Xiaodong Zhang have been supportive of our research programs in the area of parallel computing. Andrew Conn, Brenda Dietrich, John
Forrest, David Jensen, and Bill Pulleyblank at IBM supported the work of Anshul Gupta over the years.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 1. Introduction to Parallel Computing
The past decade has seen tremendous advances in microprocessor technology. Clock rates of processors have increased from about
40 MHz (e.g., a MIPS R3000, circa 1988) to over 2.0 GHz (e.g., a Pentium 4, circa 2002). At the same time, processors are now capable
of executing multiple instructions in the same cycle. The average number of cycles per instruction (CPI) of high end processors has
improved by roughly an order of magnitude over the past 10 years. All this translates to an increase in the peak floating point operation
execution rate (floating point operations per second, or FLOPS) of several orders of magnitude. A variety of other issues have also
become important over the same period. Perhaps the most prominent of these is the ability (or lack thereof) of the memory system to
feed data to the processor at the required rate. Significant innovations in architecture and software have addressed the alleviation of
bottlenecks posed by the datapath and the memory.
The role of concurrency in accelerating computing elements has been recognized for several decades. However, their role in providing
multiplicity of datapaths, increased access to storage elements (both memory and disk), scalable performance, and lower costs is
reflected in the wide variety of applications of parallel computing. Desktop machines, engineering workstations, and compute servers
with two, four, or even eight processors connected together are becoming common platforms for design applications. Large scale
applications in science and engineering rely on larger configurations of parallel computers, often comprising hundreds of processors.
Data intensive platforms such as database or web servers and applications such as transaction processing and data mining often use
clusters of workstations that provide high aggregate disk bandwidth. Applications in graphics and visualization use multiple rendering
pipes and processing elements to compute and render realistic environments with millions of polygons in real time. Applications requiring
high availability rely on parallel and distributed platforms for redundancy. It is therefore extremely important, from the point of view of
cost, performance, and application requirements, to understand the principles, tools, and techniques for programming the wide variety of
parallel platforms currently available.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
1.1 Motivating Parallelism
Development of parallel software has traditionally been thought of as time and effort intensive. This can be largely attributed to the
inherent complexity of specifying and coordinating concurrent tasks, a lack of portable algorithms, standardized environments, and
software development toolkits. When viewed in the context of the brisk rate of development of microprocessors, one is tempted to
question the need for devoting significant effort towards exploiting parallelism as a means of accelerating applications. After all, if it takes
two years to develop a parallel application, during which time the underlying hardware and/or software platform has become obsolete,
the development effort is clearly wasted. However, there are some unmistakable trends in hardware design, which indicate that
 performance increments in the future. realizable uniprocessor (or implicitly parallel) architectures may not be able to sustain the rate of
This is a result of lack of implicit parallelism as well as other bottlenecks such as the datapath and the memory. At the same time,
standardized hardware interfaces have reduced the turnaround time from the development of a microprocessor to a parallel machine
based on the microprocessor. Furthermore, considerable progress has been made in standardization of programming environments to
ensure a longer life-cycle for parallel applications. All of these present compelling arguments in favor of parallel computing platforms.
S 1.1.1 The Computational Power Argument – from Transistors to FLOP
In 1965, Gordon Moore made the following simple observation:
"The complexity for minimum component costs has increased at a rate of roughly a factor of two per year.
Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the
rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for
at least 10 years. That means by 1975, the number of components per integrated circuit for minimum cost will be
65,000."
His reasoning was based on an empirical log-linear relationship between device complexity and time, observed over three data points.
He used this to justify that by 1975, devices with as many as 65,000 components would become feasible on a single silicon chip
occupying an area of only about one-fourth of a square inch. This projection turned out to be accurate with the fabrication of a 16K CCD
memory with about 65,000 components in 1975. In a subsequent paper in 1975, Moore attributed the log-linear relationship to
exponential behavior of die sizes, finer minimum dimensions, and "circuit and device cleverness". He went on to state that:
"There is no room left to squeeze anything out by being clever. Going forward from here we have to depend on
the two size factors - bigger dies and finer dimensions."
He revised his rate of circuit complexity doubling to 18 months and projected from 1975 onwards at this reduced rate. This curve came to
be known as "Moore's Law". Formally, Moore's Law states that circuit complexity doubles every eighteen months. This empirical
relationship has been amazingly resilient over the years both for microprocessors as well as for DRAMs. By relating component density
and increases in die-size to the computing power of a device, Moore's law has been extrapolated to state that the amount of computing
power available at a given cost doubles approximately every 18 months.
f The limits of Moore's law have been the subject of extensive debate in the past few years. Staying clear of this debate, the issue o
e translating transistors into useful OPS (operations per second) is the critical one. It is possible to fabricate devices with very larg
l transistor counts. How we use these transistors to achieve increasing rates of computation is the key architectural challenge. A logica
 and devote Section 2.1 recourse to this is to rely on parallelism – both implicit and explicit. We will briefly discuss implicit parallelism in
the rest of this book to exploiting explicit parallelism.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
1.1.2 The Memory/Disk Speed Argument
The overall speed of computation is determined not just by the speed of the processor, but also by the ability of the memory system to
feed data to it. While clock rates of high-end processors have increased at roughly 40% per year over the past decade, DRAM access
times have only improved at the rate of roughly 10% per year over this interval. Coupled with increases in instructions executed per clock
cycle, this gap between processor speed and memory presents a tremendous performance bottleneck. This growing mismatch between
processor speed and DRAM latency is typically bridged by a hierarchy of successively faster memory devices called caches that rely on
locality of data reference to deliver higher memory system performance. In addition to the latency, the net effective bandwidth between
DRAM and the processor poses other problems for sustained computation rates.
The overall performance of the memory system is determined by the fraction of the total memory requests that can be satisfied from the
. Parallel platforms typically yield better memory system Section 2.2 cache. Memory system performance is addressed in greater detail in
performance because they provide (i) larger aggregate caches, and (ii) higher aggregate bandwidth to the memory system (both typically
linear in the number of processors). Furthermore, the principles that are at the heart of parallel algorithms, namely locality of data
reference, also lend themselves to cache-friendly serial algorithms. This argument can be extended to disks where parallel platforms can
be used to achieve high aggregate bandwidth to secondary storage. Here, parallel algorithms yield insights into the development of
out-of-core computations. Indeed, some of the fastest growing application areas of parallel computing in data servers (database servers,
web servers) rely not so much on their high aggregate computation rates but rather on the ability to pump data out at a faster rate.
1.1.3 The Data Communication Argument
As the networking infrastructure evolves, the vision of using the Internet as one large heterogeneous parallel/distributed computing
environment has begun to take shape. Many applications lend themselves naturally to such computing paradigms. Some of the most
impressive applications of massively parallel computing have been in the context of wide-area distributed platforms. The SETI (Search
for Extra Terrestrial Intelligence) project utilizes the power of a large number of home computers to analyze electromagnetic signals from
outer space. Other such efforts have attempted to factor extremely large integers and to solve large discrete optimization problems.
In many applications there are constraints on the location of data and/or resources across the Internet. An example of such an
application is mining of large commercial datasets distributed over a relatively low bandwidth network. In such applications, even if the
computing power is available to accomplish the required task without resorting to parallel computing, it is infeasible to collect the data at
a central location. In these cases, the motivation for parallelism comes not just from the need for computing resources but also from the
infeasibility or undesirability of alternate (centralized) approaches.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
1.2 Scope of Parallel Computing
Parallel computing has made a tremendous impact on a variety of areas ranging from computational simulations for scientific and
engineering applications to commercial applications in data mining and transaction processing. The cost benefits of parallelism coupled
with the performance requirements of applications present compelling arguments in favor of parallel computing. We present a small
sample of the diverse applications of parallel computing.
1.2.1 Applications in Engineering and Design
Parallel computing has traditionally been employed with great success in the design of airfoils (optimizing lift, drag, stability), internal
combustion engines (optimizing charge distribution, burn), high-speed circuits (layouts for delays and capacitive and inductive effects),
and structures (optimizing structural integrity, design parameters, cost, etc.), among others. More recently, design of
microelectromechanical and nanoelectromechanical systems (MEMS and NEMS) has attracted significant attention. While most
applications in engineering and design pose problems of multiple spatial and temporal scales and coupled physical phenomena, in the
case of MEMS/NEMS design these problems are particularly acute. Here, we often deal with a mix of quantum phenomena, molecular
dynamics, and stochastic and continuum models with physical processes such as conduction, convection, radiation, and structural
mechanics, all in a single system. This presents formidable challenges for geometric modeling, mathematical modeling, and algorithm
development, all in the context of parallel computers.
Other applications in engineering and design focus on optimization of a variety of processes. Parallel computers have been used to
solve a variety of discrete and continuous optimization problems. Algorithms such as Simplex, Interior Point Method for linear
optimization and Branch-and-bound, and Genetic programming for discrete optimization have been efficiently parallelized and are
frequently used.
1.2.2 Scientific Applications
The past few years have seen a revolution in high performance scientific computing applications. The sequencing of the human genome
by the International Human Genome Sequencing Consortium and Celera, Inc. has opened exciting new frontiers in bioinformatics.
Functional and structural characterization of genes and proteins hold the promise of understanding and fundamentally influencing
biological processes. Analyzing biological sequences with a view to developing new drugs and cures for diseases and medical
conditions requires innovative algorithms as well as large-scale computational power. Indeed, some of the newest parallel computing
technologies are targeted specifically towards applications in bioinformatics.
Advances in computational physics and chemistry have focused on understanding processes ranging in scale from quantum phenomena
to macromolecular structures. These have resulted in design of new materials, understanding of chemical pathways, and more efficient
processes. Applications in astrophysics have explored the evolution of galaxies, thermonuclear processes, and the analysis of extremely
large datasets from telescopes. Weather modeling, mineral prospecting, flood prediction, etc., rely heavily on parallel computers and
have very significant impact on day-to-day life.
Bioinformatics and astrophysics also present some of the most challenging problems with respect to analyzing extremely large datasets.
Protein and gene databases (such as PDB, SwissProt, and ENTREZ and NDB) along with Sky Survey datasets (such as the Sloan
Digital Sky Surveys) represent some of the largest scientific datasets. Effectively analyzing these datasets requires tremendous
computational power and holds the key to significant scientific discoveries.
1.2.3 Commercial Applications
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
With the widespread use of the web and associated static and dynamic content, there is increasing emphasis on cost-effective servers
capable of providing scalable performance. Parallel platforms ranging from multiprocessors to linux clusters are frequently used as web
and database servers. For instance, on heavy volume days, large brokerage houses on Wall Street handle hundreds of thousands of
simultaneous user sessions and millions of orders. Platforms such as IBMs SP supercomputers and Sun Ultra HPC servers power these
business-critical sites. While not highly visible, some of the largest supercomputing networks are housed on Wall Street.
The availability of large-scale transaction data has also sparked considerable interest in data mining and analysis for optimizing business
and marketing decisions. The sheer volume and geographically distributed nature of this data require the use of effective parallel
algorithms for such problems as association rule mining, clustering, classification, and time-series analysis.
1.2.4 Applications in Computer Systems
As computer systems become more pervasive and computation spreads over the network, parallel processing issues become engrained
into a variety of applications. In computer security, intrusion detection is an outstanding challenge. In the case of network intrusion
detection, data is collected at distributed sites and must be analyzed rapidly for signaling intrusion. The infeasibility of collecting this data
at a central location for analysis requires effective parallel and distributed algorithms. In the area of cryptography, some of the most
spectacular applications of Internet-based parallel computing have focused on factoring extremely large integers.
Embedded systems increasingly rely on distributed control algorithms for accomplishing a variety of tasks. A modern automobile consists
of tens of processors communicating to perform complex tasks for optimizing handling and performance. In such systems, traditional
parallel and distributed algorithms for leader selection, maximal independent set, etc., are frequently used.
While parallel computing has traditionally confined itself to platforms with well behaved compute and network elements in which faults
and errors do not play a significant role, there are valuable lessons that extend to computations on ad-hoc, mobile, or faulty
environments.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
1.3 Organization and Contents of the Text
This book provides a comprehensive and self-contained exposition of problem solving using parallel computers. Algorithms and metrics
focus on practical and portable models of parallel machines. Principles of algorithm design focus on desirable attributes of parallel
algorithms and techniques for achieving these in the contest of a large class of applications and architectures. Programming techniques
cover standard paradigms such as MPI and POSIX threads that are available across a range of parallel platforms.
. These parts are as follows: Figure 1.1 Chapters in this book can be grouped into four main parts as illustrated in
Figure 1.1. Recommended sequence for reading the chapters.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, Parallel Programming Platforms, discusses the physical Chapter 2  of the book. 4  through 2  This section spans Chapters Fundamentals
organization of parallel platforms. It establishes cost metrics that can be used for algorithm design. The objective of this chapter is not to
provide an exhaustive treatment of parallel architectures; rather, it aims to provide sufficient detail required to use these machines
, addresses key factors that contribute to efficient parallel algorithms and Principles of Parallel Algorithm Design , Chapter 3 efficiently.
, Basic Communication Operations, Chapter 4 presents a suite of techniques that can be applied across a wide range of applications.
presents a core set of operations that are used throughout the book for facilitating efficient data transfer in parallel algorithms. Finally,
, Analytical Modeling of Parallel Programs, deals with metrics for quantifying the performance of a parallel algorithm. Chapter 5
, Programming Using the Message-Passing Chapter 6  of the book. 7  and 6  This section includes Chapters Parallel Programming
, Chapter 7 Paradigm, focuses on the Message Passing Interface (MPI) for programming message passing platforms, including clusters.
Programming Shared Address Space Platforms, deals with programming paradigms such as threads and directive based approaches.
Using paradigms such as POSIX threads and OpenMP, it describes various features necessary for programming shared-address-space
parallel machines. Both of these chapters illustrate various programming concepts using a variety of examples of parallel programs.
 addresses sorting algorithms such as Chapter 9  present parallel non-numerical algorithms. 12 – 9  Chapters Non-numerical Algorithms
 describes algorithms for various graph theory Chapter 10 bitonic sort, bubble sort and its variants, quicksort, sample sort, and shellsort.
problems such as minimum spanning tree, shortest paths, and connected components. Algorithms for sparse graphs are also discussed.
Chapter 12  addresses search-based methods such as branch-and-bound and heuristic search for combinatorial problems. Chapter 11
classifies and presents parallel formulations for a variety of dynamic programming algorithms.
 covers basic operations on dense matrices Chapter 8  present parallel numerical algorithms. 13  and 8  Chapters Numerical Algorithms
such as matrix multiplication, matrix-vector multiplication, and Gaussian elimination. This chapter is included before non-numerical
algorithms, as the techniques for partitioning and assigning matrices to processors are common to many non-numerical algorithms.
 describes Chapter 13 Furthermore, matrix-vector and matrix-matrix multiplication algorithms form the kernels of many graph algorithms.
algorithms for computing Fast Fourier Transforms.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
1.4 Bibliographic Remarks
Many books discuss aspects of parallel processing at varying levels of detail. Hardware aspects of parallel computers have been
, RF89 , HB84 , DeC89 , Sto93 , AG94 , Fly95 , AG94 , HX98 , LW95 , CSG98 discussed extensively in several textbooks and monographs [
, LB98 ]. A number of texts discuss paradigms and languages for programming parallel computers [ Woo86 , WF84 , Tab91 , Tab90 , Sie85
], Col89 ], Cole [ Akl97 ]. Akl [ Wal91 , Per87 , Les93 , CT92 , Con89 , Ble90 , Bab88 , BA82 , And91 , WA98 , 00 + CDK , GSNL98 , GLS99 , Pac98
] discuss various aspects of Qui94 ], and Quinn [ MS96 ], Miller and Stout [ Lei92 ], Leighton [ Fos95 ], Foster [ GR90 Gibbons and Rytter [
] discuss various aspects of parallel computing using Pfi98 ] and Pfister [ Buy99 parallel algorithm design and analysis. Buyya (Editor) [
] HQ91 ] and Hatcher and Quinn [ HS86 , Hil85 ] covers parallel algorithms for the PRAM model of computation. Hillis [ Jaj92 clusters. Jaja [
] Sha85 . Sharp [ actors ] discusses a model of concurrent computation based on Agh86 discuss data-parallel programming. Agha [
, JGD87 , Zom96 , Fou94 , CL93 addresses data-flow computing. Some books provide a general overview of topics in parallel computing [
, DDSV99 ]. Many books address parallel processing applications in numerical analysis and scientific computing [ Qui94 , Mol93 , LER92
] provide an application-oriented view of algorithm design for AFKW90  [ et al. ] and Angus 88 + FJL  [ et al. ]. Fox Car89 , GO93 , FJDS96
] discuss parallel algorithms, with emphasis on numerical applications. BT97 problems in scientific computing. Bertsekas and Tsitsiklis [
] and Dew, Earnshaw, and RS90b ] discuss parallel algorithms in computational geometry. Ranka and Sahni [ AL93 Akl and Lyons [
] covers parallel algorithms for graphics Gre91 ] address parallel algorithms for use in computer vision. Green [ DEH89 Heywood [
, KKKS94 , KGK90 , HD89b , Gup87 applications. Many books address the use of parallel processing in artificial intelligence applications [
]. RZ89 , Kow88
]. ACM91 A useful collection of reviews, bibliographies and indexes has been put together by the Association for Computing Machinery [
] present a collection of papers on various aspects of the application and potential of parallel computing. The MM91 Messina and Murli [
scope of parallel processing and various aspects of US government support have also been discussed in National Science Foundation
]. GOV99 , NSF91 reports [
A number of conferences address various aspects of parallel computing. A few important ones are the Supercomputing Conference,
ACM Symposium on Parallel Algorithms and Architectures, the International Conference on Parallel Processing, the International
Parallel and Distributed Processing Symposium, Parallel Computing, and the SIAM Conference on Parallel Processing. Important
journals in parallel processing include IEEE Transactions on Parallel and Distributed Systems, International Journal of Parallel
Programming, Journal of Parallel and Distributed Computing, Parallel Computing, IEEE Concurrency, and Parallel Processing Letters.
These proceedings and journals provide a rich source of information on the state of the art in parallel processing.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
) and list the five most powerful http://www.top500.org/  Go to the Top 500 Supercomputers site ( 1.1
supercomputers along with their FLOPS rating.
 List three major problems requiring the use of supercomputing in the following domains: 1.2
Structural Mechanics. . 1
Computational Biology. . 2
Commercial Applications. . 3
 Collect statistics on the number of components in state of the art integrated circuits over the years. Plot the 1.3
number of components as a function of time and compare the growth rate to that dictated by Moore's law.
 Repeat the above experiment for the peak FLOPS rate of processors and compare the speed to that inferred 1.4
from Moore's law.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 2. Parallel Programming Platforms
e The traditional logical view of a sequential computer consists of a memory connected to a processor via a datapath. All thre
f components – processor, memory, and datapath – present bottlenecks to the overall processing rate of a computer system. A number o
n architectural innovations over the years have addressed these bottlenecks. One of the most important innovations is multiplicity – i
t processing units, datapaths, and memory units. This multiplicity is either entirely hidden from the programmer, as in the case of implici
s parallelism, or exposed to the programmer in different forms. In this chapter, we present an overview of important architectural concept
a as they relate to parallel processing. The objective is to provide sufficient detail for programmers to be able to write efficient code on
y variety of platforms. We develop cost models and abstractions for quantifying the performance of various parallel algorithms, and identif
. bottlenecks resulting from various programming constructs
We start our discussion of parallel platforms with an overview of serial and implicitly parallel architectures. This is necessitated by the fact
that it is often possible to re-engineer codes to achieve significant speedups (2 x to 5 x unoptimized speed) using simple program
transformations. Parallelizing sub-optimal serial codes often has undesirable effects of unreliable speedups and misleading runtimes. For
this reason, we advocate optimizing serial performance of codes before attempting parallelization. As we shall demonstrate through this
chapter, the tasks of serial and parallel optimization often have very similar characteristics. After discussing serial and implicitly parallel
architectures, we devote the rest of this chapter to organization of parallel platforms, underlying cost models for algorithms, and platform
2.1 abstractions for portable algorithm design. Readers wishing to delve directly into parallel architectures may choose to skip Sections
. 2.2 and
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.1 Implicit Parallelism: Trends in Microprocessor Architectures*
While microprocessor technology has delivered significant improvements in clock speeds over the past decade, it has also exposed a
variety of other performance bottlenecks. To alleviate these bottlenecks, microprocessor designers have explored alternate routes to
cost-effective performance gains. In this section, we will outline some of these trends with a view to understanding their limitations and
how they impact algorithm and code development. The objective here is not to provide a comprehensive description of processor
architectures. There are several excellent texts referenced in the bibliography that address this topic.
Clock speeds of microprocessors have posted impressive gains - two to three orders of magnitude over the past 20 years. However, these
increments in clock speed are severely diluted by the limitations of memory technology. At the same time, higher levels of device
integration have also resulted in a very large transistor count, raising the obvious issue of how best to utilize them. Consequently,
techniques that enable execution of multiple instructions in a single clock cycle have become popular. Indeed, this trend is evident in the
current generation of microprocessors such as the Itanium, Sparc Ultra, MIPS, and Power4. In this section, we briefly explore mechanisms
used by various processors for supporting multiple instruction execution.
2.1.1 Pipelining and Superscalar Execution
Processors have long relied on pipelines for improving execution rates. By overlapping various stages in instruction execution (fetch,
schedule, decode, operand fetch, execute, store, among others), pipelining enables faster execution. The assembly-line analogy works
well for understanding pipelines. If the assembly of a car, taking 100 time units, can be broken into 10 pipelined stages of 10 units each, a
single assembly line can produce a car every 10 time units! This represents a 10-fold speedup over producing cars entirely serially, one
after the other. It is also evident from this example that to increase the speed of a single pipeline, one would break down the tasks into
smaller and smaller units, thus lengthening the pipeline and increasing overlap in execution. In the context of processors, this enables
faster clock rates since the tasks are now smaller. For example, the Pentium 4, which operates at 2.0 GHz, has a 20 stage pipeline. Note
that the speed of a single pipeline is ultimately limited by the largest atomic task in the pipeline. Furthermore, in typical instruction traces,
every fifth to sixth instruction is a branch instruction. Long instruction pipelines therefore need effective techniques for predicting branch
destinations so that pipelines can be speculatively filled. The penalty of a misprediction increases as the pipelines become deeper since a
larger number of instructions need to be flushed. These factors place limitations on the depth of a processor pipeline and the resulting
performance gains.
An obvious way to improve instruction execution rate beyond this level is to use multiple pipelines. During each clock cycle, multiple
instructions are piped into the processor in parallel. These instructions are executed on multiple functional units. We illustrate this process
with the help of an example.
Example 2.1 Superscalar execution
Consider a processor with two pipelines and the ability to simultaneously issue two instructions. These processors are
sometimes also referred to as super-pipelined processors. The ability of a processor to issue multiple instructions in the
 allows two issues per Figure 2.1 same cycle is referred to as superscalar execution. Since the architecture illustrated in
clock cycle, it is also referred to as two-way superscalar or dual issue execution.
Figure 2.1. Example of a two-way superscalar execution of instructions.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 for adding four numbers. The first and second instructions Figure 2.1 Consider the execution of the first code fragment in
are independent and therefore can be issued concurrently. This is illustrated in the simultaneous issue of the instructions
 = 0. The instructions are fetched, decoded, and the operands are fetched. Thet  at load R2, @1008  and load R1, @1000
 are also mutually independent, although they must be executed add R2, @100C  and add R1, @1004 next two instructions,
 = 1 since the processors are pipelined.t after the first two instructions. Consequently, they can be issued concurrently at
 cannot be executed store R1, @2000  and add R1, R2  = 5. The next two instructions,t These instructions terminate at
 instruction add ) is used by the latter. Therefore, only the R1 concurrently since the result of the former (contents of register
 can be executed only after the add R1, R2  = 3. Note that the instructiont  instruction at store  = 2 and thet is issued at
. The schedule Figure 2.1(b) previous two instructions have been executed. The instruction schedule is illustrated in
assumes that each memory access takes a single cycle. In reality, this may not be the case. The implications of this
 on memory system performance. Section 2.2 assumption are discussed in
In principle, superscalar execution seems natural, even simple. However, a number of issues need to be resolved. First, as illustrated in
, instructions in a program may be related to each other. The results of an instruction may be required for subsequent Example 2.1
 for adding four Figure 2.1 . For instance, consider the second code fragment in true data dependency instructions. This is referred to as
, and similarly between subsequent instructions. add R1, @1004  and load R1, @1000 numbers. There is a true data dependency between
Dependencies of this type must be resolved before simultaneous issue of instructions. This has two implications. First, since the resolution
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
is done at runtime, it must be supported in hardware. The complexity of this hardware can be high. Second, the amount of instruction level
parallelism in a program is often limited and is a function of coding technique. In the second code fragment, there can be no simultaneous
 also illustrate that in many cases it is possible to Figure 2.1(a) issue, leading to poor resource utilization. The three code fragments in
extract more parallelism by reordering the instructions and by altering the code. Notice that in this example the code reorganization
corresponds to exposing parallelism in a form that can be used by the instruction issue mechanism.
Another source of dependency between instructions results from the finite resources shared by various pipelines. As an example, consider
the co-scheduling of two floating point operations on a dual issue machine with a single floating point unit. Although there might be no data
dependencies between the instructions, they cannot be scheduled together since both need the floating point unit. This form of
. resource dependency dependency in which two instructions compete for a single processor resource is referred to as
The flow of control through a program enforces a third form of dependency between instructions. Consider the execution of a conditional
 across branches a priori branch instruction. Since the branch destination is known only at the point of execution, scheduling instructions
 and are typically handled procedural dependencies  or branch dependencies may lead to errors. These dependencies are referred to as
by speculatively scheduling across branches and rolling back in case of errors. Studies of typical traces have shown that on average, a
branch instruction is encountered between every five to six instructions. Therefore, just as in populating instruction pipelines, accurate
branch prediction is critical for efficient superscalar execution.
The ability of a processor to detect and schedule concurrent instructions is critical to superscalar performance. For instance, consider the
y  which also computes the sum of four numbers. The reader will note that this is merely a semanticall Figure 2.1 third code fragment in
load equivalent reordering of the first code fragment. However, in this case, there is a data dependency between the first two instructions –
y . Therefore, these instructions cannot be issued simultaneously. However, if the processor had the abilit add R1, @1004  and R1, @1000
t  – with the first instruction. In the nex load R2, @1008 to look ahead, it would realize that it is possible to schedule the third instruction –
t issue cycle, instructions two and four can be scheduled, and so on. In this way, the same execution schedule can be derived for the firs
 to accomplish desired reordering. out-of-order and third code fragments. However, the processor needs the ability to issue instructions
 issue of instructions can be highly limited as illustrated by this example. Most current in-order The parallelism available in
, exploits dynamic instruction issue microprocessors are capable of out-of-order issue and completion. This model, also referred to as
maximum instruction level parallelism. The processor uses a window of instructions from which it selects instructions for simultaneous
issue. This window corresponds to the look-ahead of the scheduler.
. Figure 2.1 The performance of superscalar architectures is limited by the available instruction level parallelism. Consider the example in
For simplicity of discussion, let us ignore the pipelining aspects of the example and focus on the execution aspects of the program.
Assuming two execution units (multiply-add units), the figure illustrates that there are several zero-issue cycles (cycles in which the
floating point unit is idle). These are essentially wasted cycles from the point of view of the execution unit. If, during a particular cycle, no
; if only part of the execution units are used during a cycle, vertical waste instructions are issued on the execution units, it is referred to as
. In the example, we have two cycles of vertical waste and one cycle with horizontal waste. In all, only three horizontal waste it is termed
of the eight available cycles are used for computation. This implies that the code fragment will yield no more than three-eighths of the peak
rated FLOP count of the processor. Often, due to limited parallelism, resource dependencies, or the inability of a processor to extract
parallelism, the resources of superscalar processors are heavily under-utilized. Current microprocessors typically support up to four-issue
superscalar execution.
2.1.2 Very Long Instruction Word Processors
The parallelism extracted by superscalar processors is often limited by the instruction look-ahead. The hardware logic for dynamic
dependency analysis is typically in the range of 5-10% of the total logic on conventional microprocessors (about 5% on the four-way
superscalar Sun UltraSPARC). This complexity grows roughly quadratically with the number of issues and can become a bottleneck. An
alternate concept for exploiting instruction-level parallelism used in very long instruction word (VLIW) processors relies on the compiler to
resolve dependencies and resource availability at compile time. Instructions that can be executed concurrently are packed into groups and
parceled off to the processor as a single long instruction word (thus the name) to be executed on multiple functional units at the same
time.
The VLIW concept, first used in Multiflow Trace (circa 1984) and subsequently as a variant in the Intel IA64 architecture, has both
advantages and disadvantages compared to superscalar processors. Since scheduling is done in software, the decoding and instruction
issue mechanisms are simpler in VLIW processors. The compiler has a larger context from which to select instructions and can use a
variety of transformations to optimize parallelism when compared to a hardware issue unit. Additional parallel instructions are typically
made available to the compiler to control parallel execution. However, compilers do not have the dynamic program state (e.g., the branch
history buffer) available to make scheduling decisions. This reduces the accuracy of branch and memory prediction, but allows the use of
more sophisticated static prediction schemes. Other runtime situations such as stalls on data fetch because of cache misses are
extremely difficult to predict accurately. This limits the scope and performance of static compiler-based scheduling.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Finally, the performance of VLIW processors is very sensitive to the compilers' ability to detect data and resource dependencies and read
and write hazards, and to schedule instructions for maximum parallelism. Loop unrolling, branch prediction and speculative execution all
play important roles in the performance of VLIW processors. While superscalar and VLIW processors have been successful in exploiting
implicit parallelism, they are generally limited to smaller scales of concurrency in the range of four- to eight-way parallelism.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.2 Limitations of Memory System Performance*
The effective performance of a program on a computer relies not just on the speed of the processor but also on the ability of the memory
system to feed data to the processor. At the logical level, a memory system, possibly consisting of multiple levels of caches, takes in a
 is referred to asl  nanoseconds. Here,l  containing the requested word after b request for a memory word and returns a block of data of size
 of the bandwidth  of the memory. The rate at which data can be pumped from the memory to the processor determines the latency the
memory system.
It is very important to understand the difference between latency and bandwidth since different, often competing, techniques are required
for addressing these. As an analogy, if water comes out of the end of a fire hose 2 seconds after a hydrant is turned on, then the latency of
the system is 2 seconds. Once the flow starts, if the hose pumps water at 1 gallon/second then the 'bandwidth' of the hose is 1
gallon/second. If we need to put out a fire immediately, we might desire a lower latency. This would typically require higher water pressure
from the hydrant. On the other hand, if we wish to fight bigger fires, we might desire a higher flow rate, necessitating a wider hose and
hydrant. As we shall see here, this analogy works well for memory systems as well. Latency and bandwidth both play critical roles in
determining memory system performance. We examine these separately in greater detail using a few examples.
To study the effect of memory system latency, we assume in the following examples that a memory block consists of one word. We later
relax this assumption while examining the role of memory bandwidth. Since we are primarily interested in maximum achievable
performance, we also assume the best case cache-replacement policy. We refer the reader to the bibliography for a detailed discussion of
memory system design.
Example 2.2 Effect of memory latency on performance
Consider a processor operating at 1 GHz (1 ns clock) connected to a DRAM with a latency of 100 ns (no caches). Assume
that the processor has two multiply-add units and is capable of executing four instructions in each cycle of 1 ns. The peak
processor rating is therefore 4 GFLOPS. Since the memory latency is equal to 100 cycles and block size is one word,
every time a memory request is made, the processor must wait 100 cycles before it can process the data. Consider the
problem of computing the dot-product of two vectors on such a platform. A dot-product computation performs one
multiply-add on a single pair of vector elements, i.e., each floating point operation requires one data fetch. It is easy to see
that the peak speed of this computation is limited to one floating point operation every 100 ns, or a speed of 10 MFLOPS,
a very small fraction of the peak processor rating. This example highlights the need for effective memory system
performance in achieving high computation rates.
2.2.1 Improving Effective Memory Latency Using Caches
Handling the mismatch in processor and DRAM speeds has motivated a number of architectural innovations in memory system design.
One such innovation addresses the speed mismatch by placing a smaller and faster memory between the processor and the DRAM. This
memory, referred to as the cache, acts as a low-latency high-bandwidth storage. The data needed by the processor is first fetched into the
cache. All subsequent accesses to data items residing in the cache are serviced by the cache. Thus, in principle, if a piece of data is
repeatedly used, the effective latency of this memory system can be reduced by the cache. The fraction of data references satisfied by the
 of the computation on the system. The effective computation rate of many applications is bounded not hit ratio cache is called the cache
by the processing rate of the CPU, but by the rate at which data can be pumped into the CPU. Such computations are referred to as being
. The performance of memory bound programs is critically impacted by the cache hit ratio. memory bound
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Example 2.3 Impact of caches on memory system performance
As in the previous example, consider a 1 GHz processor with a 100 ns latency DRAM. In this case, we introduce a cache
of size 32 KB with a latency of 1 ns or one cycle (typically on the processor itself). We use this setup to multiply two
 of dimensions 32 x 32. We have carefully chosen these numbers so that the cache is large enough to B  and A matrices
. Once again, we assume an ideal cache placement strategy in which C , as well as the result matrix B  and A store matrices
none of the data items are overwritten by others. Fetching the two matrices into the cache corresponds to fetching 2K
 matrices takes n  x n words, which takes approximately 200 µs. We know from elementary algorithmics that multiplying two
t  operations. For our problem, this corresponds to 64K operations, which can be performed in 16K cycles (or 16 µs) a 3 n 2
four instructions per cycle. The total time for the computation is therefore approximately the sum of time for load/store
6 operations and the time for the computation itself, i.e., 200+16 µs. This corresponds to a peak computation rate of 64K/21
or 303 MFLOPS. Note that this is a thirty-fold improvement over the previous example, although it is still less than 10% of
the peak processor performance. We see in this example that by placing a small cache memory, we are able to improve
processor utilization considerably.
The improvement in performance resulting from the presence of the cache is based on the assumption that there is repeated reference to
 of reference. In our temporal locality the same data item. This notion of repeated reference to a data item in a small time window is called
 notation.) Data reuse is O ) computation. (See the Appendix for an explanation of the 3 n ( O ) data accesses and 2 n ( O example, we had
critical for cache performance because if each data item is used only once, it would still have to be fetched once per use from the DRAM,
and therefore the DRAM latency would be paid for each operation.
2.2.2 Impact of Memory Bandwidth
Memory bandwidth refers to the rate at which data can be moved between the processor and memory. It is determined by the bandwidth
of the memory bus as well as the memory units. One commonly used technique to improve memory bandwidth is to increase the size of
the memory blocks. For an illustration, let us relax our simplifying restriction on the size of the memory block and assume that a single
. cache line memory request returns a contiguous block of four words. The single unit of four words in this case is also referred to as a
Conventional computers typically fetch two to eight words together into the cache. We will see how this helps the performance of
applications for which data reuse is limited.
Example 2.4 Effect of block size: dot-product of two vectors
Consider again a memory system with a single cycle cache and 100 cycle latency DRAM with the processor operating at 1
GHz. If the block size is one word, the processor takes 100 cycles to fetch each word. For each pair of words, the
dot-product performs one multiply-add, i.e., two FLOPs. Therefore, the algorithm performs one FLOP every 100 cycles for
. Example 2.2 a peak speed of 10 MFLOPS as illustrated in
Now let us consider what happens if the block size is increased to four words, i.e., the processor can fetch a four-word
cache line every 100 cycles. Assuming that the vectors are laid out linearly in memory, eight FLOPs (four multiply-adds)
can be performed in 200 cycles. This is because a single memory access fetches four consecutive words in the vector.
Therefore, two accesses can fetch four elements of each of the vectors. This corresponds to a FLOP every 25 ns, for a
peak speed of 40 MFLOPS. Note that increasing the block size from one to four words did not change the latency of the
memory system. However, it increased the bandwidth four-fold. In this case, the increased bandwidth of the memory
system enabled us to accelerate the dot-product algorithm which has no data reuse at all.
Another way of quickly estimating performance bounds is to estimate the cache hit ratio, using it to compute mean access
time per word, and relating this to the FLOP rate via the underlying algorithm. For example, in this example, there are two
DRAM accesses (cache misses) for every eight data accesses required by the algorithm. This corresponds to a cache hit
ratio of 75%. Assuming that the dominant overhead is posed by the cache misses, the average memory access time
contributed by the misses is 25% at 100 ns (or 25 ns/word). Since the dot-product has one operation/word, this
corresponds to a computation rate of 40 MFLOPS as before. A more accurate estimate of this rate would compute the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
average memory access time as 0.75 x 1 + 0.25 x 100 or 25.75 ns/word. The corresponding computation rate is 38.8
MFLOPS.
 corresponds to a wide data bus (4 words or 128 bits) connected to multiple memory Example 2.4 Physically, the scenario illustrated in
banks. In practice, such wide buses are expensive to construct. In a more practical system, consecutive words are sent on the memory
bus on subsequent bus cycles after the first word is retrieved. For example, with a 32 bit data bus, the first word is put on the bus after 100
ns (the associated latency) and one word is put on each subsequent bus cycle. This changes our calculations above slightly since the
entire cache line becomes available only after 100 + 3 x (memory bus cycle) ns. Assuming a data bus operating at 200 MHz, this adds 15
ns to the cache line access time. This does not change our bound on the execution rate significantly.
The above examples clearly illustrate how increased bandwidth results in higher peak computation rates. They also make certain
assumptions that have significance for the programmer. The data layouts were assumed to be such that consecutive data words in
 of memory spatial locality memory were used by successive instructions. In other words, if we take a computation-centric view, there is a
access. If we take a data-layout centric point of view, the computation is ordered so that successive computations require contiguous data.
If the computation (or access pattern) does not have spatial locality, then effective bandwidth can be much smaller than the peak
bandwidth.
An example of such an access pattern is in reading a dense matrix column-wise when the matrix has been stored in a row-major fashion in
memory. Compilers can often be relied on to do a good job of restructuring computation to take advantage of spatial locality.
Example 2.5 Impact of strided access
Consider the following code fragment:
1 for (i = 0; i < 1000; i++)
2 column_sum[i] = 0.0;
3 for (j = 0; j < 1000; j++)
4 column_sum[i] += b[j][i];
. There are two observations that can be made: column_sum  into a vector b The code fragment sums columns of the matrix
 is accessed in a column order as b  is small and easily fits into the cache; and (ii) the matrix column_sum (i) the vector
. For a matrix of size 1000 x 1000, stored in a row-major order, this corresponds to accessing Figure 2.2(a) illustrated in
 entry. Therefore, it is likely that only one word in each cache line fetched from memory will be used. th every 1000
Consequently, the code fragment as written above is likely to yield poor performance.
Figure 2.2. Multiplying a matrix with a vector: (a) multiplying column-by-column, keeping a running sum;
(b) computing each element of the result as a dot product of a row of the matrix with the vector.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The above example illustrates problems with strided access (with strides greater than one). The lack of spatial locality in computation
causes poor memory system performance. Often it is possible to restructure the computation to remove strided access. In the case of our
example, a simple rewrite of the loops is possible as follows:
Example 2.6 Eliminating strided access
Consider the following restructuring of the column-sum fragment:
1 for (i = 0; i < 1000; i++)
2 column_sum[i] = 0.0;
3 for (j = 0; j < 1000; j++)
4 for (i = 0; i < 1000; i++)
5 column_sum[i] += b[j][i];
. However, the reader will note that this Figure 2.2(b) In this case, the matrix is traversed in a row-order as illustrated in
 can be retained in the cache through the loops. Indeed, for column_sum code fragment relies on the fact that the vector
this particular example, our assumption is reasonable. If the vector is larger, we would have to break the iteration space
 an iteration space. The tiling into blocks and compute the product one block at a time. This concept is also called
improved performance of this loop is left as an exercise for the reader.
So the next question is whether we have effectively solved the problems posed by memory latency and bandwidth. While peak processor
rates have grown significantly over the past decades, memory latency and bandwidth have not kept pace with this increase. Consequently,
for typical computers, the ratio of peak FLOPS rate to peak memory bandwidth is anywhere between 1 MFLOPS/MBs (the ratio signifies
FLOPS per megabyte/second of bandwidth) to 100 MFLOPS/MBs. The lower figure typically corresponds to large scale vector
supercomputers and the higher figure to fast microprocessor based computers. This figure is very revealing in that it tells us that on
average, a word must be reused 100 times after being fetched into the full bandwidth storage (typically L1 cache) to be able to achieve full
processor utilization. Here, we define full-bandwidth as the rate of data transfer required by a computation to make it processor bound.
The series of examples presented in this section illustrate the following concepts:
Exploiting spatial and temporal locality in applications is critical for amortizing memory latency and increasing effective memory
bandwidth.
Certain applications have inherently greater temporal locality than others, and thus have greater tolerance to low memory
bandwidth. The ratio of the number of operations to number of memory accesses is a good indicator of anticipated tolerance to
memory bandwidth.
Memory layouts and organizing computation appropriately can make a significant impact on the spatial and temporal locality.
2.2.3 Alternate Approaches for Hiding Memory Latency
Imagine sitting at your computer browsing the web during peak network traffic hours. The lack of response from your browser can be
alleviated using one of three simple approaches:
e (i) we anticipate which pages we are going to browse ahead of time and issue requests for them in advance; (ii) we open multipl
) browsers and access different pages in each browser, thus while we are waiting for one page to load, we could be reading others; or (iii
, prefetching we access a whole bunch of pages in one go – amortizing the latency across various accesses. The first approach is called
, and the third one corresponds to spatial locality in accessing memory words. Of these three approaches, multithreading the second
spatial locality of memory accesses has been discussed before. We focus on prefetching and multithreading as techniques for latency
hiding in this section.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Multithreading for Latency Hiding
A thread is a single stream of control in the flow of a program. We illustrate threads with a simple example:
Example 2.7 Threaded execution of matrix multiplication
. c  to get vector b  by a vector a  matrix n  x n Consider the following code segment for multiplying an
1 for(i=0;i<n;i++)
2 c[i] = dot_product(get_row(a, i), b);
. Notice that each b  with the vector a  as the dot product of the corresponding row of c This code computes each element of
dot-product is independent of the other, and therefore represents a concurrent unit of execution. We can safely rewrite the
above code segment as:
1 for(i=0;i<n;i++)
2 c[i] = create_thread(dot_product, get_row(a, i), b);
The only difference between the two code segments is that we have explicitly specified each instance of the dot-product
, there are a number of APIs for specifying threads. We Chapter 7 computation as being a thread. (As we shall learn in
have simply chosen an intuitive name for a function to create threads.) Now, consider the execution of each instance of the
. The first instance of this function accesses a pair of vector elements and waits for them. In the dot_product function
l meantime, the second instance of this function can access two other vector elements in the next cycle, and so on. After
 is the latency of the memory system, the first function instance gets the requested data from memoryl units of time, where
and can perform the required computation. In the next cycle, the data items for the next function instance arrive, and so on.
In this way, in every clock cycle, we can perform a computation.
 is predicated upon two assumptions: the memory system is capable of servicing multiple Example 2.7 The execution schedule in
outstanding requests, and the processor is capable of switching threads at every cycle. In addition, it also requires the program to have an
explicit specification of concurrency in the form of threads. Multithreaded processors are capable of maintaining the context of a number of
threads of computation with outstanding requests (memory accesses, I/O, or communication requests) and execute them as the requests
are satisfied. Machines such as the HEP and Tera rely on multithreaded processors that can switch the context of execution in every cycle.
Consequently, they are able to hide latency effectively, provided there is enough concurrency (threads) to keep the processor from idling.
The tradeoffs between concurrency and latency will be a recurring theme through many chapters of this text.
Prefetching for Latency Hiding
In a typical program, a data item is loaded and used by a processor in a small time window. If the load results in a cache miss, then the
use stalls. A simple solution to this problem is to advance the load operation so that even if there is a cache miss, the data is likely to have
arrived by the time it is used. However, if the data item has been overwritten between load and use, a fresh load is issued. Note that this is
no worse than the situation in which the load had not been advanced. A careful examination of this technique reveals that prefetching
works for much the same reason as multithreading. In advancing the loads, we are trying to identify independent threads of execution that
have no resource dependency (i.e., use the same registers) with respect to other threads. Many compilers aggressively try to advance
loads to mask memory system latency.
Example 2.8 Hiding latency by prefetching
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 loop. In the first iteration of the loop, the processor for  using a single b  and a Consider the problem of adding two vectors
. Since these are not in the cache, the processor must pay the memory latency. While these requests b[0]  and a[0] requests
. Assuming that each request is generated in one cycle (1 ns) b[1]  and a[1] are being serviced, the processor also requests
and memory requests are satisfied in 100 ns, after 100 such requests the first set of data items is returned by the memory
system. Subsequently, one pair of vector components will be returned every cycle. In this way, in each subsequent cycle,
one addition can be performed and processor cycles are not wasted.
2.2.4 Tradeoffs of Multithreading and Prefetching
While it might seem that multithreading and prefetching solve all the problems related to memory system performance, they are critically
impacted by the memory bandwidth.
Example 2.9 Impact of bandwidth on multithreaded programs
Consider a computation running on a machine with a 1 GHz clock, 4-word cache line, single cycle access to the cache,
and 100 ns latency to DRAM. The computation has a cache hit ratio at 1 KB of 25% and at 32 KB of 90%. Consider two
cases: first, a single threaded execution in which the entire cache is available to the serial context, and second, a
multithreaded execution with 32 threads where each thread has a cache residency of 1 KB. If the computation makes one
data request in every cycle of 1 ns, in the first case the bandwidth requirement to DRAM is one word every 10 ns since the
other words come from the cache (90% cache hit ratio). This corresponds to a bandwidth of 400 MB/s. In the second case,
the bandwidth requirement to DRAM increases to three words every four cycles of each thread (25% cache hit ratio).
Assuming that all threads exhibit similar cache behavior, this corresponds to 0.75 words/ns, or 3 GB/s.
 illustrates a very important issue, namely that the bandwidth requirements of a multithreaded system may increase very Example 2.9
significantly because of the smaller cache residency of each thread. In the example, while a sustained DRAM bandwidth of 400 MB/s is
reasonable, 3.0 GB/s is more than most systems currently offer. At this point, multithreaded systems become bandwidth bound instead of
latency bound. It is important to realize that multithreading and prefetching only address the latency problem and may often exacerbate the
bandwidth problem.
Another issue relates to the additional hardware resources required to effectively use prefetching and multithreading. Consider a situation
in which we have advanced 10 loads into registers. These loads require 10 registers to be free for the duration. If an intervening instruction
overwrites the registers, we would have to load the data again. This would not increase the latency of the fetch any more than the case in
which there was no prefetching. However, now we are fetching the same data item twice, resulting in doubling of the bandwidth
. It can be Example 2.9 requirement from the memory system. This situation is similar to the one due to cache constraints as illustrated in
alleviated by supporting prefetching and multithreading with larger register files and caches.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.3 Dichotomy of Parallel Computing Platforms
In the preceding sections, we pointed out various factors that impact the performance of a serial or implicitly parallel program. The
increasing gap in peak and sustainable performance of current microprocessors, the impact of memory system performance, and the
distributed nature of many problems present overarching motivations for parallelism. We now introduce, at a high level, the elements of
parallel computing platforms that are critical for performance oriented and portable parallel programming. To facilitate our discussion of
parallel platforms, we first explore a dichotomy based on the logical and physical organization of parallel platforms. The logical
organization refers to a programmer's view of the platform while the physical organization refers to the actual hardware organization of the
platform. The two critical components of parallel computing from a programmer's perspective are ways of expressing parallel tasks and
mechanisms for specifying interaction between these tasks. The former is sometimes also referred to as the control structure and the latter
as the communication model.
2.3.1 Control Structure of Parallel Platforms
Parallel tasks can be specified at various levels of granularity. At one extreme, each program in a set of programs can be viewed as one
parallel task. At the other extreme, individual instructions within a program can be viewed as parallel tasks. Between these extremes lie a
range of models for specifying the control structure of programs and the corresponding architectural support for them.
Example 2.10 Parallelism from single instruction on multiple processors
Consider the following code segment that adds two vectors:
1 for (i = 0; i < 1000; i++)
2 c[i] = a[i] + b[i];
, etc., c[0] = a[0] + b[0]; c[1] = a[1] + b[1]; In this example, various iterations of the loop are independent of each other; i.e.,
can all be executed independently of each other. Consequently, if there is a mechanism for executing the same instruction,
 on all the processors with appropriate data, we could execute this loop much faster. add in this case
Processing units in parallel computers either operate under the centralized control of a single control unit or work independently. In
 (SIMD), a single control unit dispatches instructions to each single instruction stream, multiple data stream architectures referred to as
 illustrates a typical SIMD architecture. In an SIMD parallel computer, the same instruction is executed Figure 2.3(a) processing unit.
 instruction is dispatched to all processors and executed concurrently by add , the Example 2.10 synchronously by all processing units. In
them. Some of the earliest parallel computers such as the Illiac IV, MPP, DAP, CM-2, and MasPar MP-1 belonged to this class of
machines. More recently, variants of this concept have found use in co-processing units such as the MMX units in Intel processors and
DSP chips such as the Sharc. The Intel Pentium processor with its SSE (Streaming SIMD Extensions) provides a number of instructions
that execute the same instruction on multiple data items. These architectural enhancements rely on the highly structured (regular) nature
of the underlying computations, for example in image processing and graphics, to deliver improved performance.
Figure 2.3. A typical SIMD architecture (a) and a typical MIMD architecture (b).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
While the SIMD concept works well for structured computations on parallel data structures such as arrays, often it is necessary to
selectively turn off operations on certain data items. For this reason, most SIMD programming paradigms allow for an "activity mask". This
is a binary mask associated with each data item and operation that specifies whether it should participate in the operation or not.
 are used to support selective execution. Conditional execution can then <stmnt> <elsewhere stmnt> where (condition) Primitives such as
be detrimental to the performance of SIMD processors and therefore must be used with care.
In contrast to SIMD architectures, computers in which each processing element is capable of executing a different program independent of
 depicts a Figure 2.3(b)  (MIMD) computers. multiple instruction stream, multiple data stream the other processing elements are called
 (SPMD) model, relies on multiple single program multiple data typical MIMD computer. A simple variant of this model, called the
instances of the same program executing on different data. It is easy to see that the SPMD model has the same expressiveness as the
 block with conditions specified by the task if-else MIMD model since each of the multiple programs can be inserted into one large
identifiers. The SPMD model is widely used by many parallel platforms and requires minimal architectural support. Examples of such
platforms include the Sun Ultra Servers, multiprocessor PCs, workstation clusters, and the IBM SP.
SIMD computers require less hardware than MIMD computers because they have only one global control unit. Furthermore, SIMD
computers require less memory because only one copy of the program needs to be stored. In contrast, MIMD computers store the
program and operating system at each processor. However, the relative unpopularity of SIMD processors as general purpose compute
engines can be attributed to their specialized hardware architectures, economic factors, design constraints, product life-cycle, and
application characteristics. In contrast, platforms supporting the SPMD paradigm can be built from inexpensive off-the-shelf components
with relatively little effort in a short amount of time. SIMD computers require extensive design effort resulting in longer product
development times. Since the underlying serial processors change so rapidly, SIMD computers suffer from fast obsolescence. The
 illustrates a case in which SIMD Example 2.11 irregular nature of many applications also makes SIMD architectures less suitable.
architectures yield poor resource utilization in the case of conditional execution.
Example 2.11 Execution of conditional statements on a SIMD architecture
 is Figure 2.4(a) . The conditional statement in Figure 2.4 Consider the execution of a conditional statement illustrated in
. All other A  = C  equal to zero execute the instruction B executed in two steps. In the first step, all processors that have
) is executed. The processors that were B / A  = C processors are idle. In the second step, the 'else' part of the instruction (
active in the first step now become idle. This illustrates one of the drawbacks of SIMD architectures.
Figure 2.4. Executing a conditional statement on an SIMD computer with four processors: (a) the
conditional statement; (b) the execution of the statement in two steps.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
2.3.2 Communication Model of Parallel Platforms
. There are two primary forms of data exchange between parallel tasks – accessing a shared data space and exchanging messages
Shared-Address-Space Platforms
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The "shared-address-space" view of a parallel platform supports a common data space that is accessible to all processors. Processors
interact by modifying data objects stored in this shared-address-space. Shared-address-space platforms supporting SPMD programming
. Memory in shared-address-space platforms can be local (exclusive to a processor) or global multiprocessors are also referred to as
(common to all processors). If the time taken by a processor to access any memory word in the system (global or local) is identical, the
 (UMA) multicomputer. On the other hand, if the time taken to access certain memory uniform memory access platform is classified as a
 illustrate (b)  and 2.5(a)  (NUMA) multicomputer. Figures non-uniform memory access words is longer than others, the platform is called a
. Here, it is faster to Figure 2.5(b)  illustrates a NUMA platform. An interesting case is illustrated in Figure 2.5(c) UMA platforms, whereas
access a memory word in cache than a location in memory. However, we still classify this as a UMA architecture. The reason for this is
that all current microprocessors have cache hierarchies. Consequently, even a uniprocessor would not be termed UMA if cache access
times are considered. For this reason, we define NUMA and UMA architectures only in terms of memory access times and not cache
access times. Machines such as the SGI Origin 2000 and Sun Ultra HPC servers belong to the class of NUMA multiprocessors. The
distinction between UMA and NUMA platforms is important. If accessing local memory is cheaper than accessing global memory,
algorithms must build locality and structure data and computation accordingly.
Figure 2.5. Typical shared-address-space architectures: (a) Uniform-memory-access shared-address-space computer;
(b) Uniform-memory-access shared-address-space computer with caches and memories; (c)
Non-uniform-memory-access shared-address-space computer with local memory only.
The presence of a global memory space makes programming such platforms much easier. All read-only interactions are invisible to the
programmer, as they are coded no differently than in a serial program. This greatly eases the burden of writing parallel programs.
Read/write interactions are, however, harder to program than the read-only interactions, as these operations require mutual exclusion for
concurrent accesses. Shared-address-space programming paradigms such as threads (POSIX, NT) and directives (OpenMP) therefore
 and related mechanisms. locks support synchronization using
The presence of caches on processors also raises the issue of multiple copies of a single memory word being manipulated by two or more
processors at the same time. Supporting a shared-address-space in this context involves two major tasks: providing an address translation
mechanism that locates a memory word in the system, and ensuring that concurrent operations on multiple copies of the same memory
 mechanism. This mechanism and its cache coherence word have well-defined semantics. The latter is also referred to as the
. Supporting cache coherence requires considerable hardware support. Section 2.4.6 implementation are discussed in greater detail in
Consequently, some shared-address-space machines only support an address translation mechanism and leave the task of ensuring
. These put  and get coherence to the programmer. The native programming model for such platforms consists of primitives such as
primitives allow a processor to get (and put) variables stored at a remote processor. However, if one of the copies of this variable is
changed, the other copies are not automatically updated or invalidated.
d It is important to note the difference between two commonly used and often misunderstood terms – shared-address-space an
y shared-memory computers. The term shared-memory computer is historically used for architectures in which the memory is physicall
e shared among various processors, i.e., each processor has equal access to any memory segment. This is identical to the UMA model w
d just discussed. This is in contrast to a distributed-memory computer, in which different segments of the memory are physically associate
f with different processing elements. The dichotomy of shared- versus distributed-memory computers pertains to the physical organization o
. Either of these physical models, shared or distributed memory, can present Section 2.4 the machine and is discussed in greater detail in
the logical view of a disjoint or shared-address-space platform. A distributed-memory shared-address-space computer is identical to a
NUMA machine.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Message-Passing Platforms
h  processing nodes, each with its own exclusive address space. Eac p The logical machine view of a message-passing platform consists of
g of these processing nodes can either be single processors or a shared-address-space multiprocessor – a trend that is fast gainin
d momentum in modern message-passing parallel computers. Instances of such a view come naturally from clustered workstations an
e non-shared-address-space multicomputers. On such platforms, interactions between processes running on different nodes must b
. This exchange of messages is used to transfer data, work, and to message passing accomplished using messages, hence the name
synchronize actions among the processes. In its most general form, message-passing paradigms support execution of a different program
 nodes. p on each of the
 and send Since interactions are accomplished by sending and receiving messages, the basic operations in this programming paradigm are
 (the corresponding calls may differ across APIs but the semantics are largely identical). In addition, since the send and receive receive
operations must specify target addresses, there must be a mechanism to assign a unique identification or ID to each of the multiple
s , which return whoami processes executing a parallel program. This ID is typically made available to the program using a function such as
– to a calling process its ID. There is one other function that is typically needed to complete the basic set of message-passing operations
, which specifies the number of processes participating in the ensemble. With these four basic operations, it is possible to write numprocs
any message-passing program. Different message-passing APIs, such as the Message Passing Interface (MPI) and Parallel Virtual
Machine (PVM), support these basic operations and a variety of higher level functionality under different function names. Examples of
parallel platforms that support the message-passing paradigm include the IBM SP, SGI Origin 2000, and workstation clusters.
 nodes on a shared-address-space computer with an identical number of p It is easy to emulate a message-passing architecture containing
 disjoint parts and assigning one p nodes. Assuming uniprocessor nodes, this can be done by partitioning the shared-address-space into
such partition exclusively to each processor. A processor can then "send" or "receive" messages by writing to or reading from another
processor's partition while using appropriate synchronization primitives to inform its communication partner when it has finished reading or
writing the data. However, emulating a shared-address-space architecture on a message-passing computer is costly, since accessing
another node's memory requires sending and receiving messages.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.4 Physical Organization of Parallel Platforms
In this section, we discuss the physical architecture of parallel machines. We start with an ideal architecture, outline practical difficulties
associated with realizing this model, and discuss some conventional architectures.
2.4.1 Architecture of an Ideal Parallel Computer
 processors and a global p A natural extension of the serial model of computation (the Random Access Machine, or RAM) consists of
memory of unbounded size that is uniformly accessible to all processors. All processors access the same address space. Processors
parallel random share a common clock but may execute different instructions in each cycle. This ideal model is also referred to as a
. Since PRAMs allow concurrent access to various memory locations, depending on how simultaneous memory access machine (PRAM)
accesses are handled, PRAMs can be divided into four subclasses.
r  In this class, access to a memory location is exclusive. No concurrent read o Exclusive-read, exclusive-write (EREW) PRAM.
write operations are allowed. This is the weakest PRAM model, affording minimum concurrency in memory access.
. 1
.  In this class, multiple read accesses to a memory location are allowed Concurrent-read, exclusive-write (CREW) PRAM.
However, multiple write accesses to a memory location are serialized.
. 2
d  Multiple write accesses are allowed to a memory location, but multiple rea Exclusive-read, concurrent-write (ERCW) PRAM.
accesses are serialized.
. 3
y  This class allows multiple read and write accesses to a common memor Concurrent-read, concurrent-write (CRCW) PRAM.
location. This is the most powerful PRAM model.
. 4
Allowing concurrent read access does not create any semantic discrepancies in the program. However, concurrent write access to a
memory location requires arbitration. Several protocols are used to resolve concurrent writes. The most frequently used protocols are as
follows:
. , in which the concurrent write is allowed if all the values that the processors are attempting to write are identical Common
. , in which an arbitrary processor is allowed to proceed with the write operation and the rest fail Arbitrary
y , in which all processors are organized into a predefined prioritized list, and the processor with the highest priorit Priority
succeeds and the rest fail.
y , in which the sum of all the quantities is written (the sum-based write conflict resolution model can be extended to an Sum
associative operator defined on the quantities being written).
Architectural Complexity of the Ideal Model
 words. The m  processors and a global memory of p Consider the implementation of an EREW PRAM as a shared-memory computer with
processors are connected to the memory through a set of switches. These switches determine the memory word being accessed by each
 processors in the ensemble can access any of the memory words, provided that a word is p processor. In an EREW PRAM, each of the
). (See mp ( Q not accessed by more than one processor simultaneously. To ensure such connectivity, the total number of switches must be
 notation.) For a reasonable memory size, constructing a switching network of this complexity is Q the Appendix for an explanation of the
very expensive. Thus, PRAM models of computation are impossible to realize in practice.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
2.4.2 Interconnection Networks for Parallel Computers
Interconnection networks provide mechanisms for data transfer between processing nodes or between processors and memory modules.
 outputs. The outputs may or may not be distinct from the inputs. m  inputs and n A blackbox view of an interconnection network consists of
Typical interconnection networks are built using links and switches. A link corresponds to physical media such as a set of wires or fibers
capable of carrying information. A variety of factors influence link characteristics. For links based on conducting media, the capacitive
coupling between wires limits the speed of signal propagation. This capacitive coupling and attenuation of signal strength are functions of
the length of the link.
. Static networks consist of point-to-point communication links among dynamic  or static Interconnection networks can be classified as
 networks. Dynamic networks, on the other hand, are built using switches and direct processing nodes and are also referred to as
communication links. Communication links are connected to one another dynamically by the switches to establish paths among
 illustrates a simple static Figure 2.6(a)  networks. indirect processing nodes and memory banks. Dynamic networks are also referred to as
network of four processing elements or nodes. Each processing node is connected via a network interface to two other nodes in a mesh
 illustrates a dynamic network of four nodes connected via a network of switches to other nodes. Figure 2.6(b) configuration.
Figure 2.6. Classification of interconnection networks: (a) a static network; and (b) a dynamic network.
A single switch in an interconnection network consists of a set of input ports and a set of output ports. Switches provide a range of
functionality. The minimal functionality provided by a switch is a mapping from the input to the output ports. The total number of ports on a
 of the switch. Switches may also provide support for internal buffering (when the requested output port is degree switch is also called the
busy), routing (to alleviate congestion on the network), and multicast (same output on multiple ports). The mapping from input to output
ports can be provided using a variety of mechanisms based on physical crossbars, multi-ported memories, multiplexor-demultiplexors, and
multiplexed buses. The cost of a switch is influenced by the cost of the mapping hardware, the peripheral hardware and packaging costs.
The mapping hardware typically grows as the square of the degree of the switch, the peripheral hardware linearly as the degree, and the
packaging costs linearly as the number of pins.
The connectivity between the nodes and the network is provided by a network interface. The network interface has input and output ports
that pipe data into and out of the network. It typically has the responsibility of packetizing data, computing routing information, buffering
incoming and outgoing data for matching speeds of network and processing elements, and error checking. The position of the interface
between the processing element and the network is also important. While conventional network interfaces hang off the I/O buses,
interfaces in tightly coupled parallel machines hang off the memory bus. Since I/O buses are typically slower than memory buses, the
latter can support higher bandwidth.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
2.4.3 Network Topologies
A wide variety of network topologies have been used in interconnection networks. These topologies try to trade off cost and scalability with
performance. While pure topologies have attractive mathematical properties, in practice interconnection networks tend to be combinations
or modifications of the pure topologies discussed in this section.
Bus-Based Networks
A bus-based network is perhaps the simplest network consisting of a shared medium that is common to all the nodes. A bus has the
. This cost is typically associated with bus p desirable property that the cost of the network scales linearly as the number of nodes,
(1)). Buses are also ideal for broadcasting O interfaces. Furthermore, the distance between any two nodes in the network is constant (
information among nodes. Since the transmission medium is shared, there is little overhead associated with broadcast compared to
point-to-point message transfer. However, the bounded bandwidth of a bus places limitations on the overall performance of the network as
the number of nodes increases. Typical bus based machines are limited to dozens of nodes. Sun Enterprise servers and Intel Pentium
based shared-bus multiprocessors are examples of such architectures.
The demands on bus bandwidth can be reduced by making use of the property that in typical programs, a majority of the data accessed is
local to the node. For such programs, it is possible to provide a cache for each node. Private data is cached at the node and only remote
data is accessed through the bus.
Example 2.12 Reducing shared-bus bandwidth using caches
 data items, k  processors sharing a bus to the memory. Assuming that each processor accesses p  illustrates Figure 2.7(a)
 seconds. Now consider the kp  x cyclet , the execution time is lower bounded by cyclet and each data access takes time
) are made to local data. k . Let us assume that 50% of the memory accesses (0.5 Figure 2.7(b) hardware organization of
This local data resides in the private memory of the processor. We assume that access time to the private memory is
 + 0.5 x k  x cyclet . In this case, the total execution time is lower bounded by 0.5 x cyclet identical to the global memory, i.e.,
. Here, the first term results from accesses to local data and the second term from access to shared data. It is kp  x cyclet
 x cyclet  results in a lower bound that approaches 0.5 x Figure 2.7(b)  becomes large, the organization of p easy to see that as
. Figure 2.7(a) . This time is a 50% improvement in lower bound on execution time compared to the organization of kp
Figure 2.7. Bus-based interconnects (a) with no local caches; (b) with local memory/caches.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In practice, shared and private data is handled in a more sophisticated manner. This is briefly addressed with cache coherence issues in
. Section 2.4.6
Crossbar Networks
 memory banks is to use a crossbar network. A crossbar network employs a grid of switches or b  processors to p A simple way to connect
. The crossbar network is a non-blocking network in the sense that the connection of a processing Figure 2.8 switching nodes as shown in
node to a memory bank does not block the connection of any other processing nodes to other memory banks.
 memory banks. b  processors to p Figure 2.8. A completely non-blocking crossbar network connecting
). It is reasonable to assume that the number of pb ( Q The total number of switching nodes required to implement such a network is
; otherwise, at any given time, there will be some processing nodes that will be unable to access any memory p  is at least b memory banks
). (See the 2 p ( W  is increased, the complexity (component count) of the switching network grows as p banks. Therefore, as the value of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 notation.) As the number of processing nodes becomes large, this switch complexity is difficult to W Appendix for an explanation of the
realize at high data rates. Consequently, crossbar networks are not very scalable in terms of cost.
Multistage Networks
The crossbar interconnection network is scalable in terms of performance but unscalable in terms of cost. Conversely, the shared bus
multistage network is scalable in terms of cost but unscalable in terms of performance. An intermediate class of networks called
 lies between these two extremes. It is more scalable than the bus in terms of performance and more scalable interconnection networks
than the crossbar in terms of cost.
. A commonly Figure 2.9  memory banks is shown in b  processing nodes and p The general schematic of a multistage network consisting of
 is the number of inputs p  stages, where p . This network consists of log omega network used multistage connection network is the
(processing nodes) and also the number of outputs (memory banks). Each stage of the omega network consists of an interconnection
 if the following is true: j  and outputi  outputs; a link exists between input p  inputs and p pattern that connects
1  Equation 2.
Figure 2.9. The schematic of a typical multistage interconnection network.
perfect . This interconnection pattern is called a j  to obtaini  represents a left-rotation operation on the binary representation of Equation 2.1
 shows a perfect shuffle interconnection pattern for eight inputs and outputs. At each stage of an omega network, a Figure 2.10 . shuffle
/2 switches or switching nodes. Each switch is in one of two connection modes. p perfect shuffle interconnection pattern feeds into a set of
 connection. pass-through . This is called the Figure 2.11(a) In one mode, the inputs are sent straight through to the outputs, as shown in
. This is called the Figure 2.11(b) In the other mode, the inputs to the switching node are crossed over and then sent out, as shown in
 connection. cross-over
Figure 2.10. A perfect shuffle interconnection for eight inputs and outputs.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 2.11. Two switching configurations of the 2 x 2 switch: (a) Pass-through; (b) Cross-over.
). Note that this cost is less than the p  log p ( Q  switching nodes, and the cost of such a network grows as p /2 x log p An omega network has
 shows an omega network for eight processors (denoted by the binary numbers on Figure 2.12 ) cost of a complete crossbar network. 2 p ( Q
the left) and eight memory banks (denoted by the binary numbers on the right). Routing data in an omega network is accomplished using a
. The data traverses the t  be the binary representation of a processor that needs to write some data into memory bank s simple scheme. Let
 are the same, then the data is routed in pass-through mode by thet  and s link to the first switching node. If the most significant bits of
switch. If these bits are different, then the data is routed through in crossover mode. This scheme is repeated at the next switching stage
. t  and s  bits in the binary representations of p  stages uses all log p using the next most significant bit. Traversing log
Figure 2.12. A complete omega network connecting eight inputs and eight outputs.
 shows data routing over an omega network from processor two (010) to memory bank seven (111) and from processor six Figure 2.13
(110) to memory bank four (100). This figure also illustrates an important property of this network. When processor two (010) is
communicating with memory bank seven (111), it blocks the path from processor six (110) to memory bank four (100). Communication link
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
AB is used by both communication paths. Thus, in an omega network, access to a memory bank by a processor may disallow access to
. blocking networks another memory bank by another processor. Networks with this property are referred to as
Figure 2.13. An example of blocking in omega network: one of the messages (010 to 111 or 110 to 100) is blocked at
link AB.
Completely-Connected Network
Figure 2.14(a) , each node has a direct communication link to every other node in the network. completely-connected network In a
illustrates a completely-connected network of eight nodes. This network is ideal in the sense that a node can send a message to another
node in a single step, since a communication link exists between them. Completely-connected networks are the static counterparts of
crossbar switching networks, since in both networks, the communication between any input/output pair does not block communication
between any other pair.
Figure 2.14. (a) A completely-connected network of eight nodes; (b) a star connected network of nine nodes.
Star-Connected Network
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, one processor acts as the central processor. Every other processor has a communication link connecting it star-connected network In a
 shows a star-connected network of nine processors. The star-connected network is similar to bus-based Figure 2.14(b) to this processor.
networks. Communication between any pair of processors is routed through the central processor, just as the shared bus forms the
medium for all communication in a bus-based network. The central processor is the bottleneck in the star topology.
 Meshes d - k Linear Arrays, Meshes, and
Due to the large number of links in completely connected networks, sparser networks are typically used to build parallel computers. A
family of such networks spans the space of linear arrays and hypercubes. A linear array is a static network in which each node (except the
) is the ring or Figure 2.15(a) two nodes at the ends) has two neighbors, one each to its left and right. A simple extension of the linear array (
). The ring has a wraparound connection between the extremities of the linear array. In this case, each node has Figure 2.15(b) a 1-D torus (
two neighbors.
Figure 2.15. Linear arrays: (a) with no wraparound links; (b) with wraparound link.
 is an extension of the linear array to two-dimensions. Each dimension has Figure 2.16(a) A two-dimensional mesh illustrated in
). Every node (except those on the periphery) is connected to four other nodes whose j , i nodes with a node identified by a two-tuple (
indices differ in any dimension by one. A 2-D mesh has the property that it can be laid out in 2-D space, making it attractive from a wiring
standpoint. Furthermore, a variety of regularly structured computations map very naturally to a 2-D mesh. For this reason, 2-D meshes
were often used as interconnects in parallel machines. Two dimensional meshes can be augmented with wraparound links to form two
. The three-dimensional cube is a generalization of the 2-D mesh to three dimensions, as Figure 2.16(b) dimensional tori illustrated in
. Each node element in a 3-D cube, with the exception of those on the periphery, is connected to six other Figure 2.16(c) illustrated in
nodes, two along each of the three dimensions. A variety of physical simulations commonly executed on parallel computers (for example,
3-D weather modeling, structural modeling, etc.) can be mapped naturally to 3-D network topologies. For this reason, 3-D cubes are used
commonly in interconnection networks for parallel computers (for example, in the Cray T3E).
Figure 2.16. Two and three dimensional meshes: (a) 2-D mesh with no wraparound; (b) 2-D mesh with wraparound link
(2-D torus); and (c) a 3-D mesh with no wraparound.
 nodes along each dimension. Just as a k  dimensions with d  meshes refers to the class of topologies consisting of d - k The general class of
 mesh family, the other extreme is formed by an interesting topology called the hypercube. The d - k linear array forms one extreme of the
Figure  dimensions. The construction of a hypercube is illustrated in p hypercube topology has two nodes along each dimension and log
, i.e., one node. A one-dimensional hypercube is constructed from two zero-dimensional 0 . A zero-dimensional hypercube consists of 2 2.17
hypercubes by connecting them. A two-dimensional hypercube of four nodes is constructed from two one-dimensional hypercubes by
 - 1) d -dimensional hypercube is constructed by connecting corresponding nodes of two ( d connecting corresponding nodes. In general a
 illustrates this for up to 16 nodes in a 4-D hypercube. Figure 2.17 dimensional hypercubes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 2.17. Construction of hypercubes from hypercubes of lower dimension.
It is useful to derive a numbering scheme for nodes in a hypercube. A simple numbering scheme can be derived from the construction of a
/2 nodes, we can derive a numbering scheme for the p , if we have a numbering of two subcubes of Figure 2.17 hypercube. As illustrated in
 nodes by prefixing the labels of one of the subcubes with a "0" and the labels of the other subcube with a "1". This numbering p cube of
scheme has the useful property that the minimum distance between two nodes is given by the number of bits that are different in the two
labels. For example, nodes labeled 0110 and 0101 are two links apart, since they differ at two bit positions. This property is useful for
deriving a number of parallel algorithms for the hypercube architecture.
Tree-Based Networks
 is one in which there is only one path between any pair of nodes. Both linear arrays and star-connected networks are tree network A
 shows networks based on complete binary trees. Static tree networks have a processing Figure 2.18 special cases of tree networks.
). Tree networks also have a dynamic counterpart. In a dynamic tree network, nodes at Figure 2.18(a) element at each node of the tree (
). Figure 2.18(b) intermediate levels are switching nodes and the leaf nodes are processing elements (
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 2.18. Complete binary tree networks: (a) a static tree network; and (b) a dynamic tree network.
To route a message in a tree, the source node sends the message up the tree until it reaches the node at the root of the smallest subtree
containing both the source and destination nodes. Then the message is routed down the tree towards the destination node.
Tree networks suffer from a communication bottleneck at higher levels of the tree. For example, when many nodes in the left subtree of a
node communicate with nodes in the right subtree, the root node must handle all the messages. This problem can be alleviated in dynamic
, fat tree tree networks by increasing the number of communication links and switching nodes closer to the root. This network, also called a
. Figure 2.19 is illustrated in
Figure 2.19. A fat tree network of 16 processing nodes.
2.4.4 Evaluating Static Interconnection Networks
We now discuss various criteria used to characterize the cost and performance of static interconnection networks. We use these criteria to
evaluate static networks introduced in the previous subsection.
 of a network is the maximum distance between any two processing nodes in the network. The distance between diameter  The Diameter
two processing nodes is defined as the shortest path (in terms of number of links) between them. The diameter of a completely-connected
. The diameter of a network is one, and that of a star-connected network is two. The diameter of a ring network is
 for the two nodes at diagonally opposed corners, and that of a two-dimensional mesh without wraparound connections is
. The diameter of a hypercube-connected network is log p since two node labels can differ in at most wraparound mesh is
 + 1)/2) because the two communicating nodes may be in separate p  positions. The diameter of a complete binary tree is 2 log(( p log
subtrees of the root node, and a message might have to travel all the way to the root and then down the other subtree.
 of a network is a measure of the multiplicity of paths between any two processing nodes. A network with connectivity  The Connectivity
high connectivity is desirable, because it lowers contention for communication resources. One measure of connectivity is the minimum
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 of arc connectivity number of arcs that must be removed from the network to break it into two disconnected networks. This is called the
the network. The arc connectivity is one for linear arrays, as well as tree and star networks. It is two for rings and 2-D meshes without
-dimensional hypercubes. d  for d wraparound, four for 2-D wraparound meshes, and
 of a network is defined as the minimum number of communication links bisection width  The Bisection Width and Bisection Bandwidth
that must be removed to partition the network into two equal halves. The bisection width of a ring is two, since any partition cuts across
-node mesh without wraparound connections is p only two communication links. Similarly, the bisection width of a two-dimensional
 The bisection width of a tree and a star is one, and that of a completely-connected network of. and with wraparound connections is
-dimensional hypercube by d /4. The bisection width of a hypercube can be derived from its construction. We construct a 2 p  nodes is p
/2 nodes, at least p  or -1) d (  - 1)-dimensional hypercubes. Since each of these subcubes contains 2 d connecting corresponding links of two (
/2 communication links must cross any partition of a hypercube into two subcubes (Problem 2.15). p
. Channel channel width The number of bits that can be communicated simultaneously over a link connecting two nodes is called the
width is equal to the number of physical wires in each communication link. The peak rate at which a single physical wire can deliver bits is
channel . The peak rate at which data can be communicated between the ends of a communication link is called channel rate called the
. Channel bandwidth is the product of channel rate and channel width. bandwidth
 nodes. p Table 2.1. A summary of the characteristics of various static network topologies connecting
Cost (No. of links) Arc Connectivity Bisection Width Diameter Network
 - 1)/2 p ( p  - 1 p /4 2 p 1 Completely-connected
 - 1 p 1 1 2 Star
 - 1 p 1 1  + 1)/2) p 2 log(( Complete binary tree
 - 1 p 1 1  - 1 p Linear array
2 2-D mesh, no wraparound
2p 4 2-D wraparound mesh
)/2 p  log p ( p log /2 p p log Hypercube
dp d 2 -1 d k 2 -cube d -ary k Wraparound
 of a network is defined as the minimum volume of communication allowed between any two halves of the bisection bandwidth The
network. It is the product of the bisection width and the channel bandwidth. Bisection bandwidth of a network is also sometimes referred to
. cross-section bandwidth as
f  Many criteria can be used to evaluate the cost of a network. One way of defining the cost of a network is in terms of the number o Cost
 nodes. A p  - 1 links to connect p communication links or the number of wires required by the network. Linear arrays and trees use only
)/2 links. p  log p  links. A hypercube-connected network has ( dp -dimensional wraparound mesh has d
The bisection bandwidth of a network can also be used as a measure of its cost, as it provides a lower bound on the area in a
two-dimensional packaging or the volume in a three-dimensional packaging. If the bisection width of a network is w, the lower bound on
). 3/2 w ( Q ), and the lower bound on the volume in a three-dimensional packaging is 2 w ( Q the area in a two-dimensional packaging is
According to this criterion, hypercubes and completely connected networks are more expensive than the other networks.
, which highlights the various cost-performance tradeoffs. Table 2.1 We summarize the characteristics of various static networks in
2.4.5 Evaluating Dynamic Interconnection Networks
A number of evaluation metrics for dynamic networks follow from the corresponding metrics for static networks. Since a message
traversing a switch must pay an overhead, it is logical to think of each switch as a node in the network, in addition to the processing nodes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The diameter of the network can now be defined as the maximum distance between any two nodes in the network. This is indicative of the
maximum delay that a message will encounter in being communicated between the selected pair of nodes. In reality, we would like the
metric to be the maximum distance between any two processing nodes; however, for all networks of interest, this is equivalent to the
maximum distance between any (processing or switching) pair of nodes.
The connectivity of a dynamic network can be defined in terms of node or edge connectivity. The node connectivity is the minimum
number of nodes that must fail (be removed from the network) to fragment the network into two parts. As before, we should consider only
switching nodes (as opposed to all nodes). However, considering all nodes gives a good approximation to the multiplicity of paths in a
dynamic network. The arc connectivity of the network can be similarly defined as the minimum number of edges that must fail (be removed
from the network) to fragment the network into two unreachable parts.
The bisection width of a dynamic network must be defined more precisely than diameter and connectivity. In the case of bisection width,
 processing nodes into two equal parts. Note that this does not restrict the partitioning of the p we consider any possible partitioning of the
switching nodes. For each such partition, we select an induced partitioning of the switching nodes such that the number of edges crossing
this partition is minimized. The minimum number of edges for any such partition is the bisection width of the dynamic network. Another
intuitive way of thinking of bisection width is in terms of the minimum number of edges that must be removed from the network so as to
partition the network into two halves with identical number of processing nodes. We illustrate this concept further in the following example:
Example 2.13 Bisection width of dynamic networks
. We illustrate here three bisections, A, B, and C, each of which partitions Figure 2.20 Consider the network illustrated in
the network into two groups of two processing nodes each. Notice that these partitions need not partition the network
nodes equally. In the example, each partition results in an edge cut of four. We conclude that the bisection width of this
graph is four.
Figure 2.20. Bisection width of a dynamic network is computed by examining various equi-partitions of
the processing nodes and selecting the minimum number of edges crossing the partition. In this case,
each partition yields an edge cut of four. Therefore, the bisection width of this graph is four.
The cost of a dynamic network is determined by the link cost, as is the case with static networks, as well as the switch cost. In typical
dynamic networks, the degree of a switch is constant. Therefore, the number of links and switches is asymptotically identical. Furthermore,
in typical networks, switch cost exceeds link cost. For this reason, the cost of dynamic networks is often determined by the number of
switching nodes in the network.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Table 2.2 We summarize the characteristics of various dynamic networks in
2.4.6 Cache Coherence in Multiprocessor Systems
While interconnection networks provide basic mechanisms for communicating messages (data), in the case of shared-address-space
computers additional hardware is required to keep multiple copies of data consistent with each other. Specifically, if there exist two copies
of the data (in different caches/memory elements), how do we ensure that different processors operate on these in a manner that follows
predefined semantics?
 processing nodes. p Table 2.2. A summary of the characteristics of various dynamic network topologies connecting
Cost (No. of links) Arc Connectivity Bisection Width Diameter Network
2 p 1 p 1 Crossbar
/2 p 2 /2 p p log Omega Network
 - 1 p 2 1 p 2 log Dynamic Tree
The problem of keeping caches in multiprocessor systems coherent is significantly more complex than in uniprocessor systems. This is
because in addition to multiple copies as in uniprocessor systems, there may also be multiple processors modifying these copies.
 are connected over a shared bus to a globally accessible 1 P  and 0 P . Two processors Figure 2.21 Consider a simple scenario illustrated in
memory. Both processors load the same variable. There are now three copies of the variable. The coherence mechanism must now
ensure that all operations performed on these copies are serializable (i.e., there exists some serial order of instruction execution that
corresponds to the parallel schedule). When a processor changes the value of its copy of the variable, one of two things must happen: the
other copies must be invalidated, or the other copies must be updated. Failing this, other processors may potentially work with incorrect
Figure 2.21(a)  protocols and are illustrated in update  and invalidate (stale) values of the variable. These two protocols are referred to as
. (b) and
Figure 2.21. Cache coherence in multiprocessor systems: (a) Invalidate protocol; (b) Update protocol for shared
variables.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In an update protocol, whenever a data item is written, all of its copies in the system are updated. For this reason, if a processor simply
reads a data item once and never uses it, subsequent updates to this item at other processors cause excess overhead in terms of latency
at source and bandwidth on the network. On the other hand, in this situation, an invalidate protocol invalidates the data item on the first
update at a remote processor and subsequent updates need not be performed on this copy.
. False sharing refers to the situation in which false sharing Another important factor affecting the performance of these protocols is
different processors update different parts of of the same cache-line. Thus, although the updates are not performed on shared variables,
the system does not detect this. In an invalidate protocol, when a processor updates its part of the cache-line, the other copies of this line
are invalidated. When other processors try to update their parts of the cache-line, the line must actually be fetched from the remote
processor. It is easy to see that false-sharing can cause a cache-line to be ping-ponged between various processors. In an update
protocol, this situation is slightly better since all reads can be performed locally and the writes must be updated. This saves an invalidate
operation that is otherwise wasted.
The tradeoff between invalidate and update schemes is the classic tradeoff between communication overhead (updates) and idling
(stalling in invalidates). Current generation cache coherent machines typically rely on invalidate protocols. The rest of our discussion of
multiprocessor cache systems therefore assumes invalidate protocols.
e  Multiple copies of a single data item are kept consistent by keeping track of th Maintaining Coherence Using Invalidate Protocols
number of copies and the state of each of these copies. We discuss here one possible set of states associated with data items and events
that trigger transitions among these states. Note that this set of states and transitions is not unique. It is possible to define other states and
associated transitions as well.
 resides in the global memory. The first step executed by both processors is x . Initially the variable Figure 2.21 Let us revisit the example in
, since it is shared by multiple processors. shared a load operation on this variable. At this point, the state of the variable is said to be
. It must also mark its own copy as invalid  executes a store on this variable, it marks all other copies of this variable as 0 P When processor
0 P . This is done to ensure that all subsequent accesses to this variable at other processors will be serviced by processor dirty modified or
 attempts to fetch this 1 P  . Processor x  executes another load operation on 1 P and not from the memory. At this point, say, processor
 services the request. Copies of this variable at processor 0 P , processor 0 P variable and, since the variable was marked dirty by processor
 and the global memory are updated and the variable re-enters the shared state. Thus, in this simple model, there are three states - 1 P
 - that a cache line goes through. dirty , and invalid , shared
. The solid lines depict processor actions and the Figure 2.22 The complete state diagram of a simple three-state protocol is illustrated in
dashed lines coherence actions. For example, when a processor executes a read on an invalid block, the block is fetched and a transition
is made from invalid to shared. Similarly, if a processor does a write on a shared block, the coherence protocol propagates a C_write (a
coherence write) on the block. This triggers a transition from shared to invalid at all the other blocks.
Figure 2.22. State diagram of a simple three-state coherence protocol.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Example 2.14 Maintaining coherence using a simple three-state protocol
. The Figure 2.23  as illustrated in 1 P  and 0 P Consider an example of two program segments being executed by processor
, and a global memory. The three-state protocol 1 P  and 0 P system consists of local memories (or caches) at processors
. Cache lines in this system can be Figure 2.22 assumed in this example corresponds to the state diagram illustrated in
either shared, invalid, or dirty. Each data item (variable) is assumed to be on a different cache line. Initially, the two
 illustrates Figure 2.23  are tagged dirty and the only copies of these variables exist in the global memory. y  and x variables
state transitions along with values of copies of the variables with each instruction execution.
Figure 2.23. Example of parallel program execution with the simple three-state coherence protocol
. Section 2.4.6 discussed in
y The implementation of coherence protocols can be carried out using a variety of hardware mechanisms – snoopy systems, director
. based systems, or combinations thereof
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Snoopy Cache Systems
Snoopy caches are typically associated with multiprocessor systems based on broadcast interconnection networks such as a bus or a ring.
In such systems, all processors snoop on (monitor) the bus for transactions. This allows the processor to make state transitions for its
 illustrates a typical snoopy bus based system. Each processor's cache has a set of tag bits associated with it Figure 2.24 cache-blocks.
that determine the state of the cache blocks. These tags are updated according to the state diagram associated with the coherence
protocol. For instance, when the snoop hardware detects that a read has been issued to a cache block that it has a dirty copy of, it asserts
control of the bus and puts the data out. Similarly, when the snoop hardware detects that a write operation has been issued on a cache
block that it has a copy of, it invalidates the block. Other state transitions are made in this fashion locally.
Figure 2.24. A simple snoopy bus based cache coherence system.
y  Snoopy protocols have been extensively studied and used in commercial systems. This is largel Performance of Snoopy Caches
because of their simplicity and the fact that existing bus based systems can be upgraded to accommodate snoopy protocols. The
performance gains of snoopy systems are derived from the fact that if different processors operate on different data items, these items can
be cached. Once these items are tagged dirty, all subsequent operations can be performed locally on the cache without generating
external traffic. Similarly, if a data item is read by a number of processors, it transitions to the shared state in the cache and all subsequent
read operations become local. In both cases, the coherence protocol does not add any overhead. On the other hand, if multiple processors
read and update the same data item, they generate coherence functions across processors. Since a shared bus has a finite bandwidth,
only a constant number of such coherence operations can execute in unit time. This presents a fundamental bottleneck for snoopy bus
based systems.
Snoopy protocols are intimately tied to multicomputers based on broadcast networks such as buses. This is because all processors must
snoop all the messages. Clearly, broadcasting all of a processor's memory operations to all the processors is not a scalable solution. An
obvious solution to this problem is to propagate coherence operations only to those processors that must participate in the operation (i.e.,
processors that have relevant copies of the data). This solution requires us to keep track of which processors have copies of various data
items and also the relevant state information for these data items. This information is stored in a directory, and the coherence mechanism
based on such information is called a directory-based system.
Directory Based Systems
Consider a simple system in which the global memory is augmented with a directory that maintains a bitmap representing cache-blocks
. As presence bits ). These bitmap entries are sometimes referred to as the Figure 2.25 and the processors at which they are cached (
. The key to the performance of directory based shared , and dirty , invalid before, we assume a three-state protocol with the states labeled
schemes is the simple observation that only processors that hold a particular block (or are reading it) participate in the state transitions due
to coherence operations. Note that there may be other state transitions triggered by processor read, write, or flush (retiring a line from
cache) but these transitions can be handled locally with the operation reflected in the presence bits and state in the directory.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 2.25. Architecture of typical directory based systems: (a) a centralized directory; and (b) a distributed directory.
 , the state of the x  access the block corresponding to variable 1 P  and 0 P , when processors Figure 2.21 Revisiting the code segment in
 executes a 0 P  share the block. When 1 P  and 0 P block is changed to shared, and the presence bits updated to indicate that processors
 is reset. All subsequent operations on this 1 P store on the variable, the state in the directory is changed to dirty and the presence bit of
 can proceed locally. If another processor reads the value, the directory notices the dirty tag and uses 0 P variable performed at processor
 updates the block in the memory, and sends it to the 0 P the presence bits to direct the request to the appropriate processor. Processor
requesting processor. The presence bits are modified to reflect this and the state transitions to shared.
,  As is the case with snoopy protocols, if different processors operate on distinct data blocks Performance of Directory Based Schemes
these blocks become dirty in the respective caches and all operations after the first one can be performed locally. Furthermore, if multiple
processors read (but do not update) a single data block, the data block gets replicated in the caches in the shared state and subsequent
reads can happen without triggering any coherence overheads.
Coherence actions are initiated when multiple processors attempt to update the same data item. In this case, in addition to the necessary
data movement, coherence operations add to the overhead in the form of propagation of state updates (invalidates or updates) and
generation of state information from the directory. The former takes the form of communication overhead and the latter adds contention.
The communication overhead is a function of the number of processors requiring state updates and the algorithm for propagating state
information. The contention overhead is more fundamental in nature. Since the directory is in memory and the memory system can only
service a bounded number of read/write operations in unit time, the number of state updates is ultimately bounded by the directory. If a
parallel program requires a large number of coherence actions (large number of read/write shared data blocks) the directory will ultimately
bound its parallel performance.
Finally, from the point of view of cost, the amount of memory required to store the directory may itself become a bottleneck as the number
 the number of p  is the number of memory blocks and m ), where mp ( O of processors increases. Recall that the directory size grows as
 for a given memory size). However, this adds to m processors. One solution would be to make the memory block larger (thus reducing
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
other overheads such as false sharing, where two processors update distinct data items in a program but the data items happen to lie in
. Chapter 7 the same memory block. This phenomenon is discussed in greater detail in
Since the directory forms a central point of contention, it is natural to break up the task of maintaining coherence across multiple
processors. The basic principle is to let each processor maintain coherence of its own memory blocks, assuming a physical (or logical)
partitioning of the memory blocks across processors. This is the principle of a distributed directory system.
g  In scalable architectures, memory is physically distributed across processors. The correspondin Distributed Directory Schemes
presence bits of the blocks are also distributed. Each processor is responsible for maintaining the coherence of its own memory blocks.
. Since each memory block has an owner (which can typically be Figure 2.25(b) The architecture of such a system is illustrated in
computed from the block address), its directory location is implicitly known to all processors. When a processor attempts to read a block
for the first time, it requests the owner for the block. The owner suitably directs this request based on presence and state information
locally available. Similarly, when a processor writes into a memory block, it propagates an invalidate to the owner, which in turn forwards
the invalidate to all processors that have a cached copy of the block. In this way, the directory is decentralized and the contention
associated with the central directory is alleviated. Note that the communication overhead associated with state update messages is not
reduced.
) simultaneous coherence operations, p ( O  As is evident, distributed directories permit Performance of Distributed Directory Schemes
provided the underlying network can sustain the associated state update messages. From this point of view, distributed directories are
inherently more scalable than snoopy systems or centralized directory systems. The latency and bandwidth of the network become
fundamental performance bottlenecks for such systems.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.5 Communication Costs in Parallel Machines
One of the major overheads in the execution of parallel programs arises from communication of information between processing elements.
The cost of communication is dependent on a variety of features including the programming model semantics, the network topology, data
handling and routing, and associated software protocols. These issues form the focus of our discussion here.
2.5.1 Message Passing Costs in Parallel Computers
The time taken to communicate a message between two nodes in a network is the sum of the time to prepare a message for transmission
and the time taken by the message to traverse the network to its destination. The principal parameters that determine the communication
latency are as follows:
s ): The startup time is the time required to handle a message at the sending and receiving nodes. This include st  ( Startup time
the time to prepare the message (adding header, trailer, and error correction information), the time to execute the routing
algorithm, and the time to establish an interface between the local node and the router. This delay is incurred only once for a
single message transfer.
. 1
e ): After a message leaves a node, it takes a finite amount of time to reach the next node in its path. The tim ht  ( Per-hop time
taken by the header of a message to travel between two directly-connected nodes in the network is called the per-hop time. It is
. The per-hop time is directly related to the latency within the routing switch for determining which node latency also known as
output buffer or channel the message should be forwarded to.
. 2
 to traverse the r  = 1/ wt  words per second, then each word takes time r ): If the channel bandwidth is wt  ( Per-word transfer time
link. This time is called the per-word transfer time. This time includes network as well as buffering overheads.
. 3
. We now discuss two routing techniques that have been used in parallel computers – store-and-forward routing and cut-through routing
Store-and-Forward Routing
In store-and-forward routing, when a message is traversing a path with multiple links, each intermediate node on the path forwards the
 shows the communication of a message Figure 2.26(a) message to the next node after it has received and stored the entire message.
through a store-and-forward network.
 (a) through a store-and-forward 3 P  to 0 P Figure 2.26. Passing a message from node
communication network; (b) and (c) extending the concept to cut-through routing. The shaded
regions represent the time that the message is in transit. The startup time associated with this
message transfer is assumed to be zero.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 links. At each link, the messagel  is being transmitted through such a network. Assume that it traverses m Suppose that a message of size
 + ht  such links, the total time is (l  for the rest of the message to traverse the link. Since there are m wt  for the header and ht incurs a cost
 communicationl  words to traverse m . Therefore, for store-and-forward routing, the total communication cost for a message of size l ) m wt
links is
2  Equation 2.
m  even for small values of m wt  is quite small. For most parallel algorithms, it is less than ht In current parallel computers, the per-hop time
 can be simplified to Equation 2.2 and thus can be ignored. For parallel platforms using store-and-forward routing, the time given by
Packet Routing
Store-and-forward routing makes poor use of communication resources. A message is sent from one node to the next only after the entire
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, in which the original message is broken into Figure 2.26(b) ). Consider the scenario shown in Figure 2.26(a) message has been received (
two equal sized parts before it is sent. In this case, an intermediate node waits for only half of the original message to arrive before
. Figure 2.26(b) passing it on. The increased utilization of communication resources and reduced communication time is apparent from
s  goes a step further and breaks the message into four parts. In addition to better utilization of communication resources, thi Figure 2.26(c)
r principle offers other advantages – lower overhead from packet loss (errors), possibility of packets taking different paths, and better erro
e correction capability. For these reasons, this technique is the basis for long-haul communication networks such as the Internet, wher
y error rates, number of hops, and variation in network state can be higher. Of course, the overhead here is that each packet must carr
. routing, error correction, and sequencing information
 word message through the network. The time taken for programming the network interfaces and computing m Consider the transfer of an
 of the message transfer. We st the routing information, etc., is independent of the message length. This is aggregated into the startup time
assume a scenario in which routing tables are static over the time of message transfer (i.e., all packets traverse the same path). While this
is not a valid assumption under all circumstances, it serves the purpose of motivating a cost model for message transfer. The message is
, s  + r broken into packets, and packets are assembled with their error, routing, and sequencing fields. The size of a packet is now given by
 is the additional information carried in the packet. The time for packetizing the message is s  is the original message and r where
w2t . If the network is capable of communicating one word every w1 mt proportional to the length of the message. We denote this time by
) to reach the s  + r ( w2t  +l ht  hops, then this packet takes timel  on each hop, and if the first packet traverses ht seconds, incurs a delay of
 - 1 additional r / m ) seconds. Since there are s  + r ( w2t destination. After this time, the destination node receives an additional packet every
packets, the total communication time is given by:
where
Packet routing is suited to networks with highly dynamic states and higher error rates, such as local- and wide-area networks. This is
because individual packets may take different routes and retransmissions can be localized to lost packets.
Cut-Through Routing
In interconnection networks for parallel computers, additional restrictions can be imposed on message transfers to further reduce the
overheads associated with packet switching. By forcing all packets to take the same path, we can eliminate the overhead of transmitting
routing information with each packet. By forcing in-sequence delivery, sequencing information can be eliminated. By associating error
information at message level rather than packet level, the overhead associated with error detection and correction can be reduced. Finally,
since error rates in interconnection networks for parallel machines are extremely low, lean error detection mechanisms can be used
instead of expensive error correction schemes.
The routing scheme resulting from these optimizations is called cut-through routing. In cut-through routing, a message is broken into fixed
. Since flits do not contain the overheads of packets, they can be much smaller than packets. A flits  or flow control digits size units called
tracer is first sent from the source to the destination node to establish a connection. Once a connection has been established, the flits are
sent one after the other. All flits follow the same path in a dovetailed fashion. An intermediate node does not wait for the entire message to
arrive before forwarding it. As soon as a flit is received at an intermediate node, the flit is passed on to the next node. Unlike
store-and-forward routing, it is no longer necessary to have buffer space at each intermediate node to store the entire message.
Therefore, cut-through routing uses less memory and memory bandwidth at intermediate nodes, and is faster.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is the per-hop time, then the header of the ht  links, andl Consider a message that is traversing such a network. If the message traverses
 after the arrival m wt  words long, then the entire message arrives in time m  to reach the destination. If the message is h lt message takes time
of the header of the message. Therefore, the total communication time for cut-through routing is
3  Equation 2.
This time is an improvement over store-and-forward routing since terms corresponding to number of hops and number of words are
 = 1), or if thel additive as opposed to multiplicative in the former. Note that if the communication is between nearest neighbors (that is,
message size is small, then the communication time is similar for store-and-forward and cut-through routing schemes.
Most current parallel computers and many local area networks support cut-through routing. The size of a flit is determined by a variety of
network parameters. The control circuitry must operate at the flit rate. Therefore, if we select a very small flit size, for a given link
bandwidth, the required flit rate becomes large. This poses considerable challenges for designing routers as it requires the control circuitry
to operate at a very high speed. On the other hand, as flit sizes become large, internal buffer sizes increase, so does the latency of
message transfer. Both of these are undesirable. Flit sizes in recent cut-through interconnection networks range from four bits to 32 bytes.
In many parallel programming paradigms that rely predominantly on short messages (such as cache lines), the latency of messages is
critical. For these, it is unreasonable for a long message traversing a link to hold up a short message. Such scenarios are addressed in
routers using multilane cut-through routing. In multilane cut-through routing, a single physical channel is split into a number of virtual
channels.
 are determined by hardware characteristics, software layers, and messaging semantics. Messaging ht , and wt , st Messaging constants
semantics associated with paradigms such as message passing are best served by variable length messages, others by fixed length short
messages. While effective bandwidth may be critical for the former, reducing latency is more important for the latter. Messaging layers for
these paradigms are tuned to reflect these requirements.
While traversing the network, if a message needs to use a link that is currently in use, then the message is blocked. This may lead to
, D , and C , B , A  illustrates deadlock in a cut-through routing network. The destinations of messages 0, 1, 2, and 3 are Figure 2.27 deadlock.
 is occupied by a flit from BA  (and the associated buffers). However, since link CB respectively. A flit from message 0 occupies the link
 is in use. We can see that no AD message 3, the flit from message 0 is blocked. Similarly, the flit from message 3 is blocked since link
messages can progress in the network and the network is deadlocked. Deadlocks can be avoided in cut-through networks by using
. Section 2.6 appropriate routing techniques and message buffers. These are discussed in
Figure 2.27. An example of deadlock in a cut-through routing network.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
A Simplified Cost Model for Communicating Messages
 hops away using cut-through routing isl , the cost of communicating a message between two nodes Section 2.5.1 As we have just seen in
given by
This equation implies that in order to optimize the cost of message transfers, we would need to:
 for each, we want to aggregate st  That is, instead of sending small messages and paying a startup cost Communicate in bulk.
small messages into a single large message and amortize the startup latency across a larger message. This is because on
. wt  or ht  is much larger than those of st typical platforms such as clusters and message-passing machines, the value of
. 1
, it is desirable to reduce the wt  To minimize the overhead paid in terms of per-word transfer time Minimize the volume of data.
volume of data communicated as much as possible.
. 2
 that a message must traverse.l  Minimize the number of hops Minimize distance of data transfer. . 3
While the first two objectives are relatively easy to achieve, the task of minimizing distance of communicating nodes is difficult, and in many
cases an unnecessary burden on the algorithm designer. This is a direct consequence of the following characteristics of parallel platforms
and paradigms:
In many message-passing libraries such as MPI, the programmer has little control on the mapping of processes onto physical
processors. In such paradigms, while tasks might have well defined topologies and may communicate only among neighbors in
the task topology, the mapping of processes to nodes might destroy this structure.
Many architectures rely on randomized (two-step) routing, in which a message is first sent to a random node from source and
from this intermediate node to the destination. This alleviates hot-spots and contention on the network. Minimizing number of
hops in a randomized routing network yields no benefits.
 )for small messages or by per-word component st  ) is typically dominated either by the startup latency ( ht The per-hop time (
) in most networks is relatively small, the per-hop time can be l ) for large messages. Since the maximum number of hops ( m wt (
ignored with little loss in accuracy.
All of these point to a simpler cost model in which the cost of transferring a message between two nodes on a network is given by:
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
4  Equation 2.
This expression has significant implications for architecture-independent algorithm design as well as for the accuracy of runtime
predictions. Since this cost model implies that it takes the same amount of time to communicate between any pair of nodes, it corresponds
to a completely connected network. Instead of designing algorithms for each specific architecture (for example, a mesh, hypercube, or
tree), we can design algorithms with this cost model in mind and port it to any target parallel computer.
This raises the important issue of loss of accuracy (or fidelity) of prediction when the algorithm is ported from our simplified model (which
 term is typically dominated ht assumes a completely connected network) to an actual machine architecture. If our initial assumption that the
 terms is valid, then the loss in accuracy should be minimal. wt  or st by the
However, it is important to note that our basic cost model is valid only for uncongested networks. Architectures have varying thresholds for
when they get congested; i.e., a linear array has a much lower threshold for congestion than a hypercube. Furthermore, different
communication patterns congest a given network to different extents. Consequently, our simplified cost model is valid only as long as the
underlying communication pattern does not congest the network.
Example 2.15 Effect of congestion on communication cost
 mesh in which each node is only communicating with its nearest neighbor. Since no links Consider a
 is the number m , where m wt  + st in the network are used for more than one communication, the time for this operation is
of words communicated. This time is consistent with our simplified model.
Consider an alternate scenario in which each node is communicating with a randomly selected node. This randomness
/4 bi-directional communications) occurring across any equi-partition of p /2 communications (or p implies that there are
the machine (since the node being communicated with could be in either half with equal probability). From our
. From these two, we can infer discussion of bisection width, we know that a 2-D mesh has a bisection width of
 messages, assuming bi-directional that some links would now have to carry at least
, the time for m communication channels. These messages must be serialized over the link. If each message is of size
. This time is not in conformity with our simplified model. this operation is at least
The above example illustrates that for a given architecture, some communication patterns can be non-congesting and others may be
congesting. This makes the task of modeling communication costs dependent not just on the architecture, but also on the communication
. For communication patterns that do not congest the network, the effective bandwidth pattern. To address this, we introduce the notion of
effective bandwidth is identical to the link bandwidth. However, for communication operations that congest the network, the effective
bandwidth is the link bandwidth scaled down by the degree of congestion on the most congested link. This is often difficult to estimate
since it is a function of process to node mapping, routing algorithms, and communication schedule. Therefore, we use a lower bound on
 is the bisection width of the b , where b / p the message communication time. The associated link bandwidth is scaled down by a factor
network.
 because wt In the rest of this text, we will work with the simplified communication model for message passing with effective per-word time
it allows us to design algorithms in an architecture-independent manner. We will also make specific notes on when a communication
operation within an algorithm congests the network and how its impact is factored into parallel runtime. The communication times in the
 meshes. While these times may be realizable on other architectures as well, this is a function of the d - k book apply to the general class of
underlying architecture.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
2.5.2 Communication Costs in Shared-Address-Space Machines
The primary goal of associating communication costs with parallel programs is to associate a figure of merit with a program to guide
program development. This task is much more difficult for cache-coherent shared-address-space machines than for message-passing or
non-cache-coherent architectures. The reasons for this are as follows:
Memory layout is typically determined by the system. The programmer has minimal control on the location of specific data
items over and above permuting data structures to optimize access. This is particularly important in distributed memory
shared-address-space architectures because it is difficult to identify local and remote accesses. If the access times for local and
remote data items are significantly different, then the cost of communication can vary greatly depending on the data layout.
Finite cache sizes can result in cache thrashing. Consider a scenario in which a node needs a certain fraction of the total data
to compute its results. If this fraction is smaller than locally available cache, the data can be fetched on first access and
computed on. However, if the fraction exceeds available cache, then certain portions of this data might get overwritten, and
consequently accessed several times. This overhead can cause sharp degradation in program performance as the problem
size is increased. To remedy this, the programmer must alter execution schedules (e.g., blocking loops as illustrated in serial
matrix multiplication in Problem 2.5) for minimizing working set size. While this problem is common to both serial and
multiprocessor platforms, the penalty is much higher in the case of multiprocessors since each miss might now involve
coherence operations and interprocessor communication.
Overheads associated with invalidate and update operations are difficult to quantify. After a data item has been fetched by a
processor into cache, it may be subject to a variety of operations at another processor. For example, in an invalidate protocol,
the cache line might be invalidated by a write operation at a remote processor. In this case, the next read operation on the data
item must pay a remote access latency cost again. Similarly, the overhead associated with an update protocol might vary
significantly depending on the number of copies of a data item. The number of concurrent copies of a data item and the
schedule of instruction execution are typically beyond the control of the programmer.
Spatial locality is difficult to model. Since cache lines are generally longer than one word (anywhere from four to 128 words),
different words might have different access latencies associated with them even for the first access. Accessing a neighbor of a
previously fetched word might be extremely fast, if the cache line has not yet been overwritten. Once again, the programmer
has minimal control over this, other than to permute data structures to maximize spatial locality of data reference.
Prefetching can play a role in reducing the overhead associated with data access. Compilers can advance loads and, if
sufficient resources exist, the overhead associated with these loads may be completely masked. Since this is a function of the
compiler, the underlying program, and availability of resources (registers/cache), it is very difficult to model accurately.
False sharing is often an important overhead in many programs. Two words used by (threads executing on) different processor
may reside on the same cache line. This may cause coherence actions and communication overheads, even though none of
the data might be shared. The programmer must adequately pad data structures used by various processors to minimize false
sharing.
Contention in shared accesses is often a major contributing overhead in shared address space machines. Unfortunately,
contention is a function of execution schedule and consequently very difficult to model accurately (independent of the
scheduling algorithm). While it is possible to get loose asymptotic estimates by counting the number of shared accesses, such
a bound is often not very meaningful.
Any cost model for shared-address-space machines must account for all of these overheads. Building these into a single cost model
results in a model that is too cumbersome to design programs for and too specific to individual machines to be generally applicable.
As a first-order model, it is easy to see that accessing a remote word results in a cache line being fetched into the local cache. The time
associated with this includes the coherence overheads, network overheads, and memory overheads. The coherence and network
overheads are functions of the underlying interconnect (since a coherence operation must be potentially propagated to remote processors
and the data item must be fetched). In the absence of knowledge of what coherence operations are associated with a specific access and
where the word is coming from, we associate a constant overhead to accessing a cache line of the shared data. For the sake of uniformity
. Because of various latency-hiding protocols, such as prefetching, st with the message-passing model, we refer to this cost as
 is associated with initiating access to a contiguous st implemented in modern processor architectures, we assume that a constant cost of
 is greater than the cache line size. We further assume that accessing shared data is costlier m  words of shared data, even if m chunk of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
p than accessing local data (for instance, on a NUMA machine, local data is likely to reside in a local memory module, while data shared by
wt  - 1 processors). Therefore, we assign a per-word access cost of p processors will need to be fetched from a nonlocal module for at least
to shared data.
m  to account for the cost of sharing a single chunk of m wt  + st From the above discussion, it follows that we can use the same expression
) with the difference that the Equation 2.4 words between a pair of processors in both shared-memory and message-passing paradigms (
wt  is likely to be much smaller on a shared-memory machine than on a distributed memory machine ( wt  relative to st value of the constant
 assumes read-only access without contention. If multiple m wt  + st is likely to be near zero for a UMA machine). Note that the cost
processes access the same data, then the cost is multiplied by the number of processes, just as in the message-passing where the
process that owns the data will need to send a message to each receiving process. If the access is read-write, then the cost will be
incurred again for subsequent access by processors other than the one writing. Once again, there is an equivalence with the
message-passing model. If a process modifies the contents of a message that it receives, then it must send it back to processes that
subsequently need access to the refreshed data. While this model seems overly simplified in the context of shared-address-space
 words between a pair of processors. m machines, we note that the model provides a good estimate of the cost of sharing an array of
The simplified model presented above accounts primarily for remote data access but does not model a variety of other overheads.
Contention for shared data access must be explicitly accounted for by counting the number of accesses to shared data between
co-scheduled tasks. The model does not explicitly include many of the other overheads. Since different machines have caches of varying
sizes, it is difficult to identify the point at which working set size exceeds the cache size resulting in cache thrashing, in an architecture
independent manner. For this reason, effects arising from finite caches are ignored in this cost model. Maximizing spatial locality (cache
line effects) is not explicitly included in the cost. False sharing is a function of the instruction schedules as well as data layouts. The cost
model assumes that shared data structures are suitably padded and, therefore, does not include false sharing costs. Finally, the cost
model does not account for overlapping communication and computation. Other models have been proposed to model overlapped
communication. However, designing even simple algorithms for these models is cumbersome. The related issue of multiple concurrent
computations (threads) on a single processor is not modeled in the expression. Instead, each processor is assumed to execute a single
concurrent unit of computation.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.6 Routing Mechanisms for Interconnection Networks
routing mechanism Efficient algorithms for routing a message to its destination are critical to the performance of parallel computers. A
determines the path a message takes through the network to get from the source to the destination node. It takes as input a message's
source and destination nodes. It may also use information about the state of the network. It returns one or more paths through the network
from the source to the destination node.
. A minimal routing mechanism always selects one of the shortest non-minimal  or minimal Routing mechanisms can be classified as
paths between the source and the destination. In a minimal routing scheme, each link brings a message closer to its destination, but the
scheme can lead to congestion in parts of the network. A non-minimal routing scheme, in contrast, may route the message along a longer
path to avoid network congestion.
deterministic Routing mechanisms can also be classified on the basis of how they use information regarding the state of the network. A
 scheme determines a unique path for a message, based on its source and destination. It does not use any information regarding routing
the state of the network. Deterministic schemes may result in uneven use of the communication resources in a network. In contrast, an
 scheme uses information regarding the current state of the network to determine the path of the message. Adaptive adaptive routing
routing detects congestion in the network and routes messages around it.
. Dimension-ordered routing assigns dimension-ordered routing One commonly used deterministic minimal routing technique is called
successive channels for traversal by a message based on a numbering scheme determined by the dimension of the channel. The
. E-cube routing  and that for a hypercube is called XY-routing dimension-ordered routing technique for a two-dimensional mesh is called
Consider a two-dimensional mesh without wraparound connections. In the XY-routing scheme, a message is sent first along the X
Sx , Sy P dimension until it reaches the column of the destination node and then along the Y dimension until it reaches its destination. Let
 represent that of the destination node. Any minimal routing scheme should return a Dx, Dy P represent the position of the source node and
. In the XY-routing scheme, the message is passed through Sy Dy  and Sx Dx |. Assume that Dy  - Sy | + | Dx  - Sx path of length |
 along the Y Dx, Dy P , ..., Dx +2, Sy P , Dx +1, Sy P  along the X dimension and then through nodes Dx, Sy P , ..., +2 Sx , Sy P , +1 Sx , Sy P intermediate nodes
|. Dy  - Sy | + | Dx  - Sx dimension to reach the destination. Note that the length of this path is indeed |
 be the d P  and s P  nodes. Let p -dimensional hypercube of d E-cube routing for hypercube-connected networks works similarly. Consider a
 bits long. d  that the binary representations of these labels are Section 2.4.3 labels of the source and destination nodes. We know from
 represents the bitwise  (where d P s P Furthermore, the minimum distance between these nodes is given by the number of ones in
 is the k , where k  and sends the message along dimension d P s P  computes s P exclusive-OR operation). In the E-cube algorithm, node
i P  , which receives the message, computesi P  . At each intermediate step, node d P s P position of the least significant nonzero bit in
 and forwards the message along the dimension corresponding to the least significant nonzero bit. This process continues until the d P
 illustrates E-cube routing in a three-dimensional hypercube network. Example 2.16 message reaches its destination.
Example 2.16 E-cube routing in a hypercube network
 = 111 represent the source and d P  = 010 and s P . Let Figure 2.28 Consider the three-dimensional hypercube shown in
 forwards the message along s P  111 = 101. In the first step,  computes 010 s P destination nodes for a message. Node
the dimension corresponding to the least significant bit to node 011. Node 011 sends the message along the dimension
 111 = 100). The message reaches node 111, which is the destination of corresponding to the most significant bit (011
the message.
 (111) in a three-dimensional hypercube d P  (010) to node s P Figure 2.28. Routing a message from node
using E-cube routing.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In the rest of this book we assume deterministic and minimal message routing for analyzing parallel algorithms.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.7 Impact of Process-Processor Mapping and Mapping Techniques
, a programmer often does not have control over how logical processes are mapped to physical Section 2.5.1 As we have discussed in
nodes in a network. For this reason, even communication patterns that are not inherently congesting may congest the network. We
illustrate this with the following example:
Example 2.17 Impact of process mapping
. The underlying architecture is a 16-node mesh with nodes labeled from 1 Figure 2.29 Consider the scenario illustrated in
). Figure 2.29(b) ) and the algorithm has been implemented as 16 processes, labeled 'a' through 'p' ( Figure 2.29(a) to 16 (
The algorithm has been tuned for execution on a mesh in such a way that there are no congesting communication
Figure . (d)  and 2.29(c) operations. We now consider two mappings of the processes to nodes as illustrated in Figures
 is an intuitive mapping and is such that a single link in the underlying architecture only carries data corresponding 2.29(c)
, on the other hand, corresponds to a situation in Figure 2.29(d) to a single communication channel between processes.
which processes have been mapped randomly to processing nodes. In this case, it is easy to see that each link in the
machine carries up to six channels of data between processes. This may potentially result in considerably larger
communication times if the required data rates on communication channels between processes is high.
Figure 2.29. Impact of process mapping on performance: (a) underlying architecture; (b) processes and
their interactions; (c) an intuitive mapping of processes to nodes; and (d) a random mapping of
processes to nodes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
It is evident from the above example that while an algorithm may be fashioned out of non-congesting communication operations, the
mapping of processes to nodes may in fact induce congestion on the network and cause degradation in performance.
2.7.1 Mapping Techniques for Graphs
While the programmer generally does not have control over process-processor mapping, it is important to understand algorithms for such
mappings. This is because these mappings can be used to determine degradation in the performance of an algorithm. Given two graphs,
' and each V  onto a vertex (or a set of vertices) in set V ' maps each vertex in the set G  into graph G '), mapping graph E ', V '( G ) and E , V ( G
'), three parameters are important. First, it E ', V '( G ) into E , V ( G '. When mapping graph E  onto an edge (or a set of edges) in E edge in the set
' is E '. The maximum number of edges mapped onto any edge in E  is mapped onto a single edge in E is possible that more than one edge in
Figure 2.29(d)  has a congestion of one and that in Figure 2.29(c) , the mapping in Example 2.17  of the mapping. In congestion called the
'. This is significant because traffic on E  may be mapped onto multiple contiguous edges in E has a congestion of six. Second, an edge in
the corresponding communication link must traverse more than one link, possibly contributing to congestion on the network. The
' may contain V  and V  of the mapping. Third, the sets dilation  is mapped onto is called the E ' that any edge in E maximum number of links in
'. The ratio of the number of nodes in the set V  corresponds to more than one node in V different numbers of vertices. In this case, a node in
 of the mapping. In the context of process-processor mapping, we want the expansion of the expansion  is called the V ' to that in set V
mapping to be identical to the ratio of virtual and physical processors.
In this section, we discuss embeddings of some commonly encountered graphs such as 2-D meshes (matrix operations illustrated in
). We Chapter 4 , respectively), and trees (broadcast, barriers in 13  and 9 ), hypercubes (sorting and FFT algorithms in Chapters Chapter 8
' contain an equal number of nodes (i.e., an expansion of one). V  and V limit the scope of the discussion to cases in which sets
Embedding a Linear Array into a Hypercube
-dimensional hypercube by mapping d  -1) can be embedded into a d  nodes (labeled 0 through 2 d A linear array (or a ring) composed of 2
) is defined as follows: x , i ( G ) of the hypercube. The function d , i ( G  of the linear array onto nodei node
d  th entry in the sequence of Gray codes ofi ) denotes the d , i ( G  (RGC). The entry binary reflected Gray code  is called the G The function
 bits by reflecting the table and prefixing the reflected entries with a d  + 1 bits are derived from a table of Gray codes of d bits. Gray codes of
. Figure 2.30(a) 1 and the original entries with a 0. This process is illustrated in
Figure 2.30. (a) A three-bit reflected Gray code ring; and (b) its embedding into a three-dimensional hypercube.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
)) differ from each other at only one bit d  + 1,i ( G ) and d , i ( G A careful look at the Gray code table reveals that two adjoining entries (
), there is a direct link in the d  + 1,i ( G  + 1 is mapped toi ), and node d , i ( G  in the linear array is mapped to nodei position. Since node
hypercube that corresponds to each direct link in the linear array. (Recall that two nodes whose labels differ at only one bit position have a
Figure 2.30(b)  has a dilation of one and a congestion of one. G direct link in a hypercube.) Therefore, the mapping specified by the function
illustrates the embedding of an eight-node ring into a three-dimensional hypercube.
Embedding a Mesh into a Hypercube
 wraparound s  x 2r Embedding a mesh into a hypercube is a natural extension of embedding a ring into a hypercube. We can embed a 2
 -node hypercube by s + r mesh into a 2
 - 1) of the hypercube (where || denotes concatenation of the two Gray codes). s , j ( G  - 1)|| r , i ( G ) of the mesh onto node j , i mapping node (
Note that immediate neighbors in the mesh are mapped to hypercube nodes whose labels differ in exactly one bit position. Therefore, this
mapping has a dilation of one and a congestion of one.
) of j , i  are 1 and 2, respectively. Node ( s  and r For example, consider embedding a 2 x 4 mesh into an eight-node hypercube. The values of
, 2) of the hypercube. Therefore, node (0, 0) of the mesh is mapped to node 000 of the hypercube, j ( G , 1)|| i ( G the mesh is mapped to node
(0, 2) is 00; concatenating the two yields the label 000 for the hypercube node. Similarly, node (0, 1) of the mesh G (0, 1) is 0 and G because
 illustrates embedding meshes into hypercubes. Figure 2.31 is mapped to node 001 of the hypercube, and so on.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 2.31. (a) A 4 x 4 mesh illustrating the mapping of mesh nodes to the nodes in a four-dimensional hypercube;
and (b) a 2 x 4 mesh embedded into a three-dimensional hypercube.
This mapping of a mesh into a hypercube has certain useful properties. All nodes in
Section 2.4.3  identical most significant bits. We know from r the same row of the mesh are mapped to hypercube nodes whose labels have
 nodes. Since each mesh s  with 2 s )-dimensional hypercube yields a subcube of dimension s  + r  bits in the node label of an ( r that fixing any
 nodes, every row in the mesh is mapped to a s node is mapped onto a unique node in the hypercube, and each row in the mesh has 2
distinct subcube in the hypercube. Similarly, each column in the mesh is mapped to a distinct subcube in the hypercube.
Embedding a Mesh into a Linear Array
 links. In contrast, a p We have, up until this point, considered embeddings of sparser networks into denser networks. A 2-D mesh has 2 x
 links. Consequently, there must be a congestion associated with this mapping. p -node linear array has p
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Consider first the mapping of a linear array into a mesh. We assume that neither the mesh nor the linear array has wraparound
. Here, the solid lines correspond to links in the Figure 2.32 connections. An intuitive mapping of a linear array into a mesh is illustrated in
 that a congestion-one, dilation-one mapping of a Figure 2.32(a) linear array and normal lines to links in the mesh. It is easy to see from
linear array to a mesh is possible.
Figure 2.32. (a) Embedding a 16 node linear array into a 2-D mesh; and (b) the inverse of the mapping. Solid lines
correspond to links in the linear array and normal lines to links in the mesh.
Consider now the inverse of this mapping, i.e., we are given a mesh and we map vertices of the mesh to those in a linear array using the
e . As before, the solid lines correspond to edges in th Figure 2.32(b) inverse of the same mapping function. This mapping is illustrated in
o linear array and normal lines to edges in the mesh. As is evident from the figure, the congestion of the mapping in this case is five – i.e., n
 for a solid line carries more than five normal lines. In general, it is easy to show that the congestion of this (inverse) mapping is
 edges to the next row, and one additional edge). node mapping (one for each of the - general p
While this is a simple mapping, the question at this point is whether we can do better. To answer this question, we use the bisection width
, and that of a linear array is 1. of the two networks. We know that the bisection width of a 2-D mesh without wraparound links is
. This implies that if we take the linear array and cut it r Assume that the best mapping of a 2-D mesh into a linear array has a congestion of
 must be at least equal to the r  mesh links. We claim that r in half (at the middle), we will cut only one linear array link, or no more than
bisection width of the mesh. This follows from the fact that an equi-partition of the linear array into two also partitions the mesh into two.
 mesh links must cross the partition, by definition of bisection width. Consequently, the one linear array link Therefore, at least
 This.  mesh links. Therefore, the congestion of any mapping is lower bounded by connecting the two halves must carry at least
. Figure 2.32(b) is almost identical to the simple (inverse) mapping we have illustrated in
The lower bound established above has a more general applicability when mapping denser networks to sparser ones. One may
. In the case y/ x  links is y  with Q  links into network x  with S reasonably believe that the lower bound on congestion of a mapping of network
, or 2. However, this lower bound is overly conservative. A tighter lower p/ p of the mapping from a mesh to a linear array, this would be 2
bound is in fact possible by examining the bisection width of the two networks. We illustrate this further in the next section.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Embedding a Hypercube into a 2-D Mesh
 is an even power p -node 2-D mesh. For the sake of convenience, we assume that p -node hypercube into a p Consider the embedding of a
 = d  nodes. We do this as follows: let  subcubes, each with of two. In this scenario, it is possible to visualize the hypercube as
/2 least significant bits and use them d  is even. We take the d  be the dimension of the hypercube. From our assumption, we know that p log
 nodes. For example, in the case of a 4D hypercube, we use the lower two bits to define the to define individual subcubes of
subcubes as (0000, 0001, 0011, 0010), (0100, 0101, 0111, 0110), (1100, 1101, 1111, 1110), and (1000, 1001, 1011, 1010). Note at this
/2 most d /2 least significant bits across all of these subcubes, we will have another subcube as defined by the d point that if we fix the
significant bits. For example, if we fix the lower two bits across the subcubes to 10, we get the nodes (0010, 0110, 1110, 1010). The
reader can verify that this corresponds to a 2-D subcube.
 node row of the  node subcube is mapped to a The mapping from a hypercube to a mesh can now be defined as follows: each
.  node hypercube is mesh. We do this by simply inverting the linear-array to hypercube mapping. The bisection width of the
 (at the  node row is 1. Therefore the congestion of this subcube-to-row mapping is The corresponding bisection width of a
. In this fashion, (b)  and Figure 2.33(a)  = 32 in p  = 16 and p edge that connects the two halves of the row). This is illustrated for the cases of
we can map each subcube to a different row in the mesh. Note that while we have computed the congestion resulting from the
subcube-to-row mapping, we have not addressed the congestion resulting from the column mapping. We map the hypercube nodes into
/2 least significant bits in the hypercube are mapped to the same column. This results in d the mesh in such a way that nodes with identical
 nodes. Using the same argument as in the case of a subcube-to-column mapping, where each subcube/column has
. Since the congestion from the row and column mappings affects subcube-to-row mapping, this results in a congestion of
. disjoint sets of edges, the total congestion of this mapping is
Figure 2.33. Embedding a hypercube into a 2-D mesh.
. Since the bisection width of a hypercube Section 2.7.1 To establish a lower bound on the congestion, we follow the same argument as in
. We notice that our mapping yields this , the lower bound on congestion is the ratio of these, i.e., /2 and that of a mesh is p is
lower bound on congestion.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Process-Processor Mapping and Design of Interconnection Networks
Our analysis in previous sections reveals that it is possible to map denser networks into sparser networks with associated congestion
overheads. This implies that a sparser network whose link bandwidth is increased to compensate for the congestion can be expected to
 will perform as well as the denser network (modulo dilation effects). For example, a mesh whose links are faster by a factor of
yield comparable performance to a hypercube. We call such a mesh a fat-mesh. A fat-mesh has the same bisection-bandwidth as a
, by using appropriate message routing techniques, the Section 2.5.1 hypercube; however it has a higher diameter. As we have seen in
effect of node distance can be minimized. It is important to note that higher dimensional networks involve more complicated layouts, wire
crossings, and variable wire-lengths. For these reasons, fattened lower dimensional networks provide attractive alternate approaches to
designing interconnects. We now do a more formal examination of the cost-performance tradeoffs of parallel architectures.
2.7.2 Cost-Performance Tradeoffs
We now examine how various cost metrics can be used to investigate cost-performance tradeoffs in interconnection networks. We
illustrate this by analyzing the performance of a mesh and a hypercube network with identical costs.
)/4 wires per channel p -node wraparound mesh with (log p If the cost of a network is proportional to the number of wires, then a square
-node hypercube with one wire per channel. Let us compare the average communication times of these two p costs as much as a
 and that in a hypercube is  between any two nodes in a two-dimensional wraparound mesh is avl networks. The average distance
 in networks that use m wt  + avl ht  + st  hops apart is given by avl  between nodes that are m )/2. The time for sending a message of size p (log
)/4, the per-word transfer time is reduced by the p cut-through routing. Since the channel width of the mesh is scaled up by a factor of (log
, then the same time on a mesh with fattened channels is given by wt same factor. Hence, if the per-word transfer time on the hypercube is
 and that for a wraparound mesh of m wt )/2 + p  (log ht  + st ). Hence, the average communication latency for a hypercube is given by p /(log wt 4
. the same cost is
Let us now investigate the behavior of these expressions. For a fixed number of nodes, as the message size is increased, the
))is p /(log m wt  for the two networks, we see that the time for a wraparound mesh (4 wt  dominates. Comparing wt communication term due to
 is sufficiently large. Under these circumstances, m  is greater than 16 and the message size p )if m wt less than the time for a hypercube (
point-to-point communication of large messages between random pairs of nodes takes less time on a wraparound mesh with cut-through
routing than on a hypercube of the same cost. Furthermore, for algorithms in which communication is suited to a mesh, the extra
bandwidth of each channel results in better performance. Note that, with store-and-forward routing, the mesh is no longer more
s -cubes (Problem d -ary k cost-efficient than a hypercube. Similar cost-performance tradeoffs can be analyzed for the general case of
. 2.25–2.29)
The communication times above are computed under light load conditions in the network. As the number of messages increases, there is
contention on the network. Contention affects the mesh network more adversely than the hypercube network. Therefore, if the network is
heavily loaded, the hypercube will outperform the mesh.
 wires per channel has a cost -node wraparound mesh with p If the cost of a network is proportional to its bisection width, then a
node hypercube with one wire per channel. Let us perform an analysis similar to the one above to investigate - equal to a p
, the per-word transfer time cost-performance tradeoffs using this cost metric. Since the mesh channels are wider by a factor of
will be lower by an identical factor. Therefore, the communication times for the hypercube and the mesh networks of the same cost are
 becomes m , respectively. Once again, as the message size  and m wt )/2 + p  (log ht  + st given by
 > 16 and sufficiently p  term dominates. Comparing this term for the two networks, we see that for wt large for a given number of nodes, the
large message sizes, a mesh outperforms a hypercube of the same cost. Therefore, for large enough messages, a mesh is always better
than a hypercube of the same cost, provided the network is lightly loaded. Even when the network is heavily loaded, the performance of a
mesh is similar to that of a hypercube of the same cost.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
2.8 Bibliographic Remarks
]. Parallel architectures and Sto93 , PH96 , PH90 Several textbooks discuss various aspects of high-performance architectures [
]. Historically, Sto93 , Sie85 , Lil92 , HB84 , DeC89 , AG94 , Fly95 , HX98 , LW95 , CSG98 interconnection networks have been well described [
]. He also proposed the MISD (multiple Fly72 the classification of parallel computers as SISD, SIMD, and MIMD was introduced by Flynn [
instruction stream, single data stream) model. MISD is less natural than the other classes, although it can be viewed as a model for
] provides a layered classification of Ni91 ] introduced the Single Program Multiple Data (SPMD) paradigm. Ni [ DRGNP pipelining. Darema [
parallel computers based on hardware architecture, address space, communication model, language, programming environment, and
applications.
] provides a tutorial on static and dynamic Fen81 Interconnection networks have been an area of active interest for decades. Feng [
]. Omega networks were Sto71 interconnection networks. The perfect shuffle interconnection pattern was introduced by Stone [
] and the Bat76 ]. Other multistage networks have also been proposed. These include the Flip network [ Law75 introduced by Lawrie [
] also provides a detailed Lei92 ]. Leighton [ Lei92 ]. Mesh of trees and pyramidal mesh are discussed by Leighton [ WF80 Baseline network [
discussion of many related networks.
]. The WB72 The C.mmp was an early research prototype MIMD shared-address-space parallel computer based on the Crossbar switch [
Sun Ultra HPC Server and Fujitsu VPP 500 are examples of crossbar-based parallel computers or their variants. Several parallel
], 83 + GGK ], the NYU Ultracomputer [ BBN89 computers were based on multistage interconnection networks including the BBN Butterfly [
] are NUMA shared-address-space Ken90 ] and the KSR-1 [ 92 + LLG ]. The SGI Origin 2000, Stanford Dash [ 85 + PBG and the IBM RP-3 [
computers.
] was among the first message-passing parallel computers based on a hypercube-connected network. These Sei85 The Cosmic Cube [
] and the Intel iPSC-1, iPSC-2, and iPSC/860. More recently, the SGI Origin 2000 uses a network nCU90 were followed by the nCUBE 2 [
] derive interesting properties of the hypercube-connected network and a variety of SS89a , SS88 similar to a hypercube. Saad and Shultz [
]. Many parallel computers, such as the Cray T3E, are based on the mesh network. The Intel Paragon XP/S SS89b other static networks [
] was based 92 + D ] are earlier examples of two-dimensional mesh-based computers. The MIT J-Machine [ Sei92 ] and the Mosaic C [ Sup91[
on a three-dimensional mesh network. The performance of mesh-connected computers can be improved by augmenting the mesh
 in Problem 2.16) was introduced by Miller et Figure 2.35 ]. The reconfigurable mesh architecture ( KR87a network with broadcast buses [
]. Other examples of reconfigurable meshes include the TRAC and PCHIP. MKRS88 al. [
] Lei85b The DADO parallel computer was based on a tree network [SM86]. It used a complete binary tree of depth 10. Leiserson [
introduced the fat-tree interconnection network and proved several interesting characteristics of it. He showed that for a given volume of
] parallel computer was based Thi91 hardware, no network has much better performance than a fat tree. The Thinking Machines CM-5 [
on a fat tree interconnection network.
], the DAP Bat80 ] was among the first SIMD parallel computers. Other SIMD computers include the Goodyear MPP [ Bar68 The Illiac IV [
]. The CM-5 and DADO incorporate both SIMD and MIMD features. Nic90 ], MasPar MP-1, and MasPar MP-2 [ Thi90 610, and the CM-2 [
Both are MIMD computers but have extra hardware for fast synchronization, which enables them to operate in SIMD mode. The CM-5
had a control network to augment the data network. The control network provides such functions as broadcast, reduction, combining,
and other global operations.
] discuss embedding one interconnection network into another. Gray codes, used in RS90b ] and Ranka and Sahni [ Lei92 Leighton [
] discuss the concepts of RS90b ]. Ranka and Sahni [ RND77 embedding linear array and mesh topologies, are discussed by Reingold [
congestion, dilation, and expansion.
]. The wormhole routing technique was NM93 A comprehensive survey of cut-through routing techniques is provided by Ni and McKinley [
, in which communication buffers are provided at virtual cut-through ]. A related technique called DS86 proposed by Dally and Seitz [
] discuss deadlock-free wormhole routing DS87 ]. Dally and Seitz [ KK79 intermediate nodes, was described by Kermani and Kleinrock [
based on channel dependence graphs. Deterministic routing schemes based on dimension ordering are often used to avoid deadlocks.
]. SB77 Cut-through routing has been used in several parallel computers. The E-cube routing scheme for hypercubes was proposed by [
] discusses cost-performance tradeoffs of networks for message-passing computers. Using the bisection bandwidth of a Dal90b Dally [
network as a measure of the cost of the network, he shows that low-dimensional networks (such as two-dimensional meshes) are more
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
] derive the KV92 , Dal90a]. Kreeger and Vempaty [ Dal90b , Dal87 cost-effective than high-dimensional networks (such as hypercubes) [
bandwidth equalization factor for a mesh with respect to a hypercube-connected computer for all-to-all personalized communication
] analyze the cost-performance tradeoffs of FFT computations on mesh and hypercube networks. GK93b ). Gupta and Kumar [ Section 4.5 (
], and GR90 ], Gibbons [ Akl89 ]. Books by Akl [ Sni85 , Sni82 , LY86 , KR88 , FW78 The properties of PRAMs have been studied extensively [
]. A number of processor networks Jaj92 ] address PRAM algorithms. Our discussion of PRAM is based upon the book by Jaja [ Jaj92 Jaja [
] MV84 ]. Mehlhorn and Vishkin [ UW84 , Upf84 , MV84 , LPP89 , LPP88 , HP89 , AHMP87 have been proposed to simulate PRAM models [
 (MPC) to simulate PRAM models. The MPC is a message-passing parallel computer composed module parallel computer propose the
 processors, each with a fixed amount of memory and connected by a completely-connected network. The MPC is capable of p of
. The main drawback of p  steps if the total memory is increased by a factor of log p  log T  steps of a PRAM in T probabilistically simulating
] AHMP87  [ et al. the MPC model is that a completely-connected network is difficult to construct for a large number of processors. Alt
 (BDN). In this network, each processor is connected to a fixed number of bounded-degree network propose another model called the
) time probabilistic simulation of a PRAM on a BDN. Hornick and p  log T ( O ] describe an KU86 other processors. Karlin and Upfal [
] propose a bipartite network that connects sets of processors and memory pools. They investigate both the HP89 Preparata [
message-passing MPC and BDN based on a mesh of trees.
Many modifications of the PRAM model have been proposed that attempt to bring it closer to practical parallel computers. Aggarwal,
]. They ACS89b ] propose the LPRAM (local-memory PRAM) model and the BPRAM (block PRAM) model [ ACS89b Chandra, and Snir [
]. In this model, memory units at different levels are accessed in ACS89a also introduce a hierarchical memory model of computation [
different times. Parallel algorithms for this model induce locality by bringing data into faster memory units before using them and
], and the delay model Val90b ], XPRAM [ Gib89 returning them to the slower memory units. Other PRAM models such as phase PRAM [
, Sny86 , 93a + CKP ] have also been proposed. Many researchers have investigated abstract universal models for parallel computers [ PY88[
], and QSM DFRC96 ], CGM [ HK96  [ 3 ], C GKRS96  [ 3 ], A 93b + CKP ], LogP [ BNK92 ], Postal model [ Val90a ]. Models such as BSP [ Val90a
] have been proposed with similar objectives. Ram97[
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
 Design an experiment (i.e., design and write programs and take measurements) to determine the memory 2.1
bandwidth of your computer and to estimate the caches at various levels of the hierarchy. Use this experiment to
estimate the bandwidth and L1 cache of your computer. Justify your answer. (Hint: To test bandwidth, you do not want
reuse. To test cache size, you want reuse to see the effect of the cache and to increase this size until the reuse
decreases sharply.)
 Consider a memory system with a level 1 cache of 32 KB and DRAM of 512 MB with the processor operating at 1 2.2
GHz. The latency to L1 cache is one cycle and the latency to DRAM is 100 cycles. In each memory cycle, the
processor fetches four words (cache line size is four words). What is the peak achievable performance of a dot product
of two vectors? Note: Where necessary, assume an optimal cache placement policy.
1 /* dot product loop */
2 for (i = 0; i < dim; i++)
3 dot_prod += a[i] * b[i];
 Now consider the problem of multiplying a dense matrix with a vector using a two-loop dot-product formulation. The 2.3
. (Each row of the matrix takes 16 KB of storage.) What is the peak achievable K  x 4 K matrix is of dimension 4
performance of this technique using a two-loop dot-product based matrix-vector product?
1 /* matrix-vector product loop */
2 for (i = 0; i < dim; i++)
3 for (j = 0; i < dim; j++)
4 c[i] += a[i][j] * b[j];
. What is the K  x 4 K  Extending this further, consider the problem of multiplying two dense matrices of dimension 4 2.4
peak achievable performance using a three-loop dot-product based formulation? (Assume that matrices are laid out in
a row-major fashion.)
1 /* matrix-matrix product loop */
2 for (i = 0; i < dim; i++)
3 for (j = 0; i < dim; j++)
4 for (k = 0; k < dim; k++)
5 c[i][j] += a[i][k] * b[k][j];
 Restructure the matrix multiplication algorithm to achieve better cache performance. The most obvious cause of the 2.5
poor performance of matrix multiplication was the absence of spatial locality. In some cases, we were wasting three of
the four words fetched from memory. To fix this problem, we compute the elements of the result matrix four at a time.
Using this approach, we can increase our FLOP count with a simple restructuring of the program. However, it is
possible to achieve much higher performance from this problem. This is possible by viewing the matrix multiplication
problem as a cube in which each internal grid point corresponds to a multiply-add operation. Matrix multiplication
algorithms traverse this cube in different ways, which induce different partitions of the cube. The data required for
computing a partition grows as the surface area of the input faces of the partition and the computation as the volume
of the partition. For the algorithms discussed above, we were slicing thin partitions of the cube for which the area and
volume were comparable (thus achieving poor cache performance). To remedy this, we restructure the computation by
 data for each of the three 2 k  ( 2 k . The data associated with this is 3 x k  x k  x k partitioning the cube into subcubes of size
 since that is the K  to be equal to 8 2 k . To maximize performance, we would like 3 x 3 k matrices) and the computation is
 = 51. k amount of cache available (assuming the same machine parameters as in Problem 2.2). This corresponds to
The computation associated with a cube of this dimension is 132651 multiply-add operations or 265302 FLOPs. To
perform this computation, we needed to fetch two submatrices of size 51 x 51. This corresponds to 5202 words or
1301 cache lines. Accessing these cache lines takes 130100 ns. Since 265302 FLOPs are performed in 130100 ns,
the peak computation rate of this formulation is 2.04 GFLOPS. Code this example and plot the performance as a
. (Code on any conventional microprocessor. Make sure you note the clock speed, the microprocessor and k function of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
the cache available at each level.)
 Consider an SMP with a distributed shared-address-space. Consider a simple cost model in which it takes 10 ns to 2.6
access local cache, 100 ns to access local memory, and 400 ns to access remote memory. A parallel program is
running on this machine. The program is perfectly load balanced with 80% of all accesses going to local cache, 10% to
local memory, and 10% to remote memory. What is the effective memory access time for this computation? If the
computation is memory bound, what is the peak computation rate?
Now consider the same computation running on one processor. Here, the processor hits the cache 70% of the time
and local memory 30% of the time. What is the effective peak computation rate for one processor? What is the
fractional computation rate of a processor in a parallel configuration as compared to the serial configuration?
 Notice that the cache hit for multiple processors is higher than that for one processor. This is typically because Hint:
the aggregate cache available on multiprocessors is larger than on single processor systems.
 What are the major differences between message-passing and shared-address-space computers? Also outline the 2.7
advantages and disadvantages of the two.
 Why is it difficult to construct a true shared-memory computer? What is the minimum number of switches for 2.8
 words (where each word can be accessed independently)? b  processors to a shared memory with p connecting
 Of the four PRAM models (EREW, CREW, ERCW, and CRCW), which model is the most powerful? Why? 2.9
 levels (as the omega network). In p  is an interconnection network composed of log Butterfly network  The ] Lei92 2.10 [
 + 1 andl  is connected to the identically numbered element at levell  at a leveli a Butterfly network, each switching node
 isi S th most significant bit. Therefore, switching nodel to a switching node whose number differs from itself only at the
 ).l - p log  (2 i  = j  ori  = j  ifl  at levelj S connected to element
 illustrates a Butterfly network with eight processing nodes. Show the equivalence of a Butterfly network and Figure 2.34
an omega network.
Figure 2.34. A Butterfly network with eight processing nodes.
 Rearrange the switches of an omega network so that it looks like a Butterfly network. Hint:
. As shown there, this network is a blocking network (that Section 2.4.3  Consider the omega network described in 2.11
is, a processor that uses the network to access a memory location might prevent another processor from accessing
 = [0, P  that mapsf  processors. Define a function p another memory location). Consider an omega network that connects
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). Think of this function as p  <i  for all 0 P ]i '[ P ]) andi[ P (f ] =i '[ P  (that is, P ' of P  - 1] onto a permutation p 1, ...,
] requests communication with processori[ P mapping communication requests by the processors so that processor
].i '[ P
How many distinct permutation functions exist? . 1
How many of these functions result in non-blocking communication? . 2
What is the probability that an arbitrary function will result in non-blocking communication? . 3
 A cycle in a graph is defined as a path originating and terminating at the same node. The length of a cycle is the 2.12
-dimensional hypercube. d number of edges in the cycle. Show that there are no odd-length cycles in a
 of these bits, show that the nodes whose labels k  bits. Fixing any d -dimensional hypercube use d  The labels in a 2.13
 nodes.) k - d ( )-dimensional subcube composed of 2 k  - d  bit positions form a ( k  - d differ in the remaining
 and A ) to be the Hamming distance between B , A ( H -dimensional hypercube. Define d  be two nodes in a B  and A  Let 2.14
 and have no parallel paths . These paths are called B  and A ) to be the number of distinct paths connecting B , A ( P , and B
. Prove the following: B  and A common nodes other than
). B , A ( H  is given by B  and A The minimum distance in terms of communication links between . 1
 . d ) = B , A ( P The total number of parallel paths between any two nodes is . 2
). B , A ( H ) = B , A ( ) B, A( H = length P ) is B , A ( H  of length B  and A The number of parallel paths between . 3
) + 2. B , A ( H ) parallel paths is B , A ( H  - d The length of the remaining . 4
 In the informal derivation of the bisection width of a hypercube, we used the construction of a hypercube to show 2.15
 - 1)-dimensional hypercubes. We argued that because d -dimensional hypercube is formed from two ( d that a
 - 1 links across the d corresponding nodes in each of these subcubes have a direct communication link, there are 2
partition. However, it is possible to partition a hypercube into two parts such that neither of the partitions is a
 - 1 direct links between them. d hypercube. Show that any such partitions will have more than 2
 array of processing nodes  consists of a reconfigurable mesh  A ] MKRS88 2.16 [
. Each Figure 2.35 connected to a grid-shaped reconfigurable broadcast bus. A 4 x 4 reconfigurable mesh is shown in
node has locally-controllable bus switches. The internal connections among the four ports, north (N), east (E), west
(W), and south (S), of a node can be configured during the execution of an algorithm. Note that there are 15
connection patterns. For example, {SW, EN} represents the configuration in which port S is connected to port W and
 at any time. The switches allow 0-signal  or 1-signal port N is connected to port E. Each bit of the bus carries one of
the broadcast bus to be divided into subbuses, providing smaller reconfigurable meshes. For a given set of switch
 is a maximally-connected subset of the nodes. Other than the buses and the switches, the subbus settings, a
reconfigurable mesh is similar to the standard two-dimensional mesh. Assume that only one node is allowed to
 shared by multiple nodes at any time. subbus broadcast on a
Figure 2.35. Switch connection patterns in a reconfigurable mesh.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Determine the bisection width, the diameter, and the number of switching nodes and communication links for a
 processing nodes. What are the advantages and disadvantages of a reconfigurable mesh of
reconfigurable mesh as compared to a wraparound mesh?
 is a network that imposes a tree interconnection on a grid of processing nodes. A mesh of trees  A ] Lei92 2.17 [
 grid, a complete binary tree is  mesh of trees is constructed as follows. Starting with a
Figure 2.36 imposed on each row of the grid. Then a complete binary tree is imposed on each column of the grid.
illustrates the construction of a 4 x 4 mesh of trees. Assume that the nodes at intermediate levels are switching nodes.
 mesh. Determine the bisection width, diameter, and total number of switching nodes in a
Figure 2.36. The construction of a 4 x 4 mesh of trees: (a) a 4 x 4 grid, (b) complete binary trees
imposed over individual rows, (c) complete binary trees imposed over each column, and (d) the
complete 4 x 4 mesh of trees.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
x  x ··· d 1/ p  x d 1/ p  dimensions to construct a d  Extend the two-dimensional mesh of trees (Problem 2.17) to ] Lei92 2.18 [
p
 mesh of trees. We can do this by fixing grid positions in all dimensions to different values and imposing a complete 1/d
binary tree on the one dimension that is being varied.
 mesh of trees. Calculate the diameter, bisection 1/d p  x ··· x d 1/ p  x d 1/ p Derive the total number of switching nodes in a
width, and wiring cost in terms of the total number of wires. What are the advantages and disadvantages of a mesh of
trees as compared to a wraparound mesh?
-dimensional pyramidal d . A pyramidal mesh -dimensional d  A network related to the mesh of trees is the ] Lei92 2.19 [
mesh imposes a pyramid on the underlying grid of processing nodes (as opposed to a complete tree in the mesh of
trees). The generalization is as follows. In the mesh of trees, all dimensions except one are fixed and a tree is imposed
on the remaining dimension. In a pyramid, all but two dimensions are fixed and a pyramid is imposed on the mesh
 - 1. Similarly, in a k /2 at leveli  is connected to node k  at leveli formed by these two dimensions. In a tree, each node
 - 1. Furthermore, the nodes at each level are k /2) at level j /2,i  is connected to a node ( k ) at level j , i pyramid, a node (
. Figure 2.37 connected in a mesh. A two-dimensional pyramidal mesh is illustrated in
Figure 2.37. A 4 x 4 pyramidal mesh.
 pyramidal mesh, assume that the intermediate nodes are switching nodes, and derive the For a
diameter, bisection width, arc connectivity, and cost in terms of the number of communication links and switching
nodes. What are the advantages and disadvantages of a pyramidal mesh as compared to a mesh of trees?
 One of the drawbacks of a hypercube-connected network is that different wires in the network are of ] Lei92 2.20 [
different lengths. This implies that data takes different times to traverse different communication links. It appears that
two-dimensional mesh networks with wraparound connections suffer from this drawback too. However, it is possible to
fabricate a two-dimensional wraparound mesh using wires of fixed length. Illustrate this layout by drawing such a 4 x 4
wraparound mesh.
p -node hypercube. What are the allowable values of p -node three-dimensional mesh into a p  Show how to embed a 2.21
for your embedding?
-node hypercube. p -node mesh of trees into a p  Show how to embed a 2.22
 - 1 nodes in which each node is a processing node. What is the d  Consider a complete binary tree of 2 2.23
-dimensional hypercube? d minimum-dilation mapping of such a tree onto a
 is very useful. Consider two parallel computers with different minimum congestion mapping  The concept of a 2.24
 mapping of the first into the second exists. Ignoring the dilation of r interconnection networks such that a congestion-
 times faster than the first computer, the r the mapping, if each communication link in the second computer is more than
second computer is strictly superior to the first.
-node mesh. Ignoring the dilation of the mapping, what is d -dimensional hypercube onto a 2 d Now consider mapping a
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
the minimum-congestion mapping of the hypercube onto the mesh? Use this result to determine whether a 1024-node
mesh with communication links operating at 25 million bytes per second is strictly better than a 1024-node hypercube
(whose nodes are identical to those used in the mesh) with communication links operating at two million bytes per
second.
 to be the avl  nodes. Define p -cube with d -ary k  Derive the diameter, number of links, and bisection width of a 2.25
-cube. d -ary k  for a avl average distance between any two nodes in the network. Derive
 Consider the routing of messages in a parallel computer that uses store-and-forward routing. In such a network, 2.26
. An m  x d  x wt  + st  is d  via a path of length destination P  to source P  from m the cost of sending a single message of size
, k / m  parts each of size k  is as follows. The user breaks the message into m alternate way of sending a message of size
. For this new method, derive the destination P  to source P  distinct messages one by one from k and then sends these
 hops away under the following two cases: d  to a node m expression for time to transfer a message of size
 as soon as the previous message has reached the source P Assume that another message can be sent from
next node in the path.
. 1
 only after the previous message has reached source P Assume that another message can be sent from
. destination P
. 2
. Also, what is the m  varies between 1 and k For each case, comment on the value of this expression as the value of
 = 0? st  is very large, or if st  if k optimal value of
 nodes. Assume that the channel width of each communication link is one. The p  Consider a hypercube network of 2.27
) can be increased by equating the cost of this network with p  < log d -cube (for d -ary k channel width of the links in a
that of a hypercube. Two distinct measures can be used to evaluate the cost of a network.
The cost can be expressed in terms of the total number of wires in the network (the total number of wires is a
product of the number of communication links and the channel width).
. 1
The bisection bandwidth can be used as a measure of cost. . 2
-cube with a hypercube, what is the channel width d -ary k Using each of these cost metrics and equating the cost of a
-cube with an identical number of nodes, channel rate, and cost? d -ary k of a
 The results from Problems 2.25 and 2.27 can be used in a cost-performance analysis of static interconnection 2.28
 nodes with cut-through routing. Assume a hypercube-connected p -cube network of d -ary k networks. Consider a
 nodes with channel width one. The channel width of other networks in the family is scaled up so that their p network of
' be the scaling factors for the channel width derived by equating s  and s cost is identical to that of the hypercube. Let
the costs specified by the two cost metrics in Problem 2.27.
', express the average communication time between any two nodes as a s  and s For each of the two scaling factors
-cube and the number of nodes. Plot the communication time as a function d -ary k )of a d function of the dimensionality (
s (for the µ  = 0.5 wt  = ht s, and µ  = 50.0 st  = 512 bytes, m  = 256, 512, and 1024, message size p of the dimensionality for
, what is the dimensionality of the network that yields the best performance for m  and p hypercube). For these values of
a given cost?
-cube with store-and-forward routing. d -ary k  Repeat Problem 2.28 for a 2.29
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 3. Principles of Parallel Algorithm Design
Algorithm development is a critical component of problem solving using computers. A sequential algorithm is essentially a recipe or a
sequence of basic steps for solving a given problem using a serial computer. Similarly, a parallel algorithm is a recipe that tells us how to
solve a given problem using multiple processors. However, specifying a parallel algorithm involves more than just specifying the steps. At
the very least, a parallel algorithm has the added dimension of concurrency and the algorithm designer must specify sets of steps that
can be executed simultaneously. This is essential for obtaining any performance benefit from the use of a parallel computer. In practice,
specifying a nontrivial parallel algorithm may include some or all of the following:
Identifying portions of the work that can be performed concurrently.
Mapping the concurrent pieces of work onto multiple processes running in parallel.
Distributing the input, output, and intermediate data associated with the program.
Managing accesses to data shared by multiple processors.
Synchronizing the processors at various stages of the parallel program execution.
Typically, there are several choices for each of the above steps, but usually, relatively few combinations of choices lead to a parallel
algorithm that yields performance commensurate with the computational and storage resources employed to solve the problem. Often,
different choices yield the best performance on different parallel architectures or under different parallel programming paradigms.
In this chapter, we methodically discuss the process of designing and implementing parallel algorithms. We shall assume that the onus
of providing a complete description of a parallel algorithm or program lies on the programmer or the algorithm designer. Tools and
compilers for automatic parallelization at the current state of the art seem to work well only for highly structured programs or portions of
programs. Therefore, we do not consider these in this chapter or elsewhere in this book.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
3.1 Preliminaries
Dividing a computation into smaller computations and assigning them to different processors for parallel execution are the two key steps in
the design of parallel algorithms. In this section, we present some basic terminology and introduce these two key steps in parallel
algorithm design using matrix-vector multiplication and database query processing as examples.
3.1.1 Decomposition, Tasks, and Dependency Graphs
The process of dividing a computation into smaller parts, some or all of which may potentially be executed in parallel, is called
 are programmer-defined units of computation into which the main computation is subdivided by means of Tasks . decomposition
decomposition. Simultaneous execution of multiple tasks is the key to reducing the time required to solve the entire problem. Tasks can be
of arbitrary size, but once defined, they are regarded as indivisible units of computation. The tasks into which a problem is decomposed
may not all be of the same size.
Example 3.1 Dense matrix-vector multiplication
] of thei[ y th elementi . The y  to yield another vector b  with a vector A  matrix n  x n Consider the multiplication of a dense
. As ; i.e., b  with the input vector A th row ofi product vector is the dot-product of the
, Figure 3.4 ] can be regarded as a task. Alternatively, as shown later ini[ y , the computation of each Figure 3.1 shown later in
/4 of the entries of n the computation could be decomposed into fewer, say four, tasks where each task computes roughly
. y the vector
 is the number of n  tasks, where n Figure 3.1. Decomposition of dense matrix-vector multiplication into
rows in the matrix. The portions of the matrix and the input and output vectors accessed by Task 1 are
highlighted.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 are independent and can be performed all together or in any sequence. However, in general, some tasks Figure 3.1 Note that all tasks in
may use data produced by other tasks and thus may need to wait for these tasks to finish execution. An abstraction used to express such
. A task-dependency graph is a task-dependency graph dependencies among tasks and their relative order of execution is known as a
directed acyclic graph in which the nodes represent tasks and the directed edges indicate the dependencies amongst them. The task
corresponding to a node can be executed when all tasks connected to this node by incoming edges have completed. Note that
task-dependency graphs can be disconnected and the edge-set of a task-dependency graph can be empty. This is the case for
matrix-vector multiplication, where each task computes a subset of the entries of the product vector. To see a more interesting
task-dependency graph, consider the following database query processing example.
Example 3.2 Database query processing
 shows a relational database of vehicles. Each row of the table is a record that contains data corresponding to a Table 3.1
particular vehicle, such as its ID, model, year, color, etc. in various fields. Consider the computations performed in
processing the following query:
MODEL="Civic" AND YEAR="2001" AND (COLOR="Green" OR COLOR="White")
This query looks for all 2001 Civics whose color is either Green or White. On a relational database, this query is processed
by creating a number of intermediate tables. One possible way is to first create the following four tables: a table containing
all Civics, a table containing all 2001-model cars, a table containing all green-colored cars, and a table containing all
white-colored cars. Next, the computation proceeds by combining these tables by computing their pairwise intersections or
unions. In particular, it computes the intersection of the Civic-table with the 2001-model year table, to construct a table of
all 2001-model Civics. Similarly, it computes the union of the green- and white-colored tables to compute a table storing all
cars whose color is either green or white. Finally, it computes the intersection of the table containing all the 2001 Civics
with the table containing all the green or white vehicles, and returns the desired list.
Table 3.1. A database storing information about used vehicles.
Price Dealer Color Year Model ID#
$18,000 MN Blue 2002 Civic 4523
$15,000 IL White 1999 Corolla 3476
$21,000 NY Green 2001 Camry 7623
$18,000 CA Green 2001 Prius 9834
$17,000 OR White 2001 Civic 6734
$19,000 FL Green 2001 Altima 5342
$22,000 NY Blue 2001 Maxima 3845
$18,000 VT Green 2000 Accord 8354
$17,000 CA Red 2001 Civic 4395
$18,000 WA Red 2002 Civic 7352
 can be visualized by the task-dependency graph shown in Example 3.2 The various computations involved in processing the query in
. Each node in this figure is a task that corresponds to an intermediate table that needs to be computed and the arrows between Figure 3.2
nodes indicate dependencies between the tasks. For example, before we can compute the table that corresponds to the 2001 Civics, we
must first compute the table of all the Civics and a table of all the 2001-model cars.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 3.2. The different tables and their dependencies in a query processing operation.
Note that often there are multiple ways of expressing certain computations, especially those involving associative operators such as
addition, multiplication, and logical AND or OR. Different ways of arranging computations can lead to different task-dependency graphs
 can be solved by first computing a table of all green or Example 3.2 with different characteristics. For instance, the database query in
white cars, then performing an intersection with a table of all 2001 model cars, and finally combining the results with the table of all Civics.
. Figure 3.3 This sequence of computation results in the task-dependency graph shown in
Figure 3.3. An alternate data-dependency graph for the query processing operation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
3.1.2 Granularity, Concurrency, and Task-Interaction
 of the decomposition. A decomposition into granularity The number and size of tasks into which a problem is decomposed determines the
. For coarse-grained  and a decomposition into a small number of large tasks is called fine-grained a large number of small tasks is called
 would usually be considered fine-grained because each of Figure 3.1 example, the decomposition for matrix-vector multiplication shown in
 shows a coarse-grained decomposition of the same problem into four Figure 3.4 a large number of tasks performs a single dot-product.
. n /4 of the entries of the output vector of length n tasks, where each tasks computes
Figure 3.4. Decomposition of dense matrix-vector multiplication into four tasks. The portions of the matrix and the
input and output vectors accessed by Task 1 are highlighted.
. The maximum number of tasks that can be executed simultaneously in degree of concurrency A concept related to granularity is that of
. In most cases, the maximum degree of concurrency maximum degree of concurrency a parallel program at any given time is known as its
is less than the total number of tasks due to dependencies among the tasks. For example, the maximum degree of concurrency in the
 is four. In these task-graphs, maximum concurrency is available right at the beginning when tables for 3.3  and 3.2 task-graphs of Figures
Model, Year, Color Green, and Color White can be computed simultaneously. In general, for task-dependency graphs that are trees, the
maximum degree of concurrency is always equal to the number of leaves in the tree.
, which is the average number of tasks average degree of concurrency A more useful indicator of a parallel program's performance is the
that can run concurrently over the entire duration of execution of the program.
Both the maximum and the average degrees of concurrency usually increase as the granularity of tasks becomes smaller (finer). For
 has a fairly small granularity and a large degree of Figure 3.1 example, the decomposition of matrix-vector multiplication shown in
 has a larger granularity and a smaller degree of concurrency. Figure 3.4 concurrency. The decomposition for the same problem shown in
The degree of concurrency also depends on the shape of the task-dependency graph and the same granularity, in general, does not
, which are abstractions of the task Figure 3.5 guarantee the same degree of concurrency. For example, consider the two task graphs in
, respectively (Problem 3.1). The number inside each node represents the amount of work required to 3.3  and 3.2 graphs of Figures
 is 2.33 and that of the Figure 3.5(a) complete the task corresponding to that node. The average degree of concurrency of the task graph in
 is 1.88 (Problem 3.1), although both task-dependency graphs are based on the same decomposition. Figure 3.5(b) task graph in
, respectively. 3.3  and 3.2 Figure 3.5. Abstractions of the task graphs of Figures
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. In a critical path A feature of a task-dependency graph that determines the average degree of concurrency for a given granularity is its
finish  and the nodes with no outgoing edges by start nodes task-dependency graph, let us refer to the nodes with no incoming edges by
. The longest directed path between any pair of start and finish nodes is known as the critical path. The sum of the weights of nodes nodes
, where the weight of a node is the size or the amount of work associated with the critical path length along this path is known as the
corresponding task. The ratio of the total amount of work to the critical-path length is the average degree of concurrency. Therefore, a
shorter critical path favors a higher degree of concurrency. For example, the critical path length is 27 in the task-dependency graph shown
. Since the total amount of work required to solve the Figure 3.5(b)  and is 34 in the task-dependency graph shown in Figure 3.5(a) in
problems using the two decompositions is 63 and 64, respectively, the average degree of concurrency of the two task-dependency graphs
is 2.33 and 1.88, respectively.
Although it may appear that the time required to solve a problem can be reduced simply by increasing the granularity of decomposition and
utilizing the resulting concurrency to perform more and more tasks in parallel, this is not the case in most practical scenarios. Usually, there
 multiplications and additions in 2 n is an inherent bound on how fine-grained a decomposition a problem permits. For instance, there are
) tasks even by using 2 n ( O  and the problem cannot be decomposed into more than Example 3.1 matrix-vector multiplication considered in
the most fine-grained decomposition.
Other than limited granularity and degree of concurrency, there is another important practical factor that limits our ability to obtain
 among tasks running on interaction unbounded speedup (ratio of serial to parallel execution time) from parallelization. This factor is the
different physical processors. The tasks that a problem is decomposed into often share input, output, or intermediate data. The
dependencies in a task-dependency graph usually result from the fact that the output of one task is the input for another. For example, in
the database query example, tasks share intermediate data; the table generated by one task is often used by another task as input.
Depending on the definition of the tasks and the parallel programming paradigm, there may be interactions among tasks that appear to be
independent in a task-dependency graph. For example, in the decomposition for matrix-vector multiplication, although all tasks are
, tasks may have to b . Since originally there is only one copy of the vector b independent, they all need access to the entire input vector
send and receive messages for all of them to access the entire vector in the distributed-memory paradigm.
. The nodes in a task-interaction graph task-interaction graph The pattern of interaction among tasks is captured by what is known as a
represent tasks and the edges connect tasks that interact with each other. The nodes and edges of a task-interaction graph can be
assigned weights proportional to the amount of computation a task performs and the amount of interaction that occurs along an edge, if
this information is known. The edges in a task-interaction graph are usually undirected, but directed edges can be used to indicate the
direction of flow of data, if it is unidirectional. The edge-set of a task-interaction graph is usually a superset of the edge-set of the
task-dependency graph. In the database query example discussed earlier, the task-interaction graph is the same as the task-dependency
graph. We now give an example of a more interesting task-interaction graph that results from the problem of sparse matrix-vector
multiplication.
Example 3.3 Sparse matrix-vector multiplication
. A matrix is b  x 1 vector n  with a dense A  matrix n  x n  of a sparse Ab  = y Consider the problem of computing the product
considered sparse when a significant number of entries in it are zero and the locations of the non-zero entries do not
conform to a predefined structure or pattern. Arithmetic operations involving sparse matrices can often be optimized
th entryi significantly by avoiding computations involving the zeros. For instance, while computing the
] for only j[ b ] x j , i[ A  of the product vector, we need to compute the products
[8]. b [0, 8]. A [4] + b [0, 4]. A [1] + b [0, 1]. A [0] + b [0, 0]. A [0] = y  0. For example, ] j , i[ A  for which j those values of
 and have each task compute an entry y One possible way of decomposing this computation is to partition the output vector
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
] of the outputi[ y  illustrates this decomposition. In addition to assigning the computation of the element Figure 3.6(a) in it.
] of the input vector. Note that thei[ b , *] of the matrix and the element i[ A , we also make it the "owner" of row i vector to Task
 must get these elementsi  that are owned by other tasks. So Task b ] requires access to many elements ofi[ y computation of
 also inherits thei ],Taski[ b from the appropriate locations. In the message-passing paradigm, with the ownership of
[4] to b ] to all the other tasks that need it for their computation. For example, Task 4 must sendi[ b responsibility of sending
[9] to perform its own computation. The resulting task-interaction b [8], and b [5], b [0], b Tasks 0, 5, 8, and 9 and must get
. Figure 3.6(b) graph is shown in
Figure 3.6. A decomposition for sparse matrix-vector multiplication and the corresponding
.  computesi task-interaction graph. In the decomposition Task
 contains detailed quantitative analysis of overheads due to interaction and limited concurrency and their effect on the Chapter 5
performance and scalability of parallel algorithm-architecture combinations. In this section, we have provided a basic introduction to these
factors because they require important consideration in designing parallel algorithms.
3.1.3 Processes and Mapping
The tasks, into which a problem is decomposed, run on physical processors. However, for reasons that we shall soon discuss, we will use
 in this chapter to refer to a processing or computing agent that performs tasks. In this context, the term process does not process the term
adhere to the rigorous operating system definition of a process. Instead, it is an abstract entity that uses the code and data corresponding
to a task to produce the output of that task within a finite amount of time after the task is activated by the parallel program. During this time,
in addition to performing computations, a process may synchronize or communicate with other processes, if needed. In order to obtain any
speedup over a sequential implementation, a parallel program must have several processes active simultaneously, working on different
. For example, four processes could be mapping tasks. The mechanism by which tasks are assigned to processes for execution is called
. Example 3.5  each in the matrix-multiplication computation of C assigned the task of computing one submatrix of
The task-dependency and task-interaction graphs that result from a choice of decomposition play an important role in the selection of a
good mapping for a parallel algorithm. A good mapping should seek to maximize the use of concurrency by mapping independent tasks
onto different processes, it should seek to minimize the total completion time by ensuring that processes are available to execute the tasks
on the critical path as soon as such tasks become executable, and it should seek to minimize interaction among processes by mapping
tasks with a high degree of mutual interaction onto the same process. In most nontrivial parallel algorithms, these tend to be conflicting
goals. For instance, the most efficient decomposition-mapping combination is a single task mapped onto a single process. It wastes no
time in idling or interacting, but achieves no speedup either. Finding a balance that optimizes the overall parallel performance is the key to
a successful parallel algorithm. Therefore, mapping of tasks onto processes plays an important role in determining how efficient the
resulting parallel algorithm is. Even though the degree of concurrency is determined by the decomposition, it is the mapping that
determines how much of that concurrency is actually utilized, and how efficiently.
 onto four processes. Figure 3.5  shows efficient mappings for the decompositions and the task-interaction graphs of Figure 3.7 For example,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Note that, in this case, a maximum of four processes can be employed usefully, although the total number of tasks is seven. This is
because the maximum degree of concurrency is only four. The last three tasks can be mapped arbitrarily among the processes to satisfy
the constraints of the task-dependency graph. However, it makes more sense to map the tasks connected by an edge onto the same
, if Figure 3.7(b) process because this prevents an inter-task interaction from becoming an inter-processes interaction. For example, in
. In the current mapping, only a single 2 P  will need to interact with 1 P  and 0 P , then both processes 2 P Task 5 is mapped onto process
 suffices. 1 P  and 0 P interaction between
 onto four processes. Figure 3.5 Figure 3.7. Mappings of the task graphs of
3.1.4 Processes versus Processors
In the context of parallel algorithm design, processes are logical computing agents that perform tasks. Processors are the hardware units
that physically perform computations. In this text, we choose to express parallel algorithms and programs in terms of processes. In most
cases, when we refer to processes in the context of a parallel algorithm, there is a one-to-one correspondence between processes and
processors and it is appropriate to assume that there are as many processes as the number of physical CPUs on the parallel computer.
However, sometimes a higher level of abstraction may be required to express a parallel algorithm, especially if it is a complex algorithm
with multiple stages or with different forms of parallelism.
Treating processes and processors separately is also useful when designing parallel programs for hardware that supports multiple
programming paradigms. For instance, consider a parallel computer that consists of multiple computing nodes that communicate with each
other via message passing. Now each of these nodes could be a shared-address-space module with multiple CPUs. Consider
implementing matrix multiplication on such a parallel computer. The best way to design a parallel algorithm is to do so in two stages. First,
develop a decomposition and mapping strategy suitable for the message-passing paradigm and use this to exploit parallelism among the
nodes. Each task that the original matrix multiplication problem decomposes into is a matrix multiplication computation itself. The next step
is to develop a decomposition and mapping strategy suitable for the shared-memory paradigm and use this to implement each task on the
multiple CPUs of a node.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
3.2 Decomposition Techniques
As mentioned earlier, one of the fundamental steps that we need to undertake to solve a problem in parallel is to split the computations to
be performed into a set of tasks for concurrent execution defined by the task-dependency graph. In this section, we describe some
commonly used decomposition techniques for achieving concurrency. This is not an exhaustive set of possible decomposition techniques.
Also, a given decomposition is not always guaranteed to lead to the best parallel algorithm for a given problem. Despite these
shortcomings, the decomposition techniques described in this section often provide a good starting point for many problems and one or a
combination of these techniques can be used to obtain effective decompositions for a large variety of problems.
, and exploratory decomposition , data-decomposition , recursive decomposition These techniques are broadly classified as
 as they can be used to general purpose . The recursive- and data-decomposition techniques are relatively speculative decomposition
special decompose a wide variety of problems. On the other hand, speculative- and exploratory-decomposition techniques are more of a
 nature because they apply to specific classes of problems. purpose
3.2.1 Recursive Decomposition
Recursive decomposition is a method for inducing concurrency in problems that can be solved using the divide-and-conquer strategy. In
this technique, a problem is solved by first dividing it into a set of independent subproblems. Each one of these subproblems is solved by
recursively applying a similar division into smaller subproblems followed by a combination of their results. The divide-and-conquer strategy
results in natural concurrency, as different subproblems can be solved concurrently.
Example 3.4 Quicksort
 elements using the commonly used quicksort algorithm. Quicksort is a n  of A Consider the problem of sorting a sequence
 into two A  and then partitions the sequence x divide and conquer algorithm that starts by selecting a pivot element
 are greater than or 1 A  and all the elements in x  are smaller than 0 A  such that all the elements in 1 A  and 0 A subsequences
 is sorted 1 A  and 0 A  step of the algorithm. Each one of the subsequences divide . This partitioning step forms the x equal to
by recursively calling quicksort. Each one of these recursive calls further partitions the sequences. This is illustrated in
 for a sequence of 12 numbers. The recursion terminates when each subsequence contains only a single Figure 3.8
element.
Figure 3.8. The quicksort task-dependency graph based on recursive decomposition for sorting a
sequence of 12 numbers.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 also represents the task graph for the Figure 3.8 , we define a task as the work of partitioning a given subsequence. Therefore, Figure 3.8 In
problem. Initially, there is only one sequence (i.e., the root of the tree), and we can use only a single process to partition it. The completion
, corresponding to the two nodes at the first level of the tree) and each one can be 1 A  and 0 A of the root task results in two subsequences (
partitioned in parallel. Similarly, the concurrency continues to increase as we move down the tree.
Sometimes, it is possible to restructure a computation to make it amenable to recursive decomposition even if the commonly used
algorithm for the problem is not based on the divide-and-conquer strategy. For example, consider the problem of finding the minimum
, recording at A  elements. The serial algorithm for solving this problem scans the entire sequence n  of A element in an unordered sequence
. It is easy to see that this serial algorithm exhibits no Algorithm 3.1 each step the minimum element found so far as illustrated in
concurrency.
. n  of length A Algorithm 3.1 A serial program for finding the minimum in an array of numbers
) n , A  SERIAL_MIN ( procedure 1.
begin 2.
[0]; A  = min 3.
do  - 1 n to  := 1i for 4.
];i[ A  := min ) min ] <i[ A  ( if 5.
; endfor 6.
; min return 7.
 SERIAL_MIN end 8.
Once we restructure this computation as a divide-and-conquer algorithm, we can use recursive decomposition to extract concurrency.
 into A  is a divide-and-conquer algorithm for finding the minimum element in an array. In this algorithm, we split the sequence Algorithm 3.2
/2, and we find the minimum for each of these subsequences by performing a recursive call. Now the n two subsequences, each of size
overall minimum element is found by selecting the minimum of these two subsequences. The recursion terminates when there is only one
element left in each subsequence. Having restructured the serial computation in this manner, it is easy to construct a task-dependency
 illustrates such a task-dependency graph for finding the minimum of eight numbers where each task is Figure 3.9 graph for this problem.
assigned the work of finding the minimum of two numbers.
Figure 3.9. The task-dependency graph for finding the minimum number in the sequence {4, 9, 1, 7, 8, 11, 2, 12}. Each
node in the tree represents the task of finding the minimum of a pair of numbers.
. n  of length A Algorithm 3.2 A recursive program for finding the minimum in an array of numbers
) n , A  RECURSIVE_MIN ( procedure 1.
begin 2.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
then  = 1) n  ( if 3.
[0]; A  := min 4.
else 5.
/2); n , A  := RECURSIVE_MIN ( lmin 6.
/2); n  - n /2]), n[ A  := RECURSIVE_MIN (&( rmin 7.
then ) rmin  < lmin  ( if 8.
; lmin  := min 9.
else 10.
; rmin  := min 11.
; endelse 12.
; endelse 13.
; min return 14.
 RECURSIVE_MIN end 15.
3.2.2 Data Decomposition
Data decomposition is a powerful and commonly used method for deriving concurrency in algorithms that operate on large data structures.
In this method, the decomposition of computations is done in two steps. In the first step, the data on which the computations are
performed is partitioned, and in the second step, this data partitioning is used to induce a partitioning of the computations into tasks. The
) or Example 3.5 operations that these tasks perform on different data partitions are usually similar (e.g., matrix multiplication introduced in
). Example 3.10 are chosen from a small set of operations (e.g., LU factorization introduced in
The partitioning of data can be performed in many possible ways as discussed next. In general, one must explore and evaluate all possible
ways of partitioning the data and determine which one yields a natural and efficient computational decomposition.
e  In many computations, each element of the output can be computed independently of others as a function of th Partitioning Output Data
input. In such computations, a partitioning of the output data automatically induces a decomposition of the problems into tasks, where
 to Example 3.5 each task is assigned the work of computing a portion of the output. We introduce the problem of matrix-multiplication in
illustrate a decomposition based on partitioning output data.
Example 3.5 Matrix multiplication
 shows a decomposition of Figure 3.10 . C  to yield a matrix B  and A  matrices n  x n Consider the problem of multiplying two
this problem into four tasks. Each matrix is considered to be composed of four blocks or submatrices defined by splitting
/2 each, are then independently n /2 x n , roughly of size C each dimension of the matrix into half. The four submatrices of
. B  and A computed by four tasks as the sums of the appropriate products of submatrices of
Figure 3.10. (a) Partitioning of input and output matrices into 2 x 2 submatrices. (b) A decomposition of
matrix multiplication into four tasks based on the partitioning of the matrices in (a).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Most matrix algorithms, including matrix-vector and matrix-matrix multiplication, can be formulated in terms of block matrix operations. In
such a formulation, the matrix is viewed as composed of blocks or submatrices and the scalar arithmetic operations on its elements are
replaced by the equivalent matrix operations on the blocks. The results of the element and the block versions of the algorithm are
mathematically equivalent (Problem 3.10). Block versions of matrix algorithms are often used to aid decomposition.
 into four submatrices and each of the four tasks C  is based on partitioning the output matrix Figure 3.10 The decomposition shown in
computes one of these submatrices. The reader must note that data-decomposition is distinct from the decomposition of the computation
into tasks. Although the two are often related and the former often aids the latter, a given data-decomposition does not result in a unique
 shows two other decompositions of matrix multiplication, each into eight tasks, Figure 3.11 decomposition into tasks. For example,
. Figure 3.10(a) corresponding to the same data-decomposition as used in
Figure 3.11. Two examples of decomposition of matrix multiplication into eight tasks.
 describes the problem of Example 3.6 We now introduce another example to illustrate decompositions based on data partitioning.
computing the frequency of a set of itemsets in a transaction database, which can be decomposed based on the partitioning of output
data.
Example 3.6 Computing frequencies of itemsets in a transaction database
Consider the problem of computing the frequency of a set of itemsets in a transaction database. In this problem we are
 itemsets. Each transaction and itemset contains a small m  containingI  transactions and a set n  containing T given a set
 could be a grocery stores database of customer sales with T number of items, out of a possible set of items. For example,
each transaction being an individual grocery list of a shopper and each itemset could be a group of items in the store. If the
store desires to find out how many customers bought each of the designated groups of items, then it would need to find
 appears in all the transactions; i.e., the number of transactions of which eachI the number of times that each itemset in
Figure 3.12  shows an example of this type of computation. The database shown in Figure 3.12(a) itemset is a subset of.
consists of 10 transactions, and we are interested in computing the frequency of the eight itemsets shown in the second
column. The actual frequencies of these itemsets in the database, which are the output of the frequency-computing
program, are shown in the third column. For instance, itemset {D, K} appears twice, once in the second and once in the
ninth transaction.
Figure 3.12. Computing itemset frequencies in a transaction database.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 shows how the computation of frequencies of the itemsets can be decomposed into two tasks by partitioning the output into Figure 3.12(b)
two parts and having each task compute its half of the frequencies. Note that, in the process, the itemsets input has also been partitioned,
 is to have each task independently compute the subset of frequencies Figure 3.12(b) but the primary motivation for the decomposition of
assigned to it.
e  Partitioning of output data can be performed only if each output can be naturally computed as a function of th Partitioning Input Data
input. In many algorithms, it is not possible or desirable to partition the output data. For example, while finding the minimum, maximum, or
the sum of a set of numbers, the output is a single unknown value. In a sorting algorithm, the individual elements of the output cannot be
efficiently determined in isolation. In such cases, it is sometimes possible to partition the input data, and then use this partitioning to induce
concurrency. A task is created for each partition of the input data and this task performs as much computation as possible using these local
data. Note that the solutions to tasks induced by input partitions may not directly solve the original problem. In such cases, a follow-up
), p  > N  processes ( p  numbers using N computation is needed to combine the results. For example, while finding the sum of a sequence of
 subsets of nearly equal sizes. Each task then computes the sum of the numbers in one of the subsets. p we can partition the input into
 partial results can be added up to yield the final result. p Finally, the
 can also be decomposed Example 3.6 The problem of computing the frequency of a set of itemsets in a transaction database described in
 shows a decomposition based on a partitioning of the input set of transactions. Each of Figure 3.13(a) based on a partitioning of input data.
the two tasks computes the frequencies of all the itemsets in its respective subset of transactions. The two sets of frequencies, which are
the independent outputs of the two tasks, represent intermediate results. Combining the intermediate results by pairwise addition yields the
final result.
Figure 3.13. Some decompositions for computing itemset frequencies in a transaction database.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
n  In some cases, in which it is possible to partition the output data, partitioning of input data ca Partitioning both Input and Output Data
 for computing itemset frequencies. Figure 3.13(b) offer additional concurrency. For example, consider the 4-way decomposition shown in
Here, both the transaction set and the frequencies are divided into two parts and a different one of the four possible combinations is
assigned to each of the four tasks. Each task then computes a local set of frequencies. Finally, the outputs of Tasks 1 and 3 are added
together, as are the outputs of Tasks 2 and 4.
t  Algorithms are often structured as multi-stage computations such that the output of one stage is the inpu Partitioning Intermediate Data
to the subsequent stage. A decomposition of such an algorithm can be derived by partitioning the input or the output data of an
intermediate stage of the algorithm. Partitioning intermediate data can sometimes lead to higher concurrency than partitioning input or
output data. Often, the intermediate data are not generated explicitly in the serial algorithm for solving the problem and some restructuring
of the original algorithm may be required to use intermediate data partitioning to induce a decomposition.
Let us revisit matrix multiplication to illustrate a decomposition based on partitioning intermediate data. Recall that the decompositions
, have a maximum degree of concurrency of four. 3.11  and 3.10 , as shown in Figures C induced by a 2 x 2 partitioning of the output matrix
We can increase the degree of concurrency by introducing an intermediate stage in which eight tasks compute their respective product
 is the product ofj,i, k D . The submatrix Figure 3.14 , as shown in D submatrices and store the results in a temporary three-dimensional matrix
. k,j B  and k,i A
. D  with partitioning of the three-dimensional intermediate matrix B  and A Figure 3.14. Multiplication of matrices
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 shows this decomposition. After the Figure 3.15  induces a decomposition into eight tasks. D A partitioning of the intermediate matrix
 with the samej,i *, D . All submatrices C multiplication phase, a relatively inexpensive matrix addition step can compute the result matrix
/8) work each 3 n ( O  perform Figure 3.15 . The eight tasks numbered 1 through 8 in j,i C  are added to yield j  andi second and third dimensions
/4) time each in adding the appropriate 2 n ( O . Then, four tasks numbered 9 through 12 spend B  and A /2 submatrices of n /2 x n in multiplying
 shows the task-dependency graph Figure 3.16 . C  to yield the final result matrix D /2 submatrices of the intermediate matrix n /2 x n
. Figure 3.15 corresponding to the decomposition shown in
Figure 3.15. A decomposition of matrix multiplication based on partitioning the intermediate three-dimensional matrix.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Figure 3.15 Figure 3.16. The task-dependency graph of the decomposition shown in
, but are not explicitly stored. By Figure 3.11  are computed implicitly in the original decomposition shown in D Note that all elements of
, we have been able to devise a decomposition with higher concurrency. This, D restructuring the original algorithm and by explicitly storing
however, has been achieved at the cost of extra aggregate memory usage.
owner-computes  A decomposition based on partitioning output or input data is also widely referred to as the The Owner-Computes Rule
rule. The idea behind this rule is that each partition performs all the computations involving data that it owns. Depending on the nature of
the data or the type of data-partitioning, the owner-computes rule may mean different things. For instance, when we assign partitions of
the input data to tasks, then the owner-computes rule means that a task performs all the computations that can be done using these data.
On the other hand, if we partition the output data, then the owner-computes rule means that a task computes all the data in the partition
assigned to it.
3.2.3 Exploratory Decomposition
 is used to decompose problems whose underlying computations correspond to a search of a space for Exploratory decomposition
solutions. In exploratory decomposition, we partition the search space into smaller parts, and search each one of these parts concurrently,
until the desired solutions are found. For an example of exploratory decomposition, consider the 15-puzzle problem.
Example 3.7 The 15-puzzle problem
The 15-puzzle consists of 15 tiles numbered 1 through 15 and one blank tile placed in a 4 x 4 grid. A tile can be moved into
the blank position from a position adjacent to it, thus creating a blank in the tile's original position. Depending on the
configuration of the grid, up to four moves are possible: up, down, left, and right. The initial and final configurations of the
tiles are specified. The objective is to determine any sequence or a shortest sequence of moves that transforms the initial
 illustrates sample initial and final configurations and a sequence of Figure 3.17 configuration to the final configuration.
moves leading from the initial configuration to the final configuration.
Figure 3.17. A 15-puzzle problem instance showing the initial configuration (a), the final configuration (d),
and a sequence of moves leading from the initial to the final configuration.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The 15-puzzle is typically solved using tree-search techniques. Starting from the initial configuration, all possible successor configurations
are generated. A configuration may have 2, 3, or 4 possible successor configurations, each corresponding to the occupation of the empty
slot by one of its neighbors. The task of finding a path from initial to final configuration now translates to finding a path from one of these
newly generated configurations to the final configuration. Since one of these newly generated configurations must be closer to the solution
by one move (if a solution exists), we have made some progress towards finding the solution. The configuration space generated by the
tree search is often referred to as a state space graph. Each node of the graph is a configuration and each edge of the graph connects
configurations that can be reached from one another by a single move of a tile.
One method for solving this problem in parallel is as follows. First, a few levels of configurations starting from the initial configuration are
generated serially until the search tree has a sufficient number of leaf nodes (i.e., configurations of the 15-puzzle). Now each node is
assigned to a task to explore further until at least one of them finds a solution. As soon as one of the concurrent tasks finds a solution it
 illustrates one such decomposition into four tasks in which task 4 finds the Figure 3.18 can inform the others to terminate their searches.
solution.
Figure 3.18. The states generated by an instance of the 15-puzzle problem.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Note that even though exploratory decomposition may appear similar to data-decomposition (the search space can be thought of as being
the data that get partitioned) it is fundamentally different in the following way. The tasks induced by data-decomposition are performed in
their entirety and each task performs useful computations towards the solution of the problem. On the other hand, in exploratory
decomposition, unfinished tasks can be terminated as soon as an overall solution is found. Hence, the portion of the search space
searched (and the aggregate amount of work performed) by a parallel formulation can be very different from that searched by a serial
algorithm. The work performed by the parallel formulation can be either smaller or greater than that performed by the serial algorithm. For
. If the solution lies right at Figure 3.19 example, consider a search space that has been partitioned into four concurrent tasks as shown in
), then it will be found almost immediately by the parallel Figure 3.19(a) the beginning of the search space corresponding to task 3 (
formulation. The serial algorithm would have found the solution only after performing work equivalent to searching the entire space
Figure corresponding to tasks 1 and 2. On the other hand, if the solution lies towards the end of the search space corresponding to task 1 (
), then the parallel formulation will perform almost four times the work of the serial algorithm and will yield no speedup. 3.19(b)
Figure 3.19. An illustration of anomalous speedups resulting from exploratory decomposition.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
3.2.4 Speculative Decomposition
 is used when a program may take one of many possible computationally significant branches depending on Speculative decomposition
the output of other computations that precede it. In this situation, while one task is performing the computation whose output is used in
deciding the next computation, other tasks can concurrently start the computations of the next stage. This scenario is similar to evaluating
 is available. While one task is performing switch  statement in C in parallel before the input for the switch one or more of the branches of a
the computation that will eventually resolve the switch, other tasks could pick up the multiple branches of the switch in parallel. When the
 has finally been computed, the computation corresponding to the correct branch would be used while that switch input for the
corresponding to the other branches would be discarded. The parallel run time is smaller than the serial run time by the amount of time
required to evaluate the condition on which the next task depends because this time is utilized to perform a useful computation for the next
stage in parallel. However, this parallel formulation of a switch guarantees at least some wasteful computation. In order to minimize the
wasted computation, a slightly different formulation of speculative decomposition could be used, especially in situations where one of the
outcomes of the switch is more likely than the others. In this case, only the most promising branch is taken up a task in parallel with the
preceding computation. In case the outcome of the switch is different from what was anticipated, the computation is rolled back and the
correct branch of the switch is taken.
The speedup due to speculative decomposition can add up if there are multiple speculative stages. An example of an application in which
. A detailed description of discrete event simulation is beyond the scope discrete event simulation speculative decomposition is useful is
of this chapter; however, we give a simplified description of the problem.
Example 3.8 Parallel discrete event simulation
Consider the simulation of a system that is represented as a network or a directed graph. The nodes of this network
represent components. Each component has an input buffer of jobs. The initial state of each component or node is idle. An
idle component picks up a job from its input queue, if there is one, processes that job in some finite amount of time, and
puts it in the input buffer of the components which are connected to it by outgoing edges. A component has to wait if the
input buffer of one of its outgoing neighbors if full, until that neighbor picks up a job to create space in the buffer. There is a
finite number of input job types. The output of a component (and hence the input to the components connected to it) and
the time it takes to process a job is a function of the input job. The problem is to simulate the functioning of the network for
a given sequence or a set of sequences of input jobs and compute the total completion time and possibly other aspects of
 shows a simple network for a discrete event solution problem. Figure 3.20 system behavior.
Figure 3.20. A simple network for discrete event simulation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 appears inherently sequential because the Example 3.8 The problem of simulating a sequence of input jobs on the network described in
input of a typical component is the output of another. However, we can define speculative tasks that start simulating a subpart of the
network, each assuming one of several possible inputs to that stage. When an actual input to a certain stage becomes available (as a
result of the completion of another selector task from a previous stage), then all or part of the work required to simulate this input would
have already been finished if the speculation was correct, or the simulation of this stage is restarted with the most recent correct input if the
speculation was incorrect.
Speculative decomposition is different from exploratory decomposition in the following way. In speculative decomposition, the input at a
branch leading to multiple parallel tasks is unknown, whereas in exploratory decomposition, the output of the multiple tasks originating at a
branch is unknown. In speculative decomposition, the serial algorithm would strictly perform only one of the tasks at a speculative stage
because when it reaches the beginning of that stage, it knows exactly which branch to take. Therefore, by preemptively computing for
multiple possibilities out of which only one materializes, a parallel program employing speculative decomposition performs more aggregate
work than its serial counterpart. Even if only one of the possibilities is explored speculatively, the parallel algorithm may perform more or
the same amount of work as the serial algorithm. On the other hand, in exploratory decomposition, the serial algorithm too may explore
different alternatives one after the other, because the branch that may lead to the solution is not known beforehand. Therefore, the parallel
program may perform more, less, or the same amount of aggregate work compared to the serial algorithm depending on the location of the
solution in the search space.
3.2.5 Hybrid Decompositions
So far we have discussed a number of decomposition methods that can be used to derive concurrent formulations of many algorithms.
These decomposition techniques are not exclusive, and can often be combined together. Often, a computation is structured into multiple
stages and it is sometimes necessary to apply different types of decomposition in different stages. For example, while finding the minimum
, available. An P  numbers, a purely recursive decomposition may result in far more tasks than the number of processes, n of a large set of
 roughly equal parts and have each task compute the minimum of the sequence P efficient decomposition would partition the input into
 intermediate results by using the recursive decomposition P assigned to it. The final result can be obtained by finding the minimum of the
. Figure 3.21 shown in
Figure 3.21. Hybrid decomposition for finding the minimum of an array of size 16 using four tasks.
, we used a Example 3.4 As another example of an application of hybrid decomposition, consider performing quicksort in parallel. In
) tasks for the problem of sorting a n ( O recursive decomposition to derive a concurrent formulation of quicksort. This formulation results in
. But due to the dependencies among these tasks and due to uneven sizes of the tasks, the effective concurrency is n sequence of size
) time, which puts an upper limit on the n ( O quite limited. For example, the first task for splitting the input list into two parts takes
performance gain possible via parallelization. But the step of splitting lists performed by tasks in parallel quicksort can also be
. The resulting hybrid decomposition that combines Section 9.4.1 decomposed using the input decomposition technique discussed in
recursive decomposition and the input data-decomposition leads to a highly concurrent formulation of quicksort.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
3.3 Characteristics of Tasks and Interactions
The various decomposition techniques described in the previous section allow us to identify the concurrency that is available in a
problem and decompose it into tasks that can be executed in parallel. The next step in the process of designing a parallel algorithm is to
take these tasks and assign (i.e., map) them onto the available processes. While devising a mapping scheme to construct a good
parallel algorithm, we often take a cue from the decomposition. The nature of the tasks and the interactions among them has a bearing
on the mapping. In this section, we shall discuss the various properties of tasks and inter-task interactions that affect the choice of a
good mapping.
3.3.1 Characteristics of Tasks
The following four characteristics of the tasks have a large influence on the suitability of a mapping scheme.
Static task generation  The tasks that constitute a parallel algorithm may be generated either statically or dynamically. Task Generation
refers to the scenario where all the tasks are known before the algorithm starts execution. Data decomposition usually leads to static
task generation. Examples of data-decomposition leading to a static task generation include matrix-multiplication and LU factorization
(Problem 3.5). Recursive decomposition can also lead to a static task-dependency graph. Finding the minimum of a list of numbers
) is an example of a static recursive task-dependency graph. Figure 3.9 (
 during the execution of the algorithm. In such decompositions, the actual dynamic task generation Certain decompositions lead to a
, although the high level rules or guidelines governing task a priori tasks and the task-dependency graph are not explicitly available
generation are known as a part of the algorithm. Recursive decomposition can lead to dynamic task generation. For example, consider
). The tasks are generated dynamically, and the size and shape of the task tree is Figure 3.8 the recursive decomposition in quicksort (
determined by the values in the input array to be sorted. An array of the same size can lead to task-dependency graphs of different
shapes and with a different total number of tasks.
Exploratory decomposition can be formulated to generate tasks either statically or dynamically. For example, consider the 15-puzzle
. One way to generate a static task-dependency graph using exploratory decomposition is as follows. Section 3.2.3 problem discussed in
First, a preprocessing task starts with the initial configuration and expands the search tree in a breadth-first manner until a predefined
number of configurations are generated. These configuration now represent independent tasks, which can be mapped onto different
processes and run independently. A different decomposition that generates tasks dynamically would be one in which a task takes a state
as input, expands it through a predefined number of steps of breadth-first search and spawns new tasks to perform the same
computation on each of the resulting states (unless it has found the solution, in which case the algorithm terminates).
s  The size of a task is the relative amount of time required to complete it. The complexity of mapping schemes often depend Task Sizes
; i.e., whether or not they require roughly the same amount of time. If the amount of time uniform on whether or not the tasks are
. For example, the tasks in the decompositions for matrix non-uniform required by the tasks varies significantly, then they are said to be
 are Figure 3.8  would be considered uniform. On the other hand, the tasks in quicksort in 3.11  and 3.10 multiplication shown in Figures
non-uniform.
e  The third characteristic that influences the choice of mapping scheme is knowledge of the task size. If th Knowledge of Task Sizes
size of all the tasks is known, then this information can often be used in mapping of tasks to processes. For example, in the various
decompositions for matrix multiplication discussed so far, the computation time for each task is known before the parallel program starts.
 how many moves will lead to a priori On the other hand, the size of a typical task in the 15-puzzle problem is unknown. We do not know
the solution from a given state.
s  Another important characteristic of a task is the size of data associated with it. The reason this i Size of Data Associated with Tasks
an important consideration for mapping is that the data associated with a task must be available to the process performing that task, and
the size and the location of these data may determine the process that can perform the task without incurring excessive data-movement
overheads.
Different types of data associated with a task may have different sizes. For instance, the input data may be small but the output may be
large, or vice versa. For example, the input to a task in the 15-puzzle problem may be just one state of the puzzle. This is a small input
relative to the amount of computation that may be required to find a sequence of moves from this state to a solution state. In the problem
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
of computing the minimum of a sequence, the size of the input is proportional to the amount of computation, but the output is just one
number. In the parallel formulation of the quick sort, the size of both the input and the output data is of the same order as the sequential
time needed to solve the task.
3.3.2 Characteristics of Inter-Task Interactions
In any nontrivial parallel algorithm, tasks need to interact with each other to share data, work, or synchronization information. Different
parallel algorithms require different types of interactions among concurrent tasks. The nature of these interactions makes them more
suitable for certain programming paradigms and mapping schemes, and less suitable for others. The types of inter-task interactions can
be described along different dimensions, each corresponding to a distinct characteristic of the underlying computations.
r  One way of classifying the type of interactions that take place among concurrent tasks is to consider whether o Static versus Dynamic
 pattern. An interaction pattern is static if for each task, the interactions happen at dynamic  or static not these interactions have a
predetermined times, and if the set of tasks to interact with at these times is known prior to the execution of the algorithm. In other words,
, but the stage of the computation at which each a priori in a static interaction pattern, not only is the task-interaction graph known
interaction occurs is also known. An interaction pattern is dynamic if the timing of interactions or the set of tasks to interact with cannot
be determined prior to the execution of the algorithm.
e Static interactions can be programmed easily in the message-passing paradigm, but dynamic interactions are harder to program. Th
f reason is that interactions in message-passing require active involvement of both interacting tasks – the sender and the receiver o
e information. The unpredictable nature of dynamic iterations makes it hard for both the sender and the receiver to participate in th
g interaction at the same time. Therefore, when implementing a parallel algorithm with dynamic interactions in the message-passin
e paradigm, the tasks must be assigned additional synchronization or polling responsibility. Shared-address space programming can cod
. both types of interactions equally easily
The decompositions for parallel matrix multiplication presented earlier in this chapter exhibit static inter-task interactions. For an example
of dynamic interactions, consider solving the 15-puzzle problem in which tasks are assigned different states to explore after an initial step
that generates the desirable number of states by applying breadth-first search on the initial state. It is possible that a certain state leads
to all dead ends and a task exhausts its search space without reaching the goal state, while other tasks are still busy trying to find a
solution. The task that has exhausted its work can pick up an unexplored state from the queue of another busy task and start exploring it.
The interactions involved in such a transfer of work from one task to another are dynamic.
s  Another way of classifying the interactions is based upon their spatial structure. An interaction pattern i Regular versus Irregular
 if it has some structure that can be exploited for efficient implementation. On the other hand, an interaction regular considered to be
 if no such regular pattern exists. Irregular and dynamic communications are harder to handle, particularly in the irregular pattern is called
message-passing programming paradigm. An example of a decomposition with a regular interaction pattern is the problem of image
dithering.
Example 3.9 Image dithering
In image dithering, the color of each pixel in the image is determined as the weighted average of its original value and
the values of its neighboring pixels. We can easily decompose this computation, by breaking the image into square
regions and using a different task to dither each one of these regions. Note that each task needs to access the pixel
values of the region assigned to it as well as the values of the image surrounding its region. Thus, if we regard the
tasks as nodes of a graph with an edge linking a pair of interacting tasks, the resulting pattern is a two-dimensional
. Figure 3.22 mesh, as shown in
Figure 3.22. The regular two-dimensional task-interaction graph for image
dithering. The pixels with dotted outline require color values from the boundary
pixels of the neighboring tasks.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure  provides a good example of irregular interaction, which is shown in Section 3.1.2 Sparse matrix-vector multiplication discussed in
 it needs to A  which rows of matrix a priori . In this decomposition, even though each task, by virtue of the decomposition, knows 3.6
 it requires. The reason is that the b  assigned to it, a task cannot know which entries of vector A access, without scanning the row(s) of
. A  depends on the structure of the sparse matrix b access pattern for
e  We have already learned that sharing of data among tasks leads to inter-task interaction. However, th Read-only versus Read-Write
read-write  or read-only type of sharing may impact the choice of the mapping. Data sharing interactions can be categorized as either
interactions. As the name suggests, in read-only interactions, tasks require only a read-access to the data shared among many
, the tasks only need to read Figure 3.10 concurrent tasks. For example, in the decomposition for parallel matrix multiplication shown in
. In read-write interactions, multiple tasks need to read and write on some shared data. For example, B  and A the shared input matrices
 uses an exhaustive search to Section 3.2.3 consider the problem of solving the 15-puzzle. The parallel formulation method proposed in
find a solution. In this formulation, each state is considered an equally suitable candidate for further expansion. The search can be made
more efficient if the states that appear to be closer to the solution are given a priority for further exploration. An alternative search
technique known as heuristic search implements such a strategy. In heuristic search, we use a heuristic to provide a relative
approximate indication of the distance of each state from the solution (i.e. the potential number of moves required to reach the solution).
In the case of the 15-puzzle, the number of tiles that are out of place in a given state could serve as such a heuristic. The states that
need to be expanded further are stored in a priority queue based on the value of this heuristic. While choosing the states to expand, we
give preference to more promising states, i.e. the ones that have fewer out-of-place tiles and hence, are more likely to lead to a quick
solution. In this situation, the priority queue constitutes shared data and tasks need both read and write access to it; they need to put the
states resulting from an expansion into the queue and they need to pick up the next most promising state for the next expansion.
r  In some interactions, the data or work needed by a task or a subset of tasks is explicitly supplied by anothe One-way versus Two-way
 interactions and usually involve predefined producer and consumer tasks. two-way task or subset of tasks. Such interactions are called
In other interactions, only one of a pair of communicating tasks initiates the interaction and completes it without interrupting the other one.
 interaction. All read-only interactions can be formulated as one-way interactions. Read-write one-way Such an interaction is called a
interactions can be either one-way or two-way.
The shared-address-space programming paradigms can handle both one-way and two-way interactions equally easily. However,
one-way interactions cannot be directly programmed in the message-passing paradigm because the source of the data to be transferred
must explicitly send the data to the recipient. In the message-passing paradigm, all one-way interactions must be converted to two-way
interactions via program restructuring. Static one-way interactions can be easily converted to two-way communications. Since the time
, introducing a matching interaction operation in the a priori and the location in the program of a static one-way interaction is known
partner task is enough to convert a one-way static interaction to a two-way static interaction. On the other hand, dynamic one-way
interactions can require some nontrivial program restructuring to be converted to two-way interactions. The most common such
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
restructuring involves polling. Each task checks for pending requests from other tasks after regular intervals, and services such requests,
if any.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
3.4 Mapping Techniques for Load Balancing
Once a computation has been decomposed into tasks, these tasks are mapped onto processes with the objective that all tasks complete
 of executing the tasks in parallel must overheads in the shortest amount of elapsed time. In order to achieve a small execution time, the
be minimized. For a given decomposition, there are two key sources of overhead. The time spent in inter-process interaction is one source
of overhead. Another important source of overhead is the time that some processes may spend being idle. Some processes can be idle
even before the overall computation is finished for a variety of reasons. Uneven load distribution may cause some processes to finish
earlier than others. At times, all the unfinished tasks mapped onto a process may be waiting for tasks mapped onto other processes to
finish in order to satisfy the constraints imposed by the task-dependency graph. Both interaction and idling are often a function of mapping.
Therefore, a good mapping of tasks onto processes must strive to achieve the twin objectives of (1) reducing the amount of time
processes spend in interacting with each other, and (2) reducing the total amount of time some processes are idle while the others are
engaged in performing some tasks.
These two objectives often conflict with each other. For example, the objective of minimizing the interactions can be easily achieved by
assigning sets of tasks that need to interact with each other onto the same process. In most cases, such a mapping will result in a highly
unbalanced workload among the processes. In fact, following this strategy to the limit will often map all tasks onto a single process. As a
result, the processes with a lighter load will be idle when those with a heavier load are trying to finish their tasks. Similarly, to balance the
load among processes, it may be necessary to assign tasks that interact heavily to different processes. Due to the conflicts between these
objectives, finding a good mapping is a nontrivial problem.
In this section, we will discuss various schemes for mapping tasks onto processes with the primary view of balancing the task workload of
. The reader should be aware that Section 3.5 processes and minimizing their idle time. Reducing inter-process interaction is the topic of
assigning a balanced aggregate load of tasks to each process is a necessary but not sufficient condition for reducing process idling. Recall
that the tasks resulting from a decomposition are not all ready for execution at the same time. A task-dependency graph determines which
tasks can execute in parallel and which must wait for some others to
finish at a given stage in the execution of a parallel algorithm. Therefore, it is possible in a certain parallel formulation that although all
processes perform the same aggregate amount of work, at different times, only a fraction of the processes are active while the remainder
contain tasks that must wait for other tasks to finish. Similarly, poor synchronization among interacting tasks can lead to idling if one of the
tasks has to wait to send or receive data from another task. A good mapping must ensure that the computations and interactions among
 shows two mappings of 12-task Figure 3.23 processes at each stage of the execution of the parallel algorithm are well balanced.
decomposition in which the last four tasks can be started only after the first eight are finished due to dependencies among tasks. As the
figure shows, two mappings, each with an overall balanced workload, can result in different completion times.
Figure 3.23. Two mappings of a hypothetical decomposition with a synchronization.
. The parallel dynamic  and static Mapping techniques used in parallel algorithms can be broadly classified into two categories:
programming paradigm and the characteristics of tasks and the interactions among them determine whether a static or a dynamic mapping
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
is more suitable.
r  Static mapping techniques distribute the tasks among processes prior to the execution of the algorithm. Fo Static Mapping:
statically generated tasks, either static or dynamic mapping can be used. The choice of a good mapping in this case depends on
several factors, including the knowledge of task sizes, the size of data associated with tasks, the characteristics of inter-task
interactions, and even the parallel programming paradigm. Even when task sizes are known, in general, the problem of obtaining
an optimal mapping is an NP-complete problem for non-uniform tasks. However, for many practical cases, relatively inexpensive
heuristics provide fairly acceptable approximate solutions to the optimal static mapping problem.
Algorithms that make use of static mapping are in general easier to design and program.
f  Dynamic mapping techniques distribute the work among processes during the execution of the algorithm. I Dynamic Mapping:
tasks are generated dynamically, then they must be mapped dynamically too. If task sizes are unknown, then a static mapping
can potentially lead to serious load-imbalances and dynamic mappings are usually more effective. If the amount of data
associated with tasks is large relative to the computation, then a dynamic mapping may entail moving this data among
processes. The cost of this data movement may outweigh some other advantages of dynamic mapping and may render a static
mapping more suitable. However, in a shared-address-space paradigm, dynamic mapping may work well even with large data
associated with tasks if the interaction is read-only. The reader should be aware that the shared-address-space programming
Section paradigm does not automatically provide immunity against data-movement costs. If the underlying hardware is NUMA (
), then the data may physically move from a distant memory. Even in a cc-UMA architecture, the data may have to move 2.3.2
from one cache to another.
Algorithms that require dynamic mapping are usually more complicated, particularly in the message-passing programming
paradigm.
Having discussed the guidelines for choosing between static and dynamic mappings, we now describe various schemes of these two
types of mappings in detail.
3.4.1 Schemes for Static Mapping
Static mapping is often, though not exclusively, used in conjunction with a decomposition based on data partitioning. Static mapping is
also used for mapping certain problems that are expressed naturally by a static task-dependency graph. In the following subsections, we
will discuss mapping schemes based on data partitioning and task partitioning.
Mappings Based on Data Partitioning
In this section, we will discuss mappings based on partitioning two of the most common ways of representing data in algorithms, namely,
arrays and graphs. The data-partitioning actually induces a decomposition, but the partitioning or the decomposition is selected with the
final mapping in mind.
e  In a decomposition based on partitioning data, the tasks are closely associated with portions of data by th Array Distribution Schemes
owner-computes rule. Therefore, mapping the relevant data onto the processes is equivalent to mapping tasks onto processes. We now
study some commonly used techniques of distributing arrays or matrices among processes.
Block Distributions
 are some of the simplest ways to distribute an array and assign uniform contiguous portions of the array to different Block distributions
-dimensional array is distributed among the processes such that each process receives a contiguous d processes. In these distributions, a
block of array entries along a specified subset of array dimensions. Block distributions of arrays are particularly suitable when there is a
locality of interaction, i.e., computation of an element of an array requires other nearby elements in the array.
 columns. We can now select one of these dimensions, e.g., the n  rows and n  with A  two-dimensional array n  x n For example, consider an
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. That is, each p  < k  - 1, where 0 p/ n )  + 1 k ...( p/ kn th part contains rows k  parts such that the p first dimension, and partition the array into
 along the second dimension, then each partition contains a A . Similarly, if we partition A  consecutive rows of p/ n partition contains a block of
. Figure 3.24  consecutive columns. These row- and column-wise array distributions are illustrated in p/ n block of
Figure 3.24. Examples of one-dimensional partitioning of an array among eight processes.
 we A Similarly, instead of selecting a single dimension, we can select multiple dimensions to partition. For instance, in the case of array
 section of the matrix, with 2 p/ n  x 1 p/ n can select both dimensions and partition the matrix into blocks such that each block corresponds to a
 illustrates two different two-dimensional distributions, on a 4 x 4 and 2x 8 process Figure 3.25  being the number of processes. 2 p  x 1 p  = p
-dimensional block distribution. d -dimensional array, we can distribute it using up to a d grid, respectively. In general, given a
Figure 3.25. Examples of two-dimensional distributions of an array, (a) on a 4 x 4 process grid, and (b) on a 2 x 8
process grid.
Using these block distributions we can load-balance a variety of parallel computations that operate on multi-dimensional arrays. For
. One way of decomposing this computation is to Section 3.2.2  , as discussed in B  x A  = C  matrix multiplication n  x n example, consider the
 requires the same amount of computation, we can balance the computations by using C  . Since each entry of C partition the output matrix
 available processes. In the first case, each process p  uniformly among the C either a one- or two-dimensional block distribution to partition
. In , whereas in the second case, each process will get a block of size C  rows (or columns) of p/ n will get a block of
 assigned to it. C either case, the process will be responsible for computing the entries of the partition of
As the matrix-multiplication example illustrates, quite often we have the choice of mapping the computations using either a one- or a
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
two-dimensional distribution (and even more choices in the case of higher dimensional arrays). In general, higher dimensional distributions
allow us to use more processes. For example, in the case of matrix-matrix multiplication, a one-dimensional distribution will allow us to use
 to each process. On the other hand, a two-dimensional distribution will allow us to use up C  processes by assigning a single row of n up to
 to each process. C  processes by assigning a single element of 2 n to
In addition to allowing a higher degree of concurrency, higher dimensional distributions also sometimes help in reducing the amount of
 illustrates this in the case of dense matrix-multiplication. With a Figure 3.26 interactions among the different processes for many problems.
, B  and the entire matrix A  rows of matrix p/ n one-dimensional partitioning along the rows, each process needs to access the corresponding
. However, with a 2 n  + p/ 2 n . Thus the total amount of data that needs to be accessed is 5 P  for process Figure 3.26(a) as shown in
 columns of matrix B, as shown in  and A  rows of matrix two-dimensional distribution, each process needs to access
. In the two-dimensional case, the total amount of shared data that each process needs to access is 5 P  for process Figure 3.26(b)
) shared data in the one-dimensional case. 2 n ( O , which is significantly smaller compared to
Figure 3.26. Data sharing needed for matrix multiplication with (a) one-dimensional and (b) two-dimensional
 are required by the process that B  and A partitioning of the output matrix. Shaded portions of the input matrices
. C computes the shaded portion of the output matrix
Cyclic and Block-Cyclic Distributions
If the amount of work differs for different elements of a matrix, a block distribution can potentially lead to load imbalances. A classic
example of this phenomenon is LU factorization of a matrix, in which the amount of computation increases from the top left to the bottom
right of the matrix.
Example 3.10 Dense LU factorization
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 into the product of a lower A In its simplest form,the LU factorization algorithm factors a nonsingular square matrix
 be A  shows the serial algorithm. Let Algorithm 3.3 . U  with a unit diagonal and an upper triangular matrix L triangular matrix
h  major steps – eac n . The factorization process consists of n  matrix with rows and columns numbered from 1 to n  x n an
] k , n  + 1 : k[ A , first, the partial column k . In step Algorithm 3.3 consisting of an iteration of the outer loop starting at Line 3 in
k[ A ) submatrix k  - n ) x ( k  - n ] is subtracted from the ( n  + 1 : k , k[ A ] x k , n  + 1 : k[ A ]. Then, the outer product k , k[ A is divided by
 is modified A  and U  and L ]. In a practical implementation of LU factorization, separate arrays are not used for n  + 1 : k , n + 1 :
 are implicit and L  in its lower and upper triangular parts, respectively. The 1's on the principal diagonal of U  and L to store
 after factorization. U the diagonal entries actually belong to
 into a lower-triangular A Algorithm 3.3 A serial column-based algorithm to factor a nonsingular matrix
] on the left j ,i [ A . On Line 9, A  share space with U  and L . Matrices U  and an upper-triangular matrix L matrix
]. j ,i  [ U ; otherwise, it is equivalent to j  >i ] if j ,i  [ L side of the assignment is equivalent to
) A  COL_LU ( procedure 1.
begin 2.
do n to  := 1 k for 3.
do n to k  := j for 4.
]; k , k[ A ]/ k , j[ A ]:= k , j[ A 5.
; endfor 6.
do n to  + 1 k  := j for 7.
do n to  + 1 k  :=i for 8.
]; j , k[ A ] x k , i[ A ] - j , i[ A ] := j , i[ A 9.
; endfor 10.
; endfor 11.
 /*
th k ] is logically the k , n  + 1 : k[ A After this iteration, column
. U th row of k ] is logically the n  : k , k[ A  and row L column of
 */
; endfor 12.
 COL_LU end 13.
 shows a possible decomposition of LU factorization into 14 tasks using a 3 x 3 block partitioning of the matrix Figure 3.27
. Algorithm 3.3 and using a block version of
Figure 3.27. A decomposition of LU factorization into 14 tasks.
. In other words, the active part of n  + 1 to k  goes from Algorithm 3.3 , the next nested loop in n  := 1 to k For each iteration of the outer loop
, shrinks towards the bottom right corner of the matrix as the computation proceeds. Therefore, in a Figure 3.28 the matrix, as shown in
block distribution, the processes assigned to the beginning rows and columns (i.e., left rows and top columns) would perform far less work
Figure 3.27 than those assigned to the later rows and columns. For example, consider the decomposition for LU factorization shown in
with a 3 x 3 two-dimensional block partitioning of the matrix. If we map all tasks associated with a certain block onto a process in a
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
9-process ensemble, then a significant amount of idle time will result. First, computing different blocks of the matrix requires different
y ) requires onl 1,1 U 1,1 L  (which is actually 1,1 A . For example, computing the final value of Figure 3.29 amounts of work. This is illustrated in
e  requires three tasks – Task 9, Task 13, and Task 14. Secondly, th 3,3 A one task – Task 1. On the other hand, computing the final value of
e process working on a block may idle even when there are unfinished tasks associated with that block. This idling can occur if th
s constraints imposed by the task-dependency graph do not allow the remaining tasks on this process to proceed until one or more task
. mapped onto other processes are completed
th k Figure 3.28. A typical computation in Gaussian elimination and the active part of the coefficient matrix during the
iteration of the outer loop.
Figure 3.29. A naive mapping of LU factorization tasks onto processes based on a two-dimensional block distribution.
 is a variation of the block distribution scheme that can be used to alleviate the load-imbalance and idling block-cyclic distribution The
, where it is shown how a block-cyclic Chapter 8 problems. A detailed description of LU factorization with block-cyclic mapping is covered in
. The central idea behind a block-cyclic distribution is Figure 3.29 mapping leads to a substantially more balanced work distribution than in
to partition an array into many more blocks than the number of available processes. Then we assign the partitions (and the associated
tasks) to processes in a round-robin manner so that each process gets several non-adjacent blocks. More precisely, in a one-dimensional
) p a /( n  groups of p a  matrix are divided into n  x n  processes, the rows (columns) of an p block-cyclic distribution of a matrix among
 processes in a wraparound fashion p . Now, these blocks are distributed among the p/ n a consecutive rows (columns), where 1
 blocks of the matrix to each process, a  ('%' is the modulo operator). This distribution assigns p % i P  is assigned to processi b such that block
 blocks away. We can obtain a two-dimensional block-cyclic p but each subsequent block that gets assigned to the same process is
 and distributing them on a hypothetical  array by partitioning it into square blocks of size n  x n distribution of an
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 array of processes in a wraparound fashion. Similarly, the block-cyclic distribution can be extended to arrays of higher
 illustrates one- and two-dimensional block cyclic distributions of a two-dimensional array. Figure 3.30 dimensions.
Figure 3.30. Examples of one- and two-dimensional block-cyclic distributions among four processes. (a) The rows of
the array are grouped into blocks each consisting of two rows, resulting in eight blocks of rows. These blocks are
distributed to four processes in a wraparound fashion. (b) The matrix is blocked into 16 blocks each of size 4 x 4, and it
is mapped onto a 2 x 2 grid of processes in a wraparound fashion.
The reason why a block-cyclic distribution is able to significantly reduce the amount of idling is that all processes have a sampling of tasks
from all parts of the matrix. As a result, even if different parts of the matrix require different amounts of work, the overall work on each
process balances out. Also, since the tasks assigned to a process belong to different parts of the matrix, there is a good chance that at
least some of them are ready for execution at any given time.
, then each block is a single row (column) of the matrix in a one-dimensional block-cyclic p/ n  to its upper limit of a Note that if we increase
cyclic distribution and a single element of the matrix in a two-dimensional block-cyclic distribution. Such a distribution is known as a
. A cyclic distribution is an extreme case of a block-cyclic distribution and can result in an almost perfect load balance due to distribution
the extreme fine-grained underlying decomposition. However, since a process does not have any contiguous data to work on, the resulting
lack of locality may result in serious performance penalties. Additionally, such a decomposition usually leads to a high degree of
 results in maximum locality and a interaction relative to the amount computation in each task. The lower limit of 1 for the value of
 must be used to strike a a interaction optimality, but the distribution degenerates to a block distribution. Therefore, an appropriate value of
balance between interaction conservation and load balance.
As in the case of block-distributions, higher dimensional block-cyclic distributions are usually preferable as they tend to incur a lower
volume of inter-task interaction.
Randomized Block Distributions
A block-cyclic distribution may not always be able to balance computations when the distribution of work has some special patterns. For
 in which the shaded areas correspond to regions containing nonzero Figure 3.31(a) example, consider the sparse matrix shown in
, then we will end up Figure 3.31(b) elements. If this matrix is distributed using a two-dimensional block-cyclic distribution, as illustrated in
 than on any other processes. In fact some processes, like 15 P , and 10 P , 5 P , 0 P assigning more non-zero blocks to the diagonal processes
, will not get any work. 12 P
Figure 3.31. Using the block-cyclic distribution shown in (b) to distribute the computations performed in array (a) will
lead to load imbalances.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Just like Figure 3.31 , a more general form of the block distribution, can be used in situations illustrated in Randomized block distribution
a block-cyclic distribution, load balance is sought by partitioning the array into many more blocks than the number of available processes.
However, the blocks are uniformly and randomly distributed among the processes. A one-dimensional randomized block distribution can
 is V . Now, p a  < j  for 0 j ] is set to j[ V  (which is equal to the number of blocks) is used and p a  of length V be achieved as follows. A vector
 = 3. A a  = 4 and p  illustrates this for Figure 3.32  - 1]. a  + 1)i ...( ai[ V  is assigned the blocks stored ini P randomly permuted, and process
 array can be computed similarly by randomly permuting two vectors of length n  x n two-dimensional randomized block distribution of an
Figure  each and using them to choose the row and column indices of the blocks to be assigned to each process. As illustrated in
. Figure 3.31 , the random block distribution is more effective in load balancing the computations performed in 3.33
 = 3). a Figure 3.32. A one-dimensional randomized block mapping of 12 blocks onto four process (i.e.,
Figure 3.33. Using a two-dimensional random block distribution shown in (b) to distribute the computations performed
in array (a), as shown in (c).
d  The array-based distribution schemes that we described so far are quite effective in balancing the computations an Graph Partitioning
minimizing the interactions for a wide range of algorithms that use dense matrices and have structured and regular interaction patterns.
However, there are many algorithms that operate on sparse data structures and for which the pattern of interaction among data elements
is data dependent and highly irregular. Numerical simulations of physical phenomena provide a large source of such type of computations.
In these computations, the physical domain is discretized and represented by a mesh of elements. The simulation of the physical
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
phenomenon being modeled then involves computing the values of certain physical quantities at each mesh point. The computation at a
mesh point usually requires data corresponding to that mesh point and to the points that are adjacent to it in the mesh. For example,
 shows a mesh imposed on Lake Superior. The simulation of a physical phenomenon such the dispersion of a water Figure 3.34
contaminant in the lake would now involve computing the level of contamination at each vertex of this mesh at various intervals of time.
Figure 3.34. A mesh used to model Lake Superior.
Since, in general, the amount of computation at each point is the same, the load can be easily balanced by simply assigning the same
number of mesh points to each process. However, if a distribution of the mesh points to processes does not strive to keep nearby mesh
points together, then it may lead to high interaction overheads due to excessive data sharing. For example, if each process receives a
, then each process will need to access a large set of points belonging to other processes Figure 3.35 random set of points as illustrated in
to complete computations for its assigned portion of the mesh.
Figure 3.35. A random distribution of the mesh elements to eight processes.
Ideally, we would like to distribute the mesh points in a way that balances the load and at the same time minimizes the amount of data that
 parts such that each p each process needs to access in order to complete its computations. Therefore, we need to partition the mesh into
part contains roughly the same number of mesh-points or vertices, and the number of edges that cross partition boundaries (i.e., those
edges that connect points belonging to two different partitions) is minimized. Finding an exact optimal partition is an NP-complete problem.
However, algorithms that employ powerful heuristics are available to compute reasonable partitions. After partitioning the mesh in this
 processes. As a result, each process is assigned a contiguous region of p  partitions is assigned to one of the p manner, each one of these
Figure 3.36 the mesh such that the total number of mesh points that needs to be accessed across partition boundaries is minimized.
. shows a good partitioning of the Lake Superior mesh – the kind that a typical graph partitioning software would generate
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 3.36. A distribution of the mesh elements to eight processes, by using a graph-partitioning algorithm.
Mappings Based on Task Partitioning
A mapping based on partitioning a task-dependency graph and mapping its nodes onto processes can be used when the computation is
naturally expressible in the form of a static task-dependency graph with tasks of known sizes. As usual, this mapping must seek to achieve
the often conflicting objectives of minimizing idle time and minimizing the interaction time of the parallel algorithm. Determining an optimal
mapping for a general task-dependency graph is an NP-complete problem. However, specific situations often lend themselves to a simpler
optimal or acceptable approximate solution.
As a simple example of a mapping based on task partitioning, consider a task-dependency graph that is a perfect binary tree. Such a
task-dependency graph can occur in practical problems with recursive decomposition, such as the decomposition for finding the minimum
 shows a mapping of this task-dependency graph onto eight processes. It is easy to see that Figure 3.37 ). Figure 3.9 of a list of numbers (
this mapping minimizes the interaction overhead by mapping many interdependent tasks onto the same process (i.e., the tasks along a
straight branch of the tree) and others on processes only one communication link away from each other. Although there is some inevitable
idling (e.g., when process 0 works on the root task, all other processes are idle), this idling is inherent in the task-dependency graph. The
 does not introduce any further idling and all tasks that are permitted to be concurrently active by the Figure 3.37 mapping shown in
task-dependency graph are mapped onto different processes for parallel execution.
Figure 3.37. Mapping of a binary tree task-dependency graph onto a hypercube of processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
For some problems, an approximate solution to the problem of finding a good mapping can be obtained by partitioning the task-interaction
graph. In the problem of modeling contaminant dispersion in Lake Superior discussed earlier in the context of data partitioning, we can
define tasks such that each one of them is responsible for the computations associated with a certain mesh point. Now the mesh used to
discretize the lake also acts as a task-interaction graph. Therefore, for this problem, using graph-partitioning to find a good mapping can
also be viewed as task partitioning.
. A Section 3.1.2 Another similar problem where task partitioning is applicable is that of sparse matrix-vector multiplication discussed in
. This mapping assigns tasks corresponding to four Figure 3.38  is shown in Figure 3.6 simple mapping of the task-interaction graph of
 shows another partitioning for the task-interaction graph of the sparse matrix vector Figure 3.39  to each process. b consecutive entries of
 need toi  that the tasks on Process b  contains the indices ofi  for three processes. The list C Figure 3.6 multiplication problem shown in
2 in the two cases readily reveals that the C 1, and C 0, C access from tasks mapped onto other processes. A quick comparison of the lists
 between processes than the naive b mapping based on partitioning the task interaction graph entails fewer exchanges of elements of
mapping.
b  contains the indices ofi Figure 3.38. A mapping for sparse matrix-vector multiplication onto three processes. The list C
 needs to access from other processes.i that Process
Figure 3.39. Reducing interaction overhead in sparse matrix-vector multiplication by partitioning the task-interaction
graph.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Hierarchical Mappings
Certain algorithms are naturally expressed as task-dependency graphs; however, a mapping based solely on the task-dependency graph
, only a Figure 3.37 may suffer from load-imbalance or inadequate concurrency. For example, in the binary-tree task-dependency graph of
few tasks are available for concurrent execution in the top part of the tree. If the tasks are large enough, then a better mapping can be
obtained by a further decomposition of the tasks into smaller subtasks. In the case where the task-dependency graph is a binary tree with
four levels, the root task can be partitioned among eight processes, the tasks at the next level can be partitioned among four processes
each, followed by tasks partitioned among two processes each at the next level. The eight leaf tasks can have a one-to-one mapping onto
 has a task-dependency Example 3.4  illustrates such a hierarchical mapping. Parallel quicksort introduced in Figure 3.40 the processes.
. Figure 3.40 , and hence is an ideal candidate for a hierarchical mapping of the type shown in Figure 3.37 graph similar to the one shown in
Figure 3.40. An example of hierarchical mapping of a task-dependency graph. Each node represented by an array is a
supertask. The partitioning of the arrays represents subtasks, which are mapped onto eight processes.
An important practical problem to which the hierarchical mapping example discussed above applies directly is that of sparse matrix
factorization. The high-level computations in sparse matrix factorization are guided by a task-dependency graph which is known as an
 if the matrix is symmetric). However, the tasks in the elimination graph (especially the ones closer to the elimination tree  ( elimination graph
root) usually involve substantial computations and are further decomposed into subtasks using data-decomposition. A hierarchical
mapping, using task partitioning at the top layer and array partitioning at the bottom layer, is then applied to this hybrid decomposition. In
general, a hierarchical mapping can have many layers and different decomposition and mapping techniques may be suitable for different
layers.
3.4.2 Schemes for Dynamic Mapping
Dynamic mapping is necessary in situations where a static mapping may result in a highly imbalanced distribution of work among
processes or where the task-dependency graph itself if dynamic, thus precluding a static mapping. Since the primary reason for using a
dynamic mapping is balancing the workload among processes, dynamic mapping is often referred to as dynamic load-balancing. Dynamic
. distributed  or centralized mapping techniques are usually classified as either
Centralized Schemes
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In a centralized dynamic load balancing scheme, all executable tasks are maintained in a common central data structure or they are
maintained by a special process or a subset of processes. If a special process is designated to manage the pool of available tasks, then it
. Whenever a slaves  and the other processes that depend on the master to obtain work are referred to as master is often referred to as the
process has no work, it takes a portion of available work from the central data structure or the master process. Whenever a new task is
generated, it is added to this centralized data structure or reported to the master process. Centralized load-balancing schemes are usually
easier to implement than distributed schemes, but may have limited scalability. As more and more processes are used, the large number
of accesses to the common data structure or the master process tends to become a bottleneck.
n As an example of a scenario where centralized mapping may be applicable, consider the problem of sorting the entries in each row of an
. Serially, this can be accomplished by the following simple program segment: A  matrix n x
1 for (i=1; i<n; i++)
2 sort(A[i],n);
Recall that the time to sort an array using some of the commonly used sorting algorithms, such as quicksort, can vary significantly
depending on the initial order of the elements to be sorted. Therefore, each iteration of the loop in the program shown above can take
different amounts of time. A naive mapping of the task of sorting an equal number of rows to each process may lead to load-imbalance. A
possible solution to the potential load-imbalance problem in this case would be to maintain a central pool of indices of the rows that have
yet to be sorted. Whenever a process is idle, it picks up an available index, deletes it, and sorts the row with that index, as long as the pool
self of indices is not empty. This method of scheduling the independent iterations of a loop among parallel processes is known as
. scheduling
The assignment of a single task to a process at a time is quite effective in balancing the computation; however, it may lead to bottlenecks
in accessing the shared work queue, especially if each task (i.e., each loop iteration in this case) does not require a large enough amount
 processes can be D / M  time to assign work to a process, then at most D , and it takes M of computation. If the average size of each task is
, every time a process chunk scheduling kept busy effectively. The bottleneck can be eased by getting more than one task at a time. In
runs out of work it gets a group of tasks. The potential problem with such a scheme is that it may lead to load-imbalances if the number of
tasks (i.e., chunk) assigned in a single step is large. The danger of load-imbalance due to large chunk sizes can be reduced by decreasing
the chunk-size as the program progresses. That is, initially the chunk size is large, but as the number of iterations left to be executed
decreases, the chunk size also decreases. A variety of schemes have been developed for gradually adjusting the chunk size, that
decrease the chunk size either linearly or non-linearly.
Distributed Schemes
In a distributed dynamic load balancing scheme, the set of executable tasks are distributed among processes which exchange tasks at run
time to balance work. Each process can send work to or receive work from any other process. These methods do not suffer from the
bottleneck associated with the centralized schemes. Some of the critical parameters of a distributed load balancing scheme are as follows:
How are the sending and receiving processes paired together?
Is the work transfer initiated by the sender or the receiver?
How much work is transferred in each exchange? If too little work is transferred, then the receiver may not receive enough work
and frequent transfers resulting in excessive interaction may be required. If too much work is transferred, then the sender may
be out of work soon, again resulting in frequent transfers.
When is the work transfer performed? For example, in receiver initiated load balancing, work may be requested when the
process has actually run out of work or when the receiver has too little work left and anticipates being out of work soon.
e A detailed study of each of these parameters is beyond the scope of this chapter. These load balancing schemes will be revisited in th
 in the Chapter 11 context of parallel algorithms to which they apply when we discuss these algorithms in the later chapters – in particular,
context of parallel search algorithms.
Suitability to Parallel Architectures
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Note that, in principle, both centralized and distributed mapping schemes can be implemented in both message-passing and
shared-address-space paradigms. However, by its very nature any dynamic load balancing scheme requires movement of tasks from one
process to another. Hence, for such schemes to be effective on message-passing computers, the size of the tasks in terms of computation
should be much higher than the size of the data associated with the tasks. In a shared-address-space paradigm, the tasks don't need to
be moved explicitly, although there is some implied movement of data to local caches or memory banks of processes. In general, the
computational granularity of tasks to be moved can be much smaller on shared-address architecture than on message-passing
architectures.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
3.5 Methods for Containing Interaction Overheads
As noted earlier, reducing the interaction overhead among concurrent tasks is important for an efficient parallel program. The overhead
that a parallel program incurs due to interaction among its processes depends on many factors, such as the volume of data exchanged
during interactions, the frequency of interaction, the spatial and temporal pattern of interactions, etc.
In this section, we will discuss some general techniques that can be used to reduce the interaction overheads incurred by parallel
programs. These techniques manipulate one or more of the three factors above in order to reduce the interaction overhead. Some of these
are applicable while devising the decomposition and mapping schemes for the algorithms and some are applicable while programming the
algorithm in a given paradigm. All techniques may not be applicable in all parallel programming paradigms and some of them may require
support from the underlying hardware.
3.5.1 Maximizing Data Locality
In most nontrivial parallel programs, the tasks executed by different processes require access to some common data. For example, in
), all elements Figure 3.6  ( y , in which tasks correspond to computing individual elements of vector Ab  = y sparse matrix-vector multiplication
 need to be accessed by multiple tasks. In addition to sharing the original input data, interaction may result if processes b of the input vector
require data generated by other processes. The interaction overheads can be reduced by using techniques that promote the use of local
data or data that have been recently fetched. Data locality enhancing techniques encompass a wide range of schemes that try to minimize
the volume of nonlocal data that are accessed, maximize the reuse of recently accessed data, and minimize the frequency of accesses. In
many cases, these schemes are similar in nature to the data reuse optimizations often performed in modern cache based
microprocessors.
f  A fundamental technique for reducing the interaction overhead is to minimize the overall volume o Minimize Volume of Data-Exchange
shared data that needs to be accessed by concurrent processes. This is akin to maximizing the temporal data locality, i.e., making as
many of the consecutive references to the same data as possible. Clearly, performing as much of the computation as possible using
locally available data obviates the need for bringing in more data into local memory or cache for a process to perform its tasks. As
discussed previously, one way of achieving this is by using appropriate decomposition and mapping schemes. For example, in the case of
matrix multiplication, we saw that by using a two-dimensional mapping of the computations to the processes we were able to reduce the
 required 2 n  + p/ 2 n  as opposed to ) that needs to be accessed by each task to B  and A amount of shared data (i.e., matrices
). In general, using higher dimensional distribution often helps in reducing the volume of Figure 3.26 by a one-dimensional mapping (
nonlocal data that needs to be accessed.
Another way of decreasing the amount of shared data that are accessed by multiple processes is to use local data to store intermediate
results, and perform the shared data access to only place the final results of the computation. For example, consider computing the dot
 pairs of elements. Rather than adding each p/ n  tasks multiplies p  in parallel such that each of the n product of two vectors of length
individual product of a pair of numbers to the final result, each task can first create a partial dot product of its assigned portion of the
 in its own local location, and only access the final shared location once to add this partial result. This will reduce the p/ n vectors of length
. n  from p number of accesses to the shared location where the result is stored to
l  Minimizing interaction frequency is important in reducing the interaction overheads in paralle Minimize Frequency of Interactions
programs because there is a relatively high startup cost associated with each interaction on many architectures. Interaction frequency can
be reduced by restructuring the algorithm such that shared data are accessed and used in large pieces. Thus, by amortizing the startup
cost over large accesses, we can reduce the overall interaction overhead, even if such restructuring does not necessarily reduce the
overall volume of shared data that need to be accessed. This is akin to increasing the spatial locality of data access, i.e., ensuring the
proximity of consecutively accessed data locations. On a shared-address-space architecture, each time a word is accessed, an entire
cache line containing many words is fetched. If the program is structured to have spatial locality, then fewer cache lines are accessed. On
a message-passing system, spatial locality leads to fewer message-transfers over the network because each message can transfer larger
amounts of useful data. The number of messages can sometimes be reduced further on a message-passing system by combining
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
messages between the same source-destination pair into larger messages if the interaction pattern permits and if the data for multiple
messages are available at the same time, albeit in separate data structures.
Sparse matrix-vector multiplication is a problem whose parallel formulation can use this technique to reduce interaction overhead. In
typical applications, repeated sparse matrix-vector multiplication is performed with matrices of the same nonzero pattern but different
numerical nonzero values. While solving this problem in parallel, a process interacts with others to access elements of the input vector that
it may need for its local computation. Through a one-time scanning of the nonzero pattern of the rows of the sparse matrix that a process
is responsible for, it can determine exactly which elements of the input vector it needs and from which processes. Then, before starting
each multiplication, a process can first collect all the nonlocal entries of the input vector that it requires, and then perform an
interaction-free multiplication. This strategy is far superior than trying to access a nonlocal element of the input vector as and when
required in the computation.
3.5.2 Minimizing Contention and Hot Spots
Our discussion so far has been largely focused on reducing interaction overheads by directly or indirectly reducing the frequency and
volume of data transfers. However, the data-access and inter-task interaction patterns can often lead to contention that can increase the
overall interaction overhead. In general, contention occurs when multiple tasks try to access the same resources concurrently. Multiple
simultaneous transmissions of data over the same interconnection link, multiple simultaneous accesses to the same memory block, or
multiple processes sending messages to the same process at the same time, can all lead to contention. This is because only one of the
multiple operations can proceed at a time and the others are queued and proceed sequentially.
 be the p . Let Figure 3.26(b) , using the two-dimensional partitioning shown in AB  = C Consider the problem of multiplying two matrices
, for j, i C number of tasks with a one-to-one mapping of tasks onto processes. Let each task be responsible for computing a unique
 to be computed according to the following j, i C . The straightforward way of performing this computation is for
formula (written in matrix-block notation):
1  Equation 3.
 tasks will be accessing  steps, Looking at the memory access patterns of the above equation, we see that at any one of the
. For example, all A  will be accessing the same block of C . In particular, all the tasks that work on the same row of B  and A the same block of
 at once. Similarly, all the tasks working on 0,0 A  will attempt to read  processes computing
 will create B  and A . The need to concurrently access these blocks of matrices B  will be accessing the same block of C the same column of
contention on both NUMA shared-address-space and message-passing parallel architectures.
One way of reducing contention is to redesign the parallel algorithm to access data in contention-free patterns. For the matrix multiplication
. A Equation 3.1 algorithm, this contention can be eliminated by modifying the order in which the block multiplications are performed in
 by using the formula j, i C contention-free way of performing these block-multiplications is to compute
2  Equation 3.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 will be accessing block C  that work on the same row of j *, P where '%' denotes the modulo operation. By using this formula, all the tasks
 will be C  that work on the same column of ,* i P , which is different for each task. Similarly, all the tasks
, which is also different for each task. Thus, by simply rearranging the order in which the accessing block
block-multiplications are performed, we can completely eliminate the contention. For example, among the processes computing the first
. 0,0 A  instead of A  from the first block row of j 0, A  will access j 0, C , the process computing C block row of
) are a frequent source of contention for shared data structures or Section 3.4.2 Centralized schemes for dynamic mapping (
communication channels leading to the master process. The contention may be reduced by choosing a distributed mapping scheme over a
centralized one, even though the former may be harder to implement.
3.5.3 Overlapping Computations with Interactions
The amount of time that processes spend waiting for shared data to arrive or to receive additional work after an interaction has been
initiated can be reduced, often substantially, by doing some useful computations during this waiting time. There are a number of
techniques that can be used to overlap computations with interactions.
A simple way of overlapping is to initiate an interaction early enough so that it is completed before it is needed for computation. To achieve
this, we must be able to identify computations that can be performed before the interaction and do not depend on it. Then the parallel
program must be structured to initiate the interaction at an earlier point in the execution than it is needed in the original algorithm.
Typically, this is possible if the interaction pattern is spatially and temporally static (and therefore, predictable) or if multiple tasks that are
ready for execution are available on the same process so that if one blocks to wait for an interaction to complete, the process can work on
another task. The reader should note that by increasing the number of parallel tasks to promote computation-interaction overlap, we are
reducing the granularity of the tasks, which in general tends to increase overheads. Therefore, this technique must be used judiciously.
In certain dynamic mapping schemes, as soon as a process runs out of work, it requests and gets additional work from another process. It
then waits for the request to be serviced. If the process can anticipate that it is going to run out of work and initiate a work transfer
interaction in advance, then it may continue towards finishing the tasks at hand while the request for more work is being serviced.
Depending on the problem, estimating the amount of remaining work may be easy or hard.
In most cases, overlapping computations with interaction requires support from the programming paradigm, the operating system, and the
hardware. The programming paradigm must provide a mechanism to allow interactions and computations to proceed concurrently. This
mechanism should be supported by the underlying hardware. Disjoint address-space paradigms and architectures usually provide this
support via non-blocking message passing primitives. The programming paradigm provides functions for sending and receiving messages
that return control to the user's program before they have actually completed. Thus, the program can use these primitives to initiate the
interactions, and then proceed with the computations. If the hardware permits computation to proceed concurrently with message
transfers, then the interaction overhead can be reduced significantly.
On a shared-address-space architecture, the overlapping of computations and interaction is often assisted by prefetching hardware. In this
case, an access to shared data is nothing more than a regular load or store instruction. The prefetch hardware can anticipate the memory
addresses that will need to be accessed in the immediate future, and can initiate the access in advance of when they are needed. In the
absence of prefetching hardware, the same effect can be achieved by a compiler that detects the access pattern and places
pseudo-references to certain key memory locations before these locations are actually utilized by the computation. The degree of success
of this scheme is dependent upon the available structure in the program that can be inferred by the prefetch hardware and by the degree
of independence with which the prefetch hardware can function while computation is in progress.
3.5.4 Replicating Data or Computations
Replication of data or computations is another technique that may be useful in reducing interaction overheads.
In some parallel algorithms, multiple processes may require frequent read-only access to shared data structure, such as a hash-table, in
an irregular pattern. Unless the additional memory requirements are prohibitive, it may be best in a situation like this to replicate a copy of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
the shared data structure on each process so that after the initial interaction during replication, all subsequent accesses to this data
structure are free of any interaction overhead.
In the shared-address-space paradigm, replication of frequently accessed read-only data is often affected by the caches without explicit
programmer intervention. Explicit data replication is particularly suited for architectures and programming paradigms in which read-only
access to shared data is significantly more expensive or harder to express than local data accesses. Therefore, the message-passing
programming paradigm benefits the most from data replication, which may reduce interaction overhead and also significantly simplify the
writing of the parallel program.
Data replication, however, does not come without its own cost. Data replication increases the memory requirements of a parallel program.
The aggregate amount of memory required to store the replicated data increases linearly with the number of concurrent processes. This
may limit the size of the problem that can be solved on a given parallel computer. For this reason, data replication must be used
selectively to replicate relatively small amounts of data.
In addition to input data, the processes in a parallel program often share intermediate results. In some situations, it may be more
cost-effective for a process to compute these intermediate results than to get them from another process that generates them. In such
situations, interaction overhead can be traded for replicated computation. For example, while performing the Fast Fourier Transform (see
 or "twiddle factors" are computed and used at various points w  distinct powers of N -point series, N  for more details), on an Section 13.2.3
 twiddle factors. In a N in the computation. In a parallel implementation of FFT, different processes require overlapping subsets of these
message-passing paradigm, it is best for each process to locally compute all the twiddle factors it needs. Although the parallel algorithm
may perform many more twiddle factor computations than the serial algorithm, it may still be faster than sharing the twiddle factors.
3.5.5 Using Optimized Collective Interaction Operations
, often the interaction patterns among concurrent activities are static and regular. A class of such static and Section 3.3.2 As discussed in
regular interaction patterns are those that are performed by groups of tasks, and they are used to achieve regular data accesses or to
 interaction operations have been identified that collective perform certain type of computations on distributed data. A number of key such
appear frequently in many parallel algorithms. Broadcasting some data to all the processes or adding up numbers, each belonging to a
different process, are examples of such collective operations. The collective data-sharing operations can be classified into three
categories. The first category contains operations that are used by the tasks to access data, the second category of operations are used to
perform some communication-intensive computations, and finally, the third category is used for synchronization.
Highly optimized implementations of these collective operations have been developed that minimize the overheads due to data transfer as
 describes algorithms for implementing some of the commonly used collective interaction operations. Chapter 4 well as contention.
Optimized implementations of these operations are available in library form from the vendors of most parallel computers, e.g., MPI
(message passing interface). As a result, the algorithm designer does not need to think about how these operations are implemented and
, sometimes the interaction Section 3.5.6 needs to focus only on the functionality achieved by these operations. However, as discussed in
pattern may make it worthwhile for the parallel programmer to implement one's own collective communication procedure.
3.5.6 Overlapping Interactions with Other Interactions
If the data-transfer capacity of the underlying hardware permits, then overlapping interactions between multiple pairs of processes can
reduce the effective volume of communication. As an example of overlapping interactions, consider the commonly used collective
. A commonly 3 P , and 2 P , 1 P , 0 P communication operation of one-to-all broadcast in a message-passing paradigm with four processes
. In the 2 P  sends the data to 0 P  to all other processes works as follows. In the first step, 0 P used algorithm to broadcast some data from
. The entire operation is 3 P  to 0 P  sends the same data that it had received from 2 P , and concurrently, 1 P  sends the data to 0 P second step,
thus complete in two steps because the two interactions of the second step require only one time step. This operation is illustrated in
, thereby consuming three 3 P  to 2 P  to 1 P  to 0 P . On the other hand, a naive broadcast algorithm would send the data from Figure 3.41(a)
. Figure 3.41(b) steps as illustrated in
Figure 3.41. Illustration of overlapping interactions in broadcasting data from one to four
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
processes.
 may be adapted to actually Figure 3.41(b) Interestingly, however, there are situations when the naive broadcast algorithm shown in
increase the amount of overlap. Assume that a parallel algorithm needs to broadcast four data structures one after the other. The entire
interaction would require eight steps using the first two-step broadcast algorithm. However, using the naive algorithm accomplishes the
 sends the 0 P . In the second step 1 P  sends the first message to 0 P . In the first step, Figure 3.41(c) interaction in only six steps as shown in
 sends 1 P , 1 P  sends the third message to 0 P . In the third step, 2 P  simultaneously sends the first message to 1 P  while 1 P second message to
. Proceeding similarly in a pipelined fashion, the last of the four messages 3 P  sends the first message to 2 P , and 2 P the second message to
 in six. Since this method is rather expensive for a single broadcast operation, it is unlikely 3 P  after four steps and reaches 0 P is sent out of
to be included in a collective communication library. However, the programmer must infer from the interaction pattern of the algorithm that
 and write your own collective communication function. Section 3.5.5 in this scenario, it is better to make an exception to the suggestion of
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
3.6 Parallel Algorithm Models
Having discussed the techniques for decomposition, mapping, and minimizing interaction overheads, we now present some of the
commonly used parallel algorithm models. An algorithm model is typically a way of structuring a parallel algorithm by selecting a
decomposition and mapping technique and applying the appropriate strategy to minimize interactions.
3.6.1 The Data-Parallel Model
 is one of the simplest algorithm models. In this model, the tasks are statically or semi-statically mapped onto data-parallel model The
processes and each task performs similar operations on different data. This type of parallelism that is a result of identical operations
. The work may be done in phases and the data operated data parallelism being applied concurrently on different data items is called
upon in different phases may be different. Typically, data-parallel computation phases are interspersed with interactions to synchronize
the tasks or to get fresh data to the tasks. Since all tasks perform similar computations, the decomposition of the problem into tasks is
usually based on data partitioning because a uniform partitioning of data followed by a static mapping is sufficient to guarantee load
balance.
Data-parallel algorithms can be implemented in both shared-address-space and message-passing paradigms. However, the partitioned
address-space in a message-passing paradigm may allow better control of placement, and thus may offer a better handle on locality. On
the other hand, shared-address space can ease the programming effort, especially if the distribution of data is different in different
phases of the algorithm.
Interaction overheads in the data-parallel model can be minimized by choosing a locality preserving decomposition and, if applicable, by
overlapping computation and interaction and by using optimized collective interaction routines. A key characteristic of data-parallel
problems is that for most problems, the degree of data parallelism increases with the size of the problem, making it possible to use more
processes to effectively solve larger problems.
Figure . In the decomposition shown in Section 3.1.1 An example of a data-parallel algorithm is dense matrix multiplication described in
, all tasks are identical; they are applied to different data. 3.10
3.6.2 The Task Graph Model
, the computations in any parallel algorithm can be viewed as a task-dependency graph. The Section 3.1 As discussed in
task-dependency graph may be either trivial, as in the case of matrix multiplication, or nontrivial (Problem 3.5). However, in certain
, the interrelationships among the task graph model parallel algorithms, the task-dependency graph is explicitly used in mapping. In the
tasks are utilized to promote locality or to reduce interaction costs. This model is typically employed to solve problems in which the
amount of data associated with the tasks is large relative to the amount of computation associated with them. Usually, tasks are mapped
statically to help optimize the cost of data movement among tasks. Sometimes a decentralized dynamic mapping may be used, but even
then, the mapping uses the information about the task-dependency graph structure and the interaction pattern of tasks to minimize
interaction overhead. Work is more easily shared in paradigms with globally addressable space, but mechanisms are available to share
work in disjoint address space.
Typical interaction-reducing techniques applicable to this model include reducing the volume and frequency of interaction by promoting
locality while mapping the tasks based on the interaction pattern of tasks, and using asynchronous interaction methods to overlap the
interaction with computation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
), sparse matrix factorization, and many Section 9.4.1 Examples of algorithms based on the task graph model include parallel quicksort (
parallel algorithms derived via divide-and-conquer decomposition. This type of parallelism that is naturally expressed by independent
. task parallelism tasks in a task-dependency graph is called
3.6.3 The Work Pool Model
 model is characterized by a dynamic mapping of tasks onto processes for load balancing in which any task pool  or the work pool The
task may potentially be performed by any process. There is no desired premapping of tasks onto processes. The mapping may be
centralized or decentralized. Pointers to the tasks may be stored in a physically shared list, priority queue, hash table, or tree, or they
could be stored in a physically distributed data structure. The work may be statically available in the beginning, or could be dynamically
generated; i.e., the processes may generate work and add it to the global (possibly distributed) work pool. If the work is generated
) would be required so that all Section 11.4.4 dynamically and a decentralized mapping is used, then a termination detection algorithm (
processes can actually detect the completion of the entire program (i.e., exhaustion of all potential tasks) and stop looking for more work.
In the message-passing paradigm, the work pool model is typically used when the amount of data associated with tasks is relatively
small compared to the computation associated with the tasks. As a result, tasks can be readily moved around without causing too much
data interaction overhead. The granularity of the tasks can be adjusted to attain the desired level of tradeoff between load-imbalance and
the overhead of accessing the work pool for adding and extracting tasks.
) or related methods is an example of the use of the work pool model with Section 3.4.2 Parallelization of loops by chunk scheduling (
centralized mapping when the tasks are statically available. Parallel tree search where the work is represented by a centralized or
distributed data structure is an example of the use of the work pool model where the tasks are generated dynamically.
3.6.4 The Master-Slave Model
 model, one or more master processes generate work and allocate it to worker processes. The manager-worker  or the master-slave In the
 if the manager can estimate the size of the tasks or if a random mapping can do an adequate job of load a priori tasks may be allocated
balancing. In another scenario, workers are assigned smaller pieces of work at different times. The latter scheme is preferred if it is time
consuming for the master to generate work and hence it is not desirable to make all workers wait until the master has generated all work
pieces. In some cases, work may need to be performed in phases, and work in each phase must finish before work in the next phases
can be generated. In this case, the manager may cause all workers to synchronize after each phase. Usually, there is no desired
premapping of work to processes, and any worker can do any job assigned to it. The manager-worker model can be generalized to the
hierarchical or multi-level manager-worker model in which the top-level manager feeds large chunks of tasks to second-level managers,
who further subdivide the tasks among their own workers and may perform part of the work themselves. This model is generally equally
suitable to shared-address-space or message-passing paradigms since the interaction is naturally two-way; i.e., the manager knows that
it needs to give out work and workers know that they need to get work from the manager.
While using the master-slave model, care should be taken to ensure that the master does not become a bottleneck, which may happen if
the tasks are too small (or the workers are relatively fast). The granularity of tasks should be chosen such that the cost of doing work
dominates the cost of transferring work and the cost of synchronization. Asynchronous interaction may help overlap interaction and the
computation associated with work generation by the master. It may also reduce waiting times if the nature of requests from workers is
non-deterministic.
3.6.5 The Pipeline or Producer-Consumer Model
, a stream of data is passed on through a succession of processes, each of which perform some task on it. This pipeline model In the
. With the exception of the process initiating stream parallelism simultaneous execution of different programs on a data stream is called
the pipeline, the arrival of new data triggers the execution of a new task by a process in the pipeline. The processes could form such
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
pipelines in the shape of linear or multidimensional arrays, trees, or general graphs with or without cycles. A pipeline is a chain of
producers and consumers. Each process in the pipeline can be viewed as a consumer of a sequence of data items for the process
preceding it in the pipeline and as a producer of data for the process following it in the pipeline. The pipeline does not need to be a linear
chain; it can be a directed graph. The pipeline model usually involves a static mapping of tasks onto processes.
Load balancing is a function of task granularity. The larger the granularity, the longer it takes to fill up the pipeline, i.e. for the trigger
produced by the first process in the chain to propagate to the last process, thereby keeping some of the processes waiting. However, too
fine a granularity may increase interaction overheads because processes will need to interact to receive fresh data after smaller pieces
of computation. The most common interaction reduction technique applicable to this model is overlapping interaction with computation.
. Section 8.3.1 An example of a two-dimensional pipeline is the parallel LU factorization algorithm, which is discussed in detail in
3.6.6 Hybrid Models
In some cases, more than one model may be applicable to the problem at hand, resulting in a hybrid algorithm model. A hybrid model
may be composed either of multiple models applied hierarchically or multiple models applied sequentially to different phases of a parallel
algorithm. In some cases, an algorithm formulation may have characteristics of more than one algorithm model. For instance, data may
flow in a pipelined manner in a pattern guided by a task-dependency graph. In another scenario, the major computation may be
described by a task-dependency graph, but each node of the graph may represent a supertask comprising multiple subtasks that may be
) is one of the applications for which a hybrid 9.4.1  and 3.2.5 suitable for data-parallel or pipelined parallelism. Parallel quicksort (Sections
model is ideally suited.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
3.7 Bibliographic Remarks
], and Culler and Singh WA99 ], Wilkinson and Allen [ HX98 ], Hwang and Xu [ Akl97 ], Akl [ Wil95 Various texts, such as those by Wilson [
], among others, present similar or slightly varying models for parallel programs and steps in developing parallel algorithms. The CSG98[
] is among the relatively few textbooks that focus on the practical aspects of writing GH01 book by Goedecker and Hoisie [
] present comprehensive surveys of techniques for mapping tasks KA99b , KA99a high-performance parallel programs. Kwok and Ahmad [
onto processes.
Most of the algorithms used in this chapter as examples are discussed in detail in other chapters in this book dedicated to the respective
class of problems. Please refer to the bibliographic remarks in those chapters for further references on these algorithms.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
, each union and intersection operation can be performed in time proportional to the sum of the Example 3.2  In 3.1
number of records in the two input tables. Based on this, construct the weighted task-dependency graphs
, where the weight of each node is equivalent to the amount of work required by 3.3  and 3.2 corresponding to Figures
the corresponding task. What is the average degree of concurrency of each graph?
, determine the following: Figure 3.42  For the task graphs given in 3.2
Maximum degree of concurrency. . 1
Critical path length. . 2
Maximum achievable speedup over one process assuming that an arbitrarily large number of processes is
available.
. 3
The minimum number of processes needed to obtain the maximum possible speedup. . 4
The maximum achievable speedup if the number of processes is limited to (a) 2, (b) 4, and (c) 8. . 5
Figure 3.42. Task-dependency graphs for Problem 3.2.
 What are the average degrees of concurrency and critical-path lengths of task-dependency graphs corresponding 3.3
? 3.11  and 3.10 to the decompositions for matrix multiplication shown in Figures
. l  tasks and a critical-path lengtht  be the maximum degree of concurrency in a task-dependency graph with d  Let 3.4
. Prove that
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 shows the decomposition of LU Figure 3.27 . Algorithm 3.3  Consider LU factorization of a dense matrix shown in 3.5
 3. j , i , 1 j,i A  into nine blocks A factorization into 14 tasks based on a two-dimensional partitioning of the matrix
L  as a result of factorization. The diagonal blocks of U  and L  are modified into corresponding blocks of A The blocks of
 are upper triangular submatrices. U are lower triangular submatrices with unit diagonals and the diagonal blocks of
s . Tasks 2 and 3 implement the block versions of the loop on Line Algorithm 3.3  using 1,1 A Task 1 factors the submatrix
. Tasks 4 and 5 are the upper-triangular counterparts of tasks 2 and 3. The element version of LU Algorithm 3.3 4–6 of
 are 1; however, a block L  does not show these steps because the diagonal entries of Algorithm 3.3 factorization in
 with the L  as a product of the inverse of the corresponding diagonal block of U version must compute a block-row of
9 . Thus, Tasks 1– Algorithm 3.3 . Tasks 6–9 implement the block version of the loops on Lines 7–11 of A block-row of
. The remainder of the tasks Algorithm 3.3 correspond to the block version of the first iteration of the outermost loop of
Figure . Draw a task-dependency graph corresponding to the decomposition shown in A complete the factorization of
. 3.27
. Figure 3.27  Enumerate the critical paths in the decomposition of LU factorization shown in 3.6
 onto three Figure 3.27  Show an efficient mapping of the task-dependency graph of the decomposition shown in 3.7
processes. Prove informally that your mapping is the best possible mapping for three processes.
Figure 3.27  Describe and draw an efficient mapping of the task-dependency graph of the decomposition shown in 3.8
onto four processes and prove that your mapping is the best possible mapping for four processes.
s  which of the two mappings – the one onto three processe [1]  Assuming that each task takes a unit amount of time, 3.9
? or the one onto four processes – solves the problem faster
 arithmetic 3 b  1, Tasks 1, 10, and 14 require about 2/3 b  In practice, for a block size [1]
 operations; and Tasks 6, 7, 8, 9, and 13 3 b operations; Tasks 2, 3, 4, 5, 11, and 12 require about
 operations. 3 b require about 2
 submatrix) b  x b  is aj,i U , and j,i L , j,i A  (i.e., each b  with block size Figure 3.27  Prove that block steps 1 through 14 in 3.10
. b  = 3 n , where A  matrix n  x n  on an Algorithm 3.3 are mathematically equivalent to running the algorithm of
 is one possible approach. b  Using induction on Hint:
 shows the decomposition into 14 tasks of LU factorization of a matrix split into blocks using a 3 x 3 Figure 3.27 3.11
) as a m (t  partitioning is used, derive an expression for the number of tasks m  x m two-dimensional partitioning. If an
 in a decomposition along similar lines. m function of
. 2 m  - 1) + m (t ) = m (t  Show that Hint:
) as a function of m ( d  In the context of Problem 3.11, derive an expression for the maximum degree of concurrency 3.12
. m
. m ) as a function of m ( l  In the context of Problem 3.11, derive an expression for the critical-path length 3.13
. 3.3  and 3.2  Show efficient mappings for the decompositions for the database query problem shown in Figures 3.14
What is the maximum number of processes that you would use in each case?
, assume a decomposition such that each execution of Line 7 is a task. Algorithm 3.4  In the algorithm shown in 3.15
Draw a task-dependency graph and a task-interaction graph.
Algorithm 3.4 A sample serial program to be parallelized.
) n , A  FFT_like_pattern( procedure 1.
begin 2.
; n 2  := log m 3.
do  - 1 m to  := 0 j for 4.
; j  := 2 k 5.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
do  - 1 n to  := 0i for 6.
]; j  XOR 2i[ A ] +i[ A ] :=i[ A 7.
endfor 8.
 FFT_like_pattern end 9.
 = 16, devise a good mapping for 16 processes. n , if Algorithm 3.4  In 3.16
 = 16, devise a good mapping for 8 processes. n , if Algorithm 3.4  In 3.17
) - 1. n 2  = (log m  is changed to Algorithm 3.4  Repeat Problems 3.15, 3.16, and 3.17 if the statement of Line 3 in 3.18
] as r  random integers in the range [1... n  of A  Consider a simplified version of bucket-sort. You are given an array 3.19
 contains indices of all thei  buckets, such that at the end of the algorithm, Bucket r input. The output data consist of
. i  that are equal to A elements in
) and an appropriate mapping A Describe a decomposition based on partitioning the input data (i.e., the array
 processes. Describe briefly how the resulting parallel algorithm would work. p onto
 buckets) and an appropriate r Describe a decomposition based on partitioning the output data (i.e., the set of
 processes. Describe briefly how the resulting parallel algorithm would work. p mapping onto
 In the context of Problem 3.19, which of the two decompositions leads to a better parallel algorithm? Should the 3.20
 have a bearing on the selection of one of the two decomposition schemes? r  and n relative values of
 Consider seven tasks with running times of 1, 2, 3, 4, 5, 5, and 10 units, respectively. Assuming that it does not 3.21
take any time to assign work to a process, compute the best- and worst-case speedup for a centralized scheme for
dynamic mapping with two processes.
 tasks that are being mapped using a centralized dynamic load balancing scheme and we M  Suppose there are 3.22
have the following information about these tasks:
Average task size is 1.
Minimum task size is 0.
. m Maximum task size is
 time to pick up a task. It takes a process
Compute the best- and worst-case speedups for self-scheduling and chunk-scheduling assuming that tasks are
). What are the actual values of the best- and worst-case speedups for the two M  <l  (l available in batches of
 = 2?l  = 100, and M  = 20, m  = 0.2,  = 10, p scheduling methods when
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 4. Basic Communication Operations
In most parallel algorithms, processes need to exchange data with other processes. This exchange of data can significantly impact the
 that it takes Section 2.5 efficiency of parallel programs by introducing interaction delays during their execution. For instance, recall from
-word message between two processes running on different nodes of an m  time for a simple exchange of an w mt  + st roughly
 is the per-word wt  is the latency or the startup time for the data transfer and st interconnection network with cut-through routing. Here
transfer time, which is inversely proportional to the available bandwidth between the nodes. Many interactions in practical parallel
programs occur in well-defined patterns involving more than two processes. Often either all processes participate together in a single
global interaction operation, or subsets of processes participate in interactions local to each subset. These common basic patterns of
interprocess interaction or communication are frequently used as building blocks in a variety of parallel algorithms. Proper
implementation of these basic communication operations on various parallel architectures is a key to the efficient execution of the
parallel algorithms that use them.
In this chapter, we present algorithms to implement some commonly used communication patterns on simple interconnection networks,
such as the linear array, two-dimensional mesh, and the hypercube. The choice of these interconnection networks is motivated primarily
by pedagogical reasons. For instance, although it is unlikely that large scale parallel computers will be based on the linear array or ring
topology, it is important to understand various communication operations in the context of linear arrays because the rows and columns of
meshes are linear arrays. Parallel algorithms that perform rowwise or columnwise communication on meshes use linear array algorithms.
The algorithms for a number of communication operations on a mesh are simple extensions of the corresponding linear array algorithms
to two dimensions. Furthermore, parallel algorithms using regular data structures such as arrays often map naturally onto one- or
two-dimensional arrays of processes. This too makes it important to study interprocess interaction on a linear array or mesh
interconnection network. The hypercube architecture, on the other hand, is interesting because many algorithms with recursive
interaction patterns map naturally onto a hypercube topology. Most of these algorithms may perform equally well on interconnection
networks other than the hypercube, but it is simpler to visualize their communication patterns on a hypercube.
The algorithms presented in this chapter in the context of simple network topologies are practical and are highly suitable for modern
parallel computers, even though most such computers are unlikely to have an interconnection network that exactly matches one of the
networks considered in this chapter. The reason is that on a modern parallel computer, the time to transfer data of a certain size between
two nodes is often independent of the relative location of the nodes in the interconnection network. This homogeneity is afforded by a
variety of firmware and hardware features such as randomized routing algorithms and cut-through routing, etc. Furthermore, the end
user usually does not have explicit control over mapping processes onto physical processors. Therefore, we assume that the transfer of
. On most architectures, this w mt  + st  pair of nodes in an interconnection network incurs a cost of any  words of data between m
assumption is reasonably accurate as long as a free link is available between the source and destination nodes for the data to traverse.
However, if many pairs of nodes are communicating simultaneously, then the messages may take longer. This can happen if the number
) of the network. In such Section 2.4.4 of messages passing through a cross-section of the network exceeds the cross-section bandwidth (
, we refer to the Section 2.5.1  to reflect the slowdown due to congestion. As discussed in wt situations, we need to adjust the value of
. We will make a note in the text when we come across communication operations that may cause wt  as effective wt adjusted value of
congestion on certain networks.
, the cost of data-sharing among processors in the shared-address-space paradigm can be modeled using Section 2.5.2 As discussed in
 relative to each other as well as relative to the computation speed wt  and st , usually with different values of w mt  + st the same expression
of the processors of the parallel computer. Therefore, parallel algorithms requiring one or more of the interaction patterns discussed in
this chapter can be assumed to incur costs whose expression is close to one derived in the context of message-passing.
In the following sections we describe various communication operations and derive expressions for their time complexity. We assume
) and that the communication time between any pair of nodes Section 2.5.1 that the interconnection network supports cut-through routing (
is practically independent of of the number of intermediate nodes along the paths between them. We also assume that the
 to each other simultaneously m communication links are bidirectional; that is, two directly-connected nodes can send messages of size
. We assume a single-port communication model, in which a node can send a message on only one of its links at a time. m wt  + st in time
Similarly, it can receive a message on only one link at a time. However, a node can receive a message while sending another message
at the same time on the same or a different link.
Many of the operations described here have duals and other related operations that we can perform by using procedures very similar to
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 of a communication operation is the opposite of the original operation and can be performed dual those for the original operations. The
by reversing the direction and sequence of messages in the original operation. We will mention such operations wherever applicable.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.1 One-to-All Broadcast and All-to-One Reduction
Parallel algorithms often require a single process to send identical data to all other processes or to a subset of them. This operation is
 that needs to be broadcast. At the termination of m . Initially, only the source process has the data of size one-to-all broadcast known as
all-to-one  copies of the initial data – one belonging to each process. The dual of one-to-all broadcast is p the procedure, there are
 words. The data m  containing M  participating processes starts with a buffer p . In an all-to-one reduction operation, each of the reduction
from all processes are combined through an associative operator and accumulated at a single destination process into one buffer of size
 is the M  th word of the accumulatedi . Reduction can be used to find the sum, product, maximum, or minimum of sets of numbers – the m
 shows one-to-all broadcast and all-to-one Figure 4.1  th words of each of the original buffers.i sum, product, maximum, or minimum of the
 processes. p reduction among
Figure 4.1. One-to-all broadcast and all-to-one reduction.
One-to-all broadcast and all-to-one reduction are used in several important parallel algorithms including matrix-vector multiplication,
Gaussian elimination, shortest paths, and vector inner product. In the following subsections, we consider the implementation of one-to-all
broadcast in detail on a variety of interconnection topologies.
4.1.1 Ring or Linear Array
 - 1 processes. However, p  - 1 messages from the source to the other p A naive way to perform one-to-all broadcast is to sequentially send
this is inefficient because the source process becomes a bottleneck. Moreover, the communication network is underutilized because only
the connection between a single pair of nodes is used at a time. A better broadcast algorithm can be devised using a technique commonly
. The source process first sends the message to another process. Now both these processes can recursive doubling known as
simultaneously send the message to two other processes that are still waiting for the message. By continuing this procedure until all the
 steps. p processes have received the data, the message can be broadcast in log
. The nodes are labeled from 0 to 7. Each Figure 4.2 The steps in a one-to-all broadcast on an eight-node linear array or ring are shown in
message transmission step is shown by a numbered, dotted arrow from the source of the message to its destination. Arrows indicating
messages sent during the same time step have the same number.
Figure 4.2. One-to-all broadcast on an eight-node ring. Node 0 is the source of the broadcast.
Each message transfer step is shown by a numbered, dotted arrow from the source of the
message to its destination. The number on an arrow indicates the time step during which the
message is transferred.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, the Figure 4.2 Note that on a linear array, the destination node to which the message is sent in each step must be carefully chosen. In
message is first sent to the farthest node (4) from the source (0). In the second step, the distance between the sending and receiving
nodes is halved, and so on. The message recipients are selected in this manner at each step to avoid congestion on the network. For
example, if node 0 sent the message to node 1 in the first step and then nodes 0 and 1 attempted to send messages to nodes 2 and 3,
respectively, in the second step, the link between nodes 1 and 2 would be congested as it would be a part of the shortest route for both the
messages in the second step.
Figure Reduction on a linear array can be performed by simply reversing the direction and the sequence of communication, as shown in
. In the first step, each odd numbered node sends its buffer to the even numbered node just before itself, where the contents of the two 4.3
buffers are combined into one. After the first step, there are four buffers left to be reduced on nodes 0, 2, 4, and 6, respectively. In the
second step, the contents of the buffers on nodes 0 and 2 are accumulated on node 0 and those on nodes 6 and 4 are accumulated on
node 4. Finally, node 4 sends its buffer to node 0, which computes the final result of the reduction.
Figure 4.3. Reduction on an eight-node ring with node 0 as the destination of the reduction.
Example 4.1 Matrix-vector multiplication
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 x 1 n  mesh of nodes to yield an n  x n  on an x  x 1 vector n  with an A  matrix n  x n Consider the problem of multiplying an
 shows one possible mapping of the Figure 4.4 . Algorithm 8.1 shows a serial algorithm for this problem. y result vector
matrix and the vectors in which each element of the matrix belongs to a different process, and the vector is distributed
among the processes in the topmost row of the mesh and the result vector is generated on the leftmost column of
processes.
Figure 4.4. One-to-all broadcast and all-to-one reduction in the multiplication of a 4
x 4 matrix with a 4 x 1 vector.
Since all the rows of the matrix must be multiplied with the vector, each process needs the element of the vector
residing in the topmost process of its column. Hence, before computing the matrix-vector product, each column of nodes
performs a one-to-all broadcast of the vector elements with the topmost process of the column as the source. This is
-node linear array, and simultaneously applying the linear array n  mesh as an n  x n done by treating each column of the
broadcast procedure described previously to all columns.
After the broadcast, each process multiplies its matrix element with the result of the broadcast. Now, each row of
processes needs to add its result to generate the corresponding element of the product vector. This is accomplished by
performing all-to-one reduction on each row of the process mesh with the first process of each row as the destination of
the reduction operation.
[2, 1] and will participate in an A  as a result of the broadcast, will multiply it with 1 P  [1] from x  will receive 9 P For example,
. 8 P [2] on y  to accumulate 11 P , and 10 P , 8 P all-to-one reduction with
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
4.1.2 Mesh
 nodes. So a number of communication  nodes as a linear array of p We can regard each row and column of a square mesh of
algorithms on the mesh are simple extensions of their linear array counterparts. A linear array communication operation can be performed
in two phases on a mesh. In the first phase, the operation is performed along one or all rows by treating the rows as linear arrays. In the
second phase, the columns are treated similarly.
 columns. First, a one-to-all  rows and Consider the problem of one-to-all broadcast on a two-dimensional square mesh with
) nodes of the same row. Once all the nodes in a row of the mesh broadcast is performed from the source to the remaining (
have acquired the data, they initiate a one-to-all broadcast in their respective columns. At the end of the second phase, every node in the
 = p  for Figure 4.5 mesh has a copy of the initial message. The communication steps for one-to-all broadcast on a mesh are illustrated in
16, with node 0 at the bottom-left corner as the source. Steps 1 and 2 correspond to the first phase, and steps 3 and 4 correspond to the
second phase.
Figure 4.5. One-to-all broadcast on a 16-node mesh.
 nodes in each of 3 / 1 p We can use a similar procedure for one-to-all broadcast on a three-dimensional mesh as well. In this case, rows of
the three dimensions of the mesh would be treated as linear arrays. As in the case of a linear array, reduction can be performed on twoand three-dimensional meshes by simply reversing the direction and the order of messages.
4.1.3 Hypercube
The previous subsection showed that one-to-all broadcast is performed in two phases on a two-dimensional mesh, with the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
communication taking place along a different dimension in each phase. Similarly, the process is carried out in three phases on a
-dimensional mesh with two nodes in each dimension. Hence, d  nodes can be regarded as a d three-dimensional mesh. A hypercube with 2
.  steps – one in each dimension d the mesh algorithm can be extended to the hypercube, except that the process is now carried out in
 shows a one-to-all broadcast on an eight-node (three-dimensional) hypercube with node 0 as the source. In this figure, Figure 4.6
communication starts along the highest dimension (that is, the dimension specified by the most significant bit of the binary representation
of a node label) and proceeds along successively lower dimensions in subsequent steps. Note that the source and the destination nodes
 are identical to the ones in the broadcast algorithm on a linear array Figure 4.6 in three communication steps of the algorithm shown in
. However, on a hypercube, the order in which the dimensions are chosen for communication does not affect the Figure 4.2 shown in
 shows only one such order. Unlike a linear array, the hypercube broadcast would not suffer from Figure 4.6 outcome of the procedure.
congestion if node 0 started out by sending the message to node 1 in the first step, followed by nodes 0 and 1 sending messages to
nodes 2 and 3, respectively, and finally nodes 0, 1, 2, and 3 sending messages to nodes 4, 5, 6, and 7, respectively.
Figure 4.6. One-to-all broadcast on a three-dimensional hypercube. The binary representations
of node labels are shown in parentheses.
4.1.4 Balanced Binary Tree
The hypercube algorithm for one-to-all broadcast maps naturally onto a balanced binary tree in which each leaf is a processing node and
 for eight nodes. In this figure, the communicating nodes Figure 4.7 intermediate nodes serve only as switching units. This is illustrated in
 shows that there is no congestion on any of the Figure 4.7 . Figure 4.6 have the same labels as in the hypercube algorithm illustrated in
 is that there Figure 4.7 communication links at any time. The difference between the communication on a hypercube and the tree shown in
is a different number of switching nodes along different paths on the tree.
Figure 4.7. One-to-all broadcast on an eight-node tree.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
4.1.5 Detailed Algorithms
 would reveal that the basic communication pattern for one-to-all broadcast is identical on all 4.7 , and 4.6 , 4.5 , 4.2 A careful look at Figures
the four interconnection networks considered in this section. We now describe procedures to implement the broadcast and reduction
operations. For the sake of simplicity, the algorithms are described here in the context of a hypercube and assume that the number of
communicating processes is a power of 2. However, they apply to any network topology, and can be easily extended to work for any
number of processes (Problem 4.1).
-node network when node 0 is the source of the broadcast. The procedure is d  shows a one-to-all broadcast procedure on a 2 Algorithm 4.1
 be the message to be broadcast, which initially X  is the label of that node. Let my_id executed at all the nodes. At any node, the value of
 communication steps, one along each dimension of a hypothetical hypercube. In d resides at the source node 0. The procedure performs
, communication proceeds from the highest to the lowest dimension (although the order in which dimensions are chosen Algorithm 4.1
 indicates the current dimension of the hypercube in which communication is taking place. Only thei does not matter). The loop counter
. For instance, on the i  least significant bits of their labels participate in communication along dimensioni nodes with zero in the
 is equal to 2 in the first time step. Therefore, only nodes 0 and 4 communicate, sincei , Figure 4.6 three-dimensional hypercube shown in
 = 1, all nodes (that is, 0, 2, 4, and 6) with zero in their least significanti their two least significant bits are zero. In the next time step, when
bits participate in communication. The procedure terminates after communication has taken place along all dimensions.
) bits, all p  (= log d  has mask  helps determine which nodes communicate in a particular iteration of the loop. The variable mask The variable
 is reset to zero (Line 5). mask of which are initially set to one (Line 3). At the beginning of each iteration, the most significant nonzero bit of
mask , Figure 4.6 Line 6 determines which nodes communicate in the current iteration of the outer loop. For instance, for the hypercube of
 are ones). The AND mask  least significant bits ofi  = 2 (thei is initially set to 111, and it would be 011 during the iteration corresponding to
 least significant bits.i operation on Line 6 selects only those nodes that have zeros in their
 send the data, and the nodes with ai  , the nodes with a zero at bit positioni Among the nodes selected for communication along dimension
, Figure 4.6  receive it. The test to determine the sending and receiving nodes is performed on Line 7. For example, ini one at bit position
 = 1, nodes 0 (000) and 4i  = 2. Similarly, fori node 0 (000) is the sender and node 4 (100) is the receiver in the iteration corresponding to
(100) are senders while nodes 2 (010) and 6 (110) are receivers.
 works only if node 0 is the source of the broadcast. For an arbitrary source, we must relabel the nodes of the hypothetical Algorithm 4.1
hypercube by XORing the label of each node with the label of the source node before we apply this procedure. A modified one-to-all
. By performing the XOR operation at Algorithm 4.2  - 1 is shown in p  between 0 and source broadcast procedure that works for any value of
 relabels the source node to 0, and relabels the other nodes relative to the source. After this relabeling, the algorithm Algorithm 4.2 Line 3,
 can be applied to perform the broadcast. Algorithm 4.1 of
-dimensional hypercube such that the final result is d  gives a procedure to perform an all-to-one reduction on a hypothetical Algorithm 4.3
accumulated on node 0. Single node-accumulation is the dual of one-to-all broadcast. Therefore, we obtain the communication pattern
required to implement reduction by reversing the order and the direction of messages in one-to-all broadcast. Procedure
) shown in X , my_id , d  is very similar to procedure ONE_TO_ALL_BC( Algorithm 4.3 ) shown in sum , X , m , my_id , d ALL_TO_ONE_REDUCE(
. One difference is that the communication in all-to-one reduction proceeds from the lowest to the highest dimension. This Algorithm 4.1
. The criterion for determining the source and the Algorithm 4.3  are manipulated ini  and mask change is reflected in the way that variables
destination among a pair of communicating nodes is also reversed (Line 7). Apart from these differences, procedure
ALL_TO_ONE_REDUCE has extra instructions (Lines 13 and 14) to add the contents of the messages received by a node in each
iteration (any associative operation can be used in place of addition).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
-node p -dimensional d  from node 0 of a X Algorithm 4.1 One-to-all broadcast of a message
). AND and XOR are bitwise logical-and and exclusive-or operations, p  = log d hypercube (
respectively.
) X , my_id , d  ONE_TO_ALL_BC( procedure 1.
begin 2.
 to 1 */ mask  bits of d  - 1; /* Set all d  := 2 mask 3.
 /* Outer loop */ do  0 downto  - 1 d  :=i for 4.
 to 0 */ mask  ofi ; /* Set bit i  XOR 2 mask  := mask 5.
 are 0 */ my_id  bits ofi  /* If lower then ) = 0 mask  AND my_id  ( if 6.
then ) = 0 i  AND 2 my_id  ( if 7.
; i  XOR 2 my_id  := msg_destination 8.
; msg_destination  to X send 9.
else 10.
; i  XOR 2 my_id  := msg_source 11.
; msg_source  from X receive 12.
; endelse 13.
; endif 14.
; endfor 15.
 ONE_TO_ALL_BC end 16.
-dimensional d  on a source  initiated by X Algorithm 4.2 One-to-all broadcast of a message
hypothetical hypercube. The AND and XOR operations are bitwise logical operations.
) X , source , my_id , d  GENERAL_ONE_TO_ALL_BC( procedure 1.
begin 2.
; source  XOR my_id  := my_virtual id 3.
 - 1; d  := 2 mask 4.
 /* Outer loop */ do  0 downto  - 1 d  :=i for 5.
 to 0 */ mask  ofi ; /* Set bit i  XOR 2 mask  := mask 6.
then ) = 0 mask  AND my_virtual_id  ( if 7.
then ) = 0 i  AND 2 my_virtual_id  ( if 8.
; i  XOR 2 my_virtual_id  := virtual_dest 9.
); source  XOR virtual_dest  to ( X send 10.
 to the label of the physical destination */ virtual_dest  /* Convert
else 11.
; i  XOR 2 my_virtual_id  := virtual_source 12.
); source  XOR virtual_source  from ( X receive 13.
 to the label of the physical source */ virtual_source  /* Convert
; endelse 14.
; endfor 15.
 GENERAL_ONE_TO_ALL_BC end 16.
-dimensional hypercube. Each node contributes d Algorithm 4.3 Single-node accumulation on a
 words, and node 0 is the destination of the sum. The AND and XOR m  containing X a message
operations are bitwise logical operations.
) sum , X , m , my_id , d  ALL_TO_ONE_REDUCE( procedure 1.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
begin 2.
]; j[ X ] := j[ sum do  - 1 m to  := 0 j for 3.
 := 0; mask 4.
do  - 1 d to  := 0i for 5.
 bits are 0 */i  /* Select nodes whose lower
then ) = 0 mask  AND my_id  ( if 6.
then  0 ) i  AND 2 my_id  ( if 7.
; i  XOR 2 my_id  := msg_destination 8.
; msg_destination  to sum send 9.
else 10.
; i  XOR 2 my_id  := msg_source 11.
; msg_source  from X receive 12.
do  - 1 m to  := 0 j for 13.
]; j[ X ] + j[ sum ] := j[ sum 14.
; endelse 15.
 to 1 */ mask  ofi ; /* Set bit i  XOR 2 mask  := mask 16.
; endfor 17.
 ALL_TO_ONE_REDUCE end 18.
4.1.6 Cost Analysis
 processes participate in the p Analyzing the cost of one-to-all broadcast and all-to-one reduction is fairly straightforward. Assume that
 point-to-point p  words. The broadcast or reduction procedure involves log m operation and the data to be broadcast or reduced contains
. Therefore, the total time taken by the procedure is m wt  + st simple message transfers, each at a time cost of
1  Equation 4.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.2 All-to-All Broadcast and Reduction
 nodes simultaneously initiate a broadcast. A process sends the p  is a generalization of one-to-all broadcast in which all All-to-all broadcast
-word message to every other process, but different processes may broadcast different messages. All-to-all broadcast is used in m same
, in all-to-all reduction matrix operations, including matrix multiplication and matrix-vector multiplication. The dual of all-to-all broadcast is
 illustrates all-to-all broadcast and all-to-all Figure 4.8 which every node is the destination of an all-to-one reduction (Problem 4.8).
reduction.
Figure 4.8. All-to-all broadcast and all-to-all reduction.
 one-to-all broadcasts, one starting at each node. If performed naively, on some p One way to perform an all-to-all broadcast is to perform
 times as long as a one-to-all broadcast. It is possible to use the communication links in the p architectures this approach may take up to
 one-to-all broadcasts simultaneously so that all messages traversing the same p interconnection network more efficiently by performing all
path at the same time are concatenated into a single message whose size is the sum of the sizes of individual messages.
The following sections describe all-to-all broadcast on linear array, mesh, and hypercube topologies.
4.2.1 Linear Array and Ring
While performing all-to-all broadcast on a linear array or a ring, all communication links can be kept busy simultaneously until the operation
is complete because each node always has some information that it can pass along to its neighbor. Each node first sends to one of its
neighbors the data it needs to broadcast. In subsequent steps, it forwards the data received from one of its neighbors to its other neighbor.
 illustrates all-to-all broadcast for an eight-node ring. The same procedure would also work on a linear array with bidirectional Figure 4.9
links. As with the previous figures, the integer label of an arrow indicates the time step during which the message is sent. In all-to-all
, each message is identified by its initial source, whose Figure 4.9 -node ensemble. In p  different messages circulate in the p broadcast,
label appears in parentheses along with the time step. For instance, the arc labeled 2 (7) between nodes 0 and 1 represents the data
 shows, if communication is performed Figure 4.9 communicated in time step 2 that node 0 received from node 7 in the preceding step. As
 - 1) steps. p  - 1) pieces of information from all other nodes in ( p circularly in a single direction, then each node receives all (
Figure 4.9. All-to-all broadcast on an eight-node ring. The label of each arrow shows the time
step and, within parentheses, the label of the node that owned the current message being
transferred before the beginning of the broadcast. The number(s) in parentheses next to each
node are the labels of nodes from which data has been received prior to the current
communication step. Only the first, second, and last communication steps are shown.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 at my_msg -node ring. The initial message to be broadcast is known locally as p  gives a procedure for all-to-all broadcast on a Algorithm 4.4
. As the program shows, all-to-all result  messages in p each node. At the end of the procedure, each node stores the collection of all
broadcast on a mesh applies the linear array procedure twice, once along the rows and once along the columns.
-node ring. p Algorithm 4.4 All-to-all broadcast on a
) result , p , my_msg , my_id  ALL_TO_ALL_BC_RING( procedure 1.
begin 2.
; p  - 1) mod my_id  := ( left 3.
; p  + 1) mod my_id  := ( right 4.
; my_msg  := result 5.
; result  := msg 6.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
do  - 1 p to  := 1i for 7.
; right  to msg send 8.
; left  from msg receive 9.
; msg result  := result 10.
; endfor 11.
 ALL_TO_ALL_BC_RING end 12.
 messages, each one destined to be accumulated at a distinct p In all-to-all reduction, the dual of all-to-all broadcast, each node starts with
node. All-to-all reduction can be performed by reversing the direction and sequence of the messages. For example, the first
[1] to msg  with node 0 sending Figure 4.9 communication step for all-to-all reduction on an 8-node ring would correspond to the last step of
7 instead of receiving it. The only additional step required is that upon receiving a message, a node must combine it with the local copy of
the message that has the same destination as the received message before forwarding the combined message to the next neighbor.
-node ring. p  gives a procedure for all-to-all reduction on a Algorithm 4.5
-node ring. p Algorithm 4.5 All-to-all reduction on a
) result , p , my_msg , my_id  ALL_TO_ALL_RED_RING( procedure 1.
begin 2.
; p  - 1) mod my_id  := ( left 3.
; p  + 1) mod my_id  := ( right 4.
 := 0; recv 5.
do  - 1 p to  := 1i for 6.
; p ) mod i  + my_id  := ( j 7.
; recv ] + j[ msg  := temp 8.
; left  to temp send 9.
; right  from recv receive 10.
; endfor 11.
; recv ] + my_id[ msg  := result 12.
 ALL_TO_ALL_RED_RING end 13.
4.2.2 Mesh
Just like one-to-all broadcast, the all-to-all broadcast algorithm for the 2-D mesh is based on the linear array algorithm, treating rows and
columns of the mesh as linear arrays. Once again, communication takes place in two phases. In the first phase, each row of the mesh
 messages corresponding to performs an all-to-all broadcast using the procedure for the linear array. In this phase, all nodes collect
, and proceeds  nodes of their respective rows. Each node consolidates this information into a single message of size the
to the second communication phase of the algorithm. The second communication phase is a columnwise all-to-all broadcast of the
-word data that originally resided on different nodes. m  pieces of p consolidated messages. By the end of this phase, each node obtains all
The distribution of data among the nodes of a 3 x 3 mesh at the beginning of the first and the second phases of the algorithm is shown in
. Figure 4.10
Figure 4.10. All-to-all broadcast on a 3 x 3 mesh. The groups of nodes communicating with each
other in each phase are enclosed by dotted boundaries. By the end of the second phase, all
nodes get (0,1,2,3,4,5,6,7) (that is, a message from each node).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 mesh. The mesh procedure for all-to-all reduction is left as  gives a procedure for all-to-all broadcast on a Algorithm 4.6
an exercise for the reader (Problem 4.4).
 nodes. p Algorithm 4.6 All-to-all broadcast on a square mesh of
) result , p , my_msg , my_id  ALL_TO_ALL_BC_MESH( procedure 1.
begin 2.
/* Communication along rows */
;  - 1)mod my_id ) + (  mod my_id  - ( my_id  := left 3.
;  + 1) mod my_id ) + (  mod my_id  - ( my_id  := right 4.
; my_msg  := result 5.
; result  := msg 6.
do  - 1 to  := 1i for 7.
; right  to msg send 8.
; left  from msg receive 9.
; msg result  := result 10.
; endfor 11.
/* Communication along columns */
; p ) mod  - my_id  := ( up 12.
; p ) mod  + my_id  := ( down 13.
; result  := msg 14.
do  - 1 to  := 1i for 15.
; down  to msg send 16.
; up  from msg receive 17.
; msg result  := result 18.
; endfor 19.
 ALL_TO_ALL_BC_MESH end 20.
4.2.3 Hypercube
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
p  dimensions. The procedure requires log p The hypercube algorithm for all-to-all broadcast is an extension of the mesh algorithm to log
-node hypercube in each step. In every step, pairs of nodes p steps. Communication takes place along a different dimension of the
exchange their data and double the size of the message to be transmitted in the next step by concatenating the received message with
 shows these steps for an eight-node hypercube with bidirectional communication channels. Figure 4.11 their current data.
Figure 4.11. All-to-all broadcast on an eight-node hypercube.
-dimensional hypercube. Communication starts from the lowest d  gives a procedure for implementing all-to-all broadcast on a Algorithm 4.7
dimension of the hypercube and then proceeds along successively higher dimensions (Line 4). In each iteration, nodes communicate in
 th least significant bit of their binaryi  th iteration differ in thei pairs so that the labels of the nodes communicating with each other in the
representations (Line 5). After an iteration's communication steps, each node concatenates the data it receives during that iteration with its
resident data (Line 8). This concatenated message is transmitted in the following iteration.
-dimensional hypercube. d Algorithm 4.7 All-to-all broadcast on a
) result , d , my_msg , my_id  ALL_TO_ALL_BC_HCUBE( procedure 1.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
begin 2.
; my_msg  := result 3.
do  - 1 d to  := 0i for 4.
; i  XOR 2 my id  := partner 5.
; partner  to result send 6.
; partner  from msg receive 7.
; msg result  := result 8.
; endfor 9.
 ALL_TO_ALL_BC_HCUBE end 10.
As usual, the algorithm for all-to-all reduction can be derived by reversing the order and direction of messages in all-to-all broadcast.
Furthermore, instead of concatenating the messages, the reduction operation needs to select the appropriate subsets of the buffer to send
-dimensional d  gives a procedure for all-to-all reduction on a Algorithm 4.8 out and accumulate received messages in each iteration.
 to index into the location where the recloc  to index into the starting location of the outgoing message and senloc hypercube. It uses
incoming message is added in each iteration.
-dimensional hypercube. AND and XOR are bitwise d Algorithm 4.8 All-to-all broadcast on a
logical-and and exclusive-or operations, respectively.
) result , d , msg , my_id  ALL_TO_ALL_RED_HCUBE( procedure 1.
begin 2.
 := 0; recloc 3.
do  0 to  - 1 d  :=i for 4.
; i  XOR 2 my_id  := partner 5.
; i  AND 2 my_id  := j 6.
; i ) AND 2 i  XOR 2 my_id  := ( k 7.
; k  + recloc  := senloc 8.
; j  + recloc  := recloc 9.
; partner  - 1] toi  + 2 senloc .. senloc[ msg send 10.
; partner  - 1] fromi [0 .. 2 temp receive 11.
do  - 1i  2 to  := 0 j for 12.
]; j ] + temp[ j  + recloc[ msg ] := j  + recloc[ msg 13.
; endfor 14.
; endfor 15.
]; my_id[ msg  := result 16.
 ALL_TO_ALL_RED_HCUBE end 17.
4.2.4 Cost Analysis
 - 1 steps of communication between nearest neighbors. Each step, involving a p On a ring or a linear array, all-to-all broadcast involves
. Therefore, the time taken by the entire operation is m wt  + st , takes time m message of size
2  Equation 4.
 nodes) concludes in time  simultaneous all-to-all broadcasts (each among Similarly, on a mesh, the first phase of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, but the . The number of nodes participating in each all-to-all broadcast in the second phase is also
 to complete. The time . Therefore, this phase takes time size of each message is now
-node two-dimensional square mesh is the sum of the times spent in the individual phases, which is p for the entire all-to-all broadcast on a
3  Equation 4.
m wt -1i  + 2 st . It takes a pair of nodes time m -1i  steps is 2 p  th of the logi -node hypercube, the size of each message exchanged in the p On a
 th step. Hence, the time to complete the entire procedure isi to send and receive messages from each other during the
4  Equation 4.
 in the expressions for the communication time of all-to-all broadcast is wt  show that the term associated with 4.4 , and 4.3 , 4.2 Equations
 - 1) for all the architectures. This term also serves as a lower bound for the communication time of all-to-all broadcast for parallel p ( m wt
 - 1) p ( m computers on which a node can communicate on only one of its ports at a time. This is because each node receives at least
words of data, regardless of the architecture. Thus, for large messages, a highly connected network like a hypercube is no better than a
simple ring in performing all-to-all broadcast or all-to-all reduction. In fact, the straightforward all-to-all broadcast algorithm for a simple
 one-to-all broadcasts, p architecture like a ring has great practical importance. A close look at the algorithm reveals that it is a sequence of
 nearest-neighbor p each with a different source. These broadcasts are pipelined so that all of them are complete in a total of
communication steps. Many parallel algorithms involve a series of one-to-all broadcasts with different sources, often interspersed with
 broadcasts would n , then Section 4.1.3 some computation. If each one-to-all broadcast is performed using the hypercube algorithm of
, all of them can be performed Figure 4.9 . On the other hand, by pipelining the broadcasts as shown in p ) log m wt  + st ( n require time
. In p n  - 1) in communication, provided that the sources of all broadcasts are different and p )( m wt  + st spending no more than time (
later chapters, we show how such pipelined broadcast improves the performance of some parallel algorithms such as Gaussian
). Section 10.4.2 ), and Floyd's algorithm for finding the shortest paths in a graph ( Section 8.3.3 ), back substitution ( Section 8.3.1 elimination (
Another noteworthy property of all-to-all broadcast is that, unlike one-to-all broadcast, the hypercube algorithm cannot be applied unaltered
to mesh and ring architectures. The reason is that the hypercube procedure for all-to-all broadcast would cause congestion on the
 shows the result of Figure 4.12 communication channels of a smaller-dimensional network with the same number of nodes. For instance,
) of the hypercube all-to-all broadcast procedure on a ring. One of the links of the ring is traversed Figure 4.11(c) performing the third step (
by all four messages and would take four times as much time to complete the communication step.
 for the Figure 4.11(c) Figure 4.12. Contention for a channel when the communication step of
hypercube is mapped onto a ring.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.3 All-Reduce and Prefix-Sum Operations
The communication pattern of all-to-all broadcast can be used to perform some other operations as well. One of these operations is a third
m  and the final results of the operation are identical buffers of size m variation of reduction, in which each node starts with a buffer of size
 buffers using an associative operator. Semantically, this operation, often referred p on each node that are formed by combining the original
 operation, is identical to performing an all-to-one reduction followed by a one-to-all broadcast of the result. This all-reduce to as the
 simultaneous all-to-one reductions take place, each with a different destination for p operation is different from all-to-all reduction, in which
the result.
An all-reduce operation with a single-word message on each node is often used to implement barrier synchronization on a
message-passing computer. The semantics of the reduction operation are such that, while executing a parallel program, no node can
finish the reduction before each node has contributed a value.
A simple method to perform all-reduce is to perform an all-to-one reduction followed by a one-to-all broadcast. However, there is a faster
 illustrates this algorithm for an eight-node Figure 4.11 way to perform all-reduce by using the communication pattern of all-to-all broadcast.
hypercube. Assume that each integer in parentheses in the figure, instead of denoting a message, denotes a number to be added that
originally resided at the node with that integer label. To perform reduction, we follow the communication steps of the all-to-all broadcast
procedure, but at the end of each step, add two numbers instead of concatenating two messages. At the termination of the reduction
l procedure, each node holds the sum (0 + 1 + 2 + ··· + 7) (rather than eight messages numbered from 0 to 7, as in the case of all-to-al
broadcast). Unlike all-to-all broadcast, each message transferred in the reduction operation has only one word. The size of the messages
does not double in each step because the numbers are added instead of being concatenated. Therefore, the total communication time for
 steps is p all log
5  Equation 4.
 are numbers (rather than messages), and the union result , and msg , my_msg  numbers if p  can be used to perform a sum of Algorithm 4.7
') on Line 8 is replaced by addition. operation ('
 operation) is another important problem that can be solved by using a communication scan  (also known as the prefix sums Finding
 (one on each node), the -1 p n , ..., 1 n , 0 n  numbers p pattern similar to that used in all-to-all broadcast and all-reduce operations. Given
 - 1. For example, if the original sequence of p  between 0 and k  for all problem is to compute the sums
, and at the end of k  resides on the node labeled k n numbers is <3, 1, 4, 0, 2>, then the sequence of prefix sums is <3, 4, 8, 8, 10>. Initially,
m  . Instead of starting with a single numbers, each node could start with a buffer or vector of size k s the procedure, the same node holds
-word result would be the sum of the corresponding elements of buffers. m and the
. The modification Figure 4.11  illustrates the prefix sums procedure for an eight-node hypercube. This figure is a modification of Figure 4.13
-node subset of those nodes k  uses information from only the k is required to accommodate the fact that in prefix sums the node with label
. To accumulate the correct prefix sum, every node maintains an additional result buffer. This k whose labels are less than or equal to
. At the end of a communication step, the content of an incoming message is added to Figure 4.13 buffer is denoted by square brackets in
the result buffer only if the message comes from a node with a smaller label than that of the recipient node. The contents of the outgoing
message (denoted by parentheses in the figure) are updated with every incoming message, just as in the case of the all-reduce operation.
For instance, after the first communication step, nodes 0, 2, and 4 do not add the data received from nodes 1, 3, and 5 to their result
buffers. However, the contents of the outgoing messages for the next step are updated.
Figure 4.13. Computing prefix sums on an eight-node hypercube. At each node, square
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
brackets show the local prefix sum accumulated in the result buffer and parentheses enclose
the contents of the outgoing message buffer for the next step.
Since not all of the messages received by a node contribute to its final result, some of the messages it receives may be redundant. We
, although the presence or absence Figure 4.13 have omitted these steps of the standard all-to-all broadcast communication pattern from
 gives a procedure to solve the prefix sums problem on a Algorithm 4.9 of these messages does not affect the results of the algorithm.
-dimensional hypercube. d
-dimensional hypercube. d Algorithm 4.9 Prefix sums on a
) result , d , my number , my_id  PREFIX_SUMS_HCUBE( procedure 1.
begin 2.
; my_number  := result 3.
; result  := msg 4.
do  - 1 d to  := 0i for 5.
; i  XOR 2 my_id  := partner 6.
; partner  to msg send 7.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
; partner  from number receive 8.
; number  + msg  := msg 9.
; number  + result  := result then ) my_id  < partner  ( if 10.
; endfor 11.
 PREFIX_SUMS_HCUBE end 12.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.4 Scatter and Gather
one-to-all  to every other node. This operation is also known as m  operation, a single node sends a unique message of size scatter In the
. One-to-all personalized communication is different from one-to-all broadcast in that the source node starts personalized communication
 unique messages, one destined for each node. Unlike one-to-all broadcast, one-to-all personalized communication does not involve p with
 operation, or gather any duplication of data. The dual of one-to-all personalized communication or the scatter operation is the
, in which a single node collects a unique message from each node. A gather operation is different from an all-to-one concatenation
 illustrates the scatter and gather operations. Figure 4.14 reduce operation in that it does not involve any combination or reduction of data.
Figure 4.14. Scatter and gather operations.
Although the scatter operation is semantically different from one-to-all broadcast, the scatter algorithm is quite similar to that of the
 shows the communication steps for the scatter operation on an eight-node hypercube. The communication patterns Figure 4.15 broadcast.
) are identical. Only the size and the contents of messages are different. In Figure 4.15 ) and scatter ( Figure 4.6 of one-to-all broadcast (
, the source node (node 0) contains all the messages. The messages are identified by the labels of their destination nodes. In Figure 4.15
the first communication step, the source transfers half of the messages to one of its neighbors. In subsequent steps, each node that has
 communication steps corresponding to p some data transfers half of it to a neighbor that has yet to receive any data. There is a total of log
 dimensions of the hypercube. p the log
Figure 4.15. The scatter operation on an eight-node hypercube.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 word message. In the first step, every odd numbered m The gather operation is simply the reverse of scatter. Each node starts with an
node sends its buffer to an even numbered neighbor behind it, which concatenates the received message with its own buffer. Only the
even numbered nodes participate in the next communication step which results in nodes with multiples of four labels gathering more data
and doubling the sizes of their data. The process continues similarly, until node 0 has gathered the entire data.
Just like one-to-all broadcast and all-to-one reduction, the hypercube algorithms for scatter and gather can be applied unaltered to linear
array and mesh interconnection topologies without any increase in the communication time.
Figure 4.15 ). As Section 2.4.3 /2-node subcubes ( p -node hypercube along a certain dimension join two p  All links of a Cost Analysis
illustrates, in each communication step of the scatter operations, data flow from one subcube to another. The data that a node owns
before starting communication in a certain dimension are such that half of them need to be sent to a node in the other subcube. In every
step, a communicating node keeps half of its data, meant for the nodes in its subcube, and sends the other half to its neighbor in the other
subcube. The time in which all data are distributed to their respective destinations is
6  Equation 4.
 - 1) p ( m wt  + p  log st The scatter and gather operations can also be performed on a linear array and on a 2-D square mesh in time
(Problem 4.7). Note that disregarding the term due to message-startup time, the cost of scatter and gather operations for large messages
 - 1) words of data must be p ( m ) is similar. In the scatter operation, at least Section 2.4.3  mesh interconnection network ( d - k on any
 - 1) words of data must be received by the destination node. p ( m transmitted out of the source node, and in the gather operation, at least
 - 1) is a lower bound on the communication time of scatter and gather operations. p ( m wt Therefore, as in the case of all-to-all broadcast,
This lower bound is independent of the interconnection network.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.5 All-to-All Personalized Communication
 to every other node. Each node sends different m , each node sends a distinct message of size all-to-all personalized communication In
Figure 4.16 messages to different nodes, unlike all-to-all broadcast, in which each node sends the same message to all other nodes.
illustrates the all-to-all personalized communication operation. A careful observation of this figure would reveal that this operation is
Figure  processes using one-dimensional array partitioning ( p equivalent to transposing a two-dimensional array of data distributed among
. This operation is used in a variety of parallel algorithms total exchange ). All-to-all personalized communication is also known as 3.24
such as fast Fourier transform, matrix transpose, sample sort, and some parallel database join operations.
Figure 4.16. All-to-all personalized communication.
Example 4.2 Matrix transposition
. Consider an n  < j , i ] for 0i , j[ A ] = j , i  [ T A  of the same size, such that T A  is a matrix A  matrix n  x n The transpose of an
 processors such that each processor contains one full row of the matrix. With this mapping, n  matrix mapped onto n  x n
 - 1]. After the transposition, n , i , 1], ..., [ i , 0], [ i  initially contains the elements of the matrix with indices [i processor P
 , buti ] initially resides on P j , i , and so on. In general, element [ 1 , 1] belongs to P i , element [ 0 , 0] belongs to P i element [
 for a 4 x 4 Figure 4.17  during the transposition. The data-communication pattern of this procedure is shown in j moves to P
matrix mapped onto four processes using one-dimensional rowwise partitioning. Note that in this figure every processor
sends a distinct element of the matrix to every other processor. This is an example of all-to-all personalized
communication.
Figure 4.17. All-to-all personalized communication in transposing a 4 x 4 matrix
using four processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 elements) of p/ 2 n  rows (that is, p/ n , then each process initially holds n p  processes such that p In general, if we use
the matrix. Performing the transposition now involves an all-to-all personalized communication of matrix blocks of size
, instead of individual elements. p/ n  x p/ n
We now discuss the implementation of all-to-all personalized communication on parallel computers with linear array, mesh, and hypercube
interconnection networks. The communication patterns of all-to-all personalized communication are identical to those of all-to-all broadcast
on all three architectures. Only the size and the contents of messages are different.
4.5.1 Ring
 shows the steps in an all-to-all personalized communication on a six-node linear array. To perform this operation, every node Figure 4.18
 is thei }, where j , i . In the figure, these pieces of data are identified by pairs of integers of the form { m  - 1 pieces of data, each of size p sends
 - 1) p ( m  is its final destination. First, each node sends all pieces of data as one consolidated message of size j source of the message and
 - 1) words of data received by a node in this step, one p ( m to one of its neighbors (all nodes communicate in the same direction). Of the
-word packet belongs to it. Therefore, each node extracts the information meant for it from the data received, and forwards the remaining m
 - 1 steps. The total size of data being transferred between p  each to the next node. This process continues for m  - 2) pieces of size p (
-word packet originating from a m  words in each successive step. In every step, each node adds to its collection one m nodes decreases by
 - 1 steps, every node receives the information from all other nodes in the ensemble. p different node. Hence, in
Figure 4.18. All-to-all personalized communication on a six-node ring. The label of each
 is the label of the node that originally owned the x }, where y , x message is of the form {
 is the label of the node that is the final destination of the message. The label y message, and
 individual n }) indicates a message that is formed by concatenating n y , n x }, ..., { 2 y , 2 x }, { 1 y , 1 x ({
messages.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In the above procedure, all messages are sent in the same direction. If half of the messages are sent in one direction and the remaining
 can be reduced by a factor of two. For the sake of simplicity, wt half are sent in the other direction, then the communication cost due to the
we ignore this constant-factor improvement.
 - 1 communication steps. Since the p  On a ring or a bidirectional linear array, all-to-all personalized communication involves Cost Analysis
), the total time taken by this operation is i  - p ( m  th step isi size of the messages transferred in the
7  Equation 4.
 - 1) words of data because it has an p ( m In the all-to-all personalized communication procedure described above, each node sends
-word packet for every other node. Assume that all messages are sent either clockwise or counterclockwise. The average distance that an m
 nodes, each performing the same type of p /2. Since there are p , which is equal to -word packet travels is m
 - 1) x p ( m communication, the total traffic (the total number of data words transferred between directly-connected nodes) on the network is
. Hence, the communication time for this operation is at p . The total number of inter-node links in the network to share this load is p /2 x p
, this is exactly the time taken by the st /2. Disregarding the message startup time p  - 1) p ( m wt , which is equal to p /2)/ 2 p  - 1) p ( m  x wt least (
linear array procedure. Therefore, the all-to-all personalized communication algorithm described in this section is optimal.
4.5.2 Mesh
 messages according to the columns of p  mesh, each node first groups its In all-to-all personalized communication on a
-word messages, one meant for each m  shows a 3 x 3 mesh, in which every node initially has nine Figure 4.19 their destination nodes.
 messages each). The  groups of node. Each node assembles its data into three groups of three messages each (in general,
first group contains the messages destined for nodes labeled 0, 3, and 6; the second group contains the messages for nodes labeled 1, 4,
and 7; and the last group has messages for nodes labeled 2, 5, and 8.
Figure 4.19. The distribution of messages at the beginning of each phase of all-to-all
 hasi personalized communication on a 3 x 3 mesh. At the end of the second phase, node
 8. The groups of nodes communicating together in i }), where 0i }, ..., {8,i messages ({0,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
each phase are enclosed in dotted boundaries.
After the messages are grouped, all-to-all personalized communication is performed independently in each row with clustered messages
 shows the distribution of Figure 4.19(b)  nodes of a particular column. . One cluster contains the information for all of size
data among the nodes at the end of this phase of communication.
Before the second communication phase, the messages in each node are sorted again, this time according to the rows of their destination
nodes; then communication similar to the first phase takes place in all the columns of the mesh. By the end of this phase, each node
receives a message from every other node.
 for the  for the number of nodes, and  We can compute the time spent in the first phase by substituting Cost Analysis
. The time spent in the second . The result of this substitution is Equation 4.7 message size in
 on a m phase is the same as that in the first phase. Therefore, the total time for all-to-all personalized communication of messages of size
-node two-dimensional square mesh is p
8  Equation 4.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 does not take into account the time Equation 4.8 The expression for the communication time of all-to-all personalized communication in
required for the local rearrangement of data (that is, sorting the messages by rows or columns). Assuming that initially the data is ready for
 is the time to rt  words of data. If mp the first communication phase, the second communication phase requires the rearrangement of
perform a read and a write operation on a single word of data in a node's local memory, then the total time spent in data rearrangement by
 (Problem 4.21). This time is much smaller than the time spent by each node in communication. mp rt a node during the entire procedure is
 for all-to-all Equation 4.8 An analysis along the lines of that for the linear array would show that the communication time given by
personalized communication on a square mesh is optimal within a small constant factor (Problem 4.11).
4.5.3 Hypercube
-node hypercube is to simply extend the two-dimensional mesh p One way of performing all-to-all personalized communication on a
 shows the communication steps required to perform this operation on a three-dimensional Figure 4.20  dimensions. p algorithm to log
 steps. Pairs of nodes exchange data in a different dimension in p hypercube. As shown in the figure, communication takes place in log
Section /2 nodes each ( p /2 links in the same dimension connects two subcubes of p -node hypercube, a set of p each step. Recall that in a
 each. While communicating in a m  packets of size p ). At any stage in all-to-all personalized communication, every node holds 2.4.3
/2 of these packets (consolidated as one message). The destinations of these packets are the p particular dimension, every node sends
nodes of the other subcube connected by the links in current dimension.
Figure 4.20. An all-to-all personalized communication algorithm on a three-dimensional
hypercube.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 communication steps. This is necessary p In the preceding procedure, a node must rearrange its messages locally before each of the log
/2 messages destined for the same node in a communication step occupy contiguous memory locations so that they p to make sure that all
can be transmitted as a single consolidated message.
/2 words of data are exchanged along the mp  In the above hypercube algorithm for all-to-all personalized communication, Cost Analysis
 iterations. The resulting total communication time is p bidirectional channels in each of the log
9  Equation 4.
 is spent by each node p  log mp rt  words of data. Hence, a total time of mp  communication steps, a node rearranges p Before each of the log
 is the time needed to perform a read and a write operation on a single rt in local rearrangement of data during the entire procedure. Here
; hence, the time to perform an all-to-all wt  is much smaller than rt word of data in a node's local memory. For most practical computers,
personalized communication is dominated by the communication time.
p Interestingly, unlike the linear array and mesh algorithms described in this section, the hypercube algorithm is not optimal. Each of the
)/2. Therefore, p  - 1) words of data and the average distance between any two nodes on a hypercube is (log p ( m nodes sends and receives
)/2 links in the hypercube network, the lower p  log p )/2. Since there is a total of ( p  - 1) x (log p ( m  x p the total data traffic on the network is
bound on the all-to-all personalized communication time is
An Optimal Algorithm
An all-to-all personalized communication effectively results in all pairs of nodes exchanging some data. On a hypercube, the best way to
 - 1 p perform this exchange is to have every pair of nodes communicate directly with each other. Thus, each node simply performs
 words of data with a different node in every step. A node must choose its communication partner in m communication steps, exchanging
 shows one such congestion-free schedule for pairwise Figure 4.21 each step so that the hypercube links do not suffer congestion.
 exchanges data with nodei  th communication step, node j exchange of data in a three-dimensional hypercube. As the figure shows, in the
). For example, in part (a) of the figure (step 1), the labels of communicating partners differ in the least significant bit. In part (g) j  XORi (
(step 7), the labels of communicating partners differ in all the bits, as the binary representation of seven is 111. In this figure, all the paths
in every communication step are congestion-free, and none of the bidirectional links carry more than one message in the same direction.
This is true in general for a hypercube of any dimension. If the messages are routed appropriately, a congestion-free schedule exists for
 that a message Section 2.4.3 -node hypercube. Recall from p  - 1 communication steps of all-to-all personalized communication on a p the
 (that is, j  andi  is the Hamming distance betweenl  links, wherel  on a hypercube must pass through at least j  to nodei traveling from node
l  traverses links in j  to nodei )). A message traveling from node j  XORi the number of nonzero bits in the binary representation of (
)). Although the message can follow one of the j  XORi dimensions (corresponding to the nonzero bits in the binary representation of (
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 > 1), a distinct path is obtained by sorting the dimensions along which thel  (assuming j  andi  that exist betweenl several paths of length
message travels in ascending order. According to this strategy, the first link is chosen in the dimension corresponding to the least
. E-cube routing ), and so on. This routing scheme is known as j  XORi significant nonzero bit of (
Figure 4.21. Seven steps in all-to-all personalized communication on an eight-node hypercube.
-dimensional hypercube is based on this strategy. d Algorithm 4.10 for all-to-all personalized communication on a
Algorithm 4.10 A procedure to perform all-to-all personalized communication on a
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. j  initially resides on node i and is destined for node j,i M -dimensional hypercube. The message d
) my_id , d  ALL_TO_ALL_PERSONAL( procedure 1.
begin 2.
do  - 1 d  2 to  := 1i for 3.
begin 4.
; i  XOR my_id  := partner 5.
; partner  to partner , my_id M send 6.
; partner  from my_id, partner M receive 7.
; endfor 8.
 ALL_TO_ALL_PERSONAL end 9.
 + st , a communication time of Algorithm 4.10  E-cube routing ensures that by choosing communication pairs according to Cost Analysis
 because there is no contention with any other message traveling in j  and nodei  is guaranteed for a message transfer between node m wt
. The total communication time for the entire operation is j  andi the same direction along the link between nodes
0  Equation 4.1
 is higher for the second hypercube algorithm, while the term st  shows the term associated with 4.10  and 4.9 A comparison of Equations
 is higher for the first algorithm. Therefore, for small messages, the startup time may dominate, and the first algorithm wt associated with
may still be useful.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.6 Circular Shift
. A permutation is a simultaneous, permutation Circular shift is a member of a broader class of global communication operations known as
 as -shift q circular  words to a unique node. We define a m one-to-one data redistribution operation in which each node sends a packet of
). The shift operation finds p  < q -node ensemble (0 < p  in a p ) mod q  +i  sends a data packet to node (i the operation in which node
application in some matrix computations and in string and image pattern matching.
4.6.1 Mesh
} q  - p  , q -shift is fairly intuitive on a ring or a bidirectional linear array. It can be performed by min{ q The implementation of a circular
neighbor-to-neighbor communications in one direction. Mesh algorithms for circular shift can be derived by using the ring algorithm.
-node square wraparound mesh in two stages. p -shift can be performed on a q If the nodes of the mesh have row-major labels, a circular
)  mod q  for a circular 5-shift on a 4 x 4 mesh. First, the entire set of data is shifted simultaneously by ( Figure 4.22 This is illustrated in
 steps along the columns. During the circular row shifts, some of the data traverse steps along the rows. Then it is shifted by
the wraparound connection from the highest to the lowest labeled nodes of the rows. All such data packets must shift an additional step
 distance that they lost while traversing the backward edge in their respective forward along the columns to compensate for the
 requires one row shift, a compensatory column shift, and finally one column shift. Figure 4.22 rows. For example, the 5-shift in
Figure 4.22. The communication steps in a circular 5-shift on a 4 x 4 mesh.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In practice, we can choose the direction of the shifts in both the rows and the columns to minimize the number of steps in a circular shift.
For instance, a 3-shift on a 4 x 4 mesh can be performed by a single backward row shift. Using this strategy, the number of unit shifts in a
. direction cannot exceed
-node mesh p -shift on a q  Taking into account the compensating column shift for some packets, the total time for any circular Cost Analysis
 has an upper bound of m using packets of size
4.6.2 Hypercube
-dimensional hypercube. We do d  nodes onto a d In developing a hypercube algorithm for the shift operation, we map a linear array with 2
Figure . i -bit binary reflected Gray code (RGC) of d  is the j  of the hypercube such that j  of the linear array to nodei this by assigning node
 on the linear array arei  illustrates this mapping for eight nodes. A property of this mapping is that any two nodes at a distance of 2 4.23
 = 0 (that is, directly-connected nodes on the linear array) when only onei separated by exactly two links on the hypercube. An exception is
hypercube link separates the two nodes.
Figure 4.23. The mapping of an eight-node linear array onto a three-dimensional hypercube to
perform a circular 5-shift as a combination of a 4-shift and a 1-shift.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 as a sum of distinct powers of 2. The number of terms in the sum is the same as the number of ones in q -shift, we expand q To perform a
. These two terms correspond to bit positions 0 and 0  + 2 2 . For example, the number 5 can be expressed as 2 q the binary representation of
-shift on a hypercube is q  distinct powers of 2, then the circular s  is the sum of q 2 in the binary representation of 5, which is 101. If
 phases. s performed in
In each phase of communication, all data packets move closer to their respective destinations by short cutting the linear array (mapped
 shows, a 5-shift is performed by a 4-shift followed by a Figure 4.23 onto the hypercube) in leaps of the powers of 2. For example, as
. Each q -shift is exactly equal to the number of ones in the binary representation of q 1-shift. The number of communication phases in a
 is 1), consists of a q phase consists of two communication steps, except the 1-shift, which, if required (that is, if the least significant bit of
) consists of two steps and the second phase of a 1-shift Figure 4.23(a) single step. For example, in a 5-shift, the first phase of a 4-shift (
 - 1. p -node hypercube is at most 2 log p  in a q ) consists of one step. Thus, the total number of steps for any Figure 4.23(b) (
All communications in a given time step are congestion-free. This is ensured by the property of the linear array mapping that all nodes
whose mutual distance on the linear array is a power of 2 are arranged in disjoint subarrays on the hypercube. Thus, all nodes can freely
, in which nodes labeled 0, 3, 4, and 7 form Figure 4.23(a) communicate in a circular fashion in their respective subarrays. This is shown in
one subarray and nodes labeled 1, 2, 5, and 6 form another subarray.
-node hypercube is p -word packets on a m The upper bound on the total communication time for any shift of
1  Equation 4.1
 by performing both forward and backward shifts. For example, on eight nodes, a p ) log m wt  + st We can reduce this upper bound to (
6-shift can be performed by a single backward 2-shift instead of a forward 4-shift followed by a forward 2-shift.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is used, then the time for circular shift on a hypercube can be improved Section 4.5 We now show that if the E-cube routing introduced in
 <l i  (l  for large messages. This is because with E-cube routing, each pair of nodes with a constant distance p by almost a factor of log
 illustrates the non-conflicting Figure 4.24 -node hypercube with bidirectional channels. p ) has a congestion-free path (Problem 4.22) in a p
-node p -shift on a q  < 8 on an eight-node hypercube. In a circular q  -shift operations for 1 q paths of all the messages in circular
 (Problem 4.23). Thus, j  is divisible by 2 q  such that j ) is the highest integer q ( g ) links, where q ( g  - p hypercube, the longest path contains log
 is m the total communication time for messages of length
2  Equation 4.1
 < 8. q -shifts on an 8-node hypercube for 1 q Figure 4.24. Circular
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.7 Improving the Speed of Some Communication Operations
So far in this chapter, we have derived procedures for various communication operations and their communication times under the
assumptions that the original messages could not be split into smaller parts and that each node had a single port for sending and
receiving data. In this section, we briefly discuss the impact of relaxing these assumptions on some of the communication operations.
4.7.1 Splitting and Routing Messages in Parts
-word packet of data travels between the source and the m , we assumed that an entire 4.6 – 4.1 In the procedures described in Sections
destination nodes along the same path. If we split large messages into smaller parts and then route these parts through different paths,
we can sometimes utilize the communication network better. We have already shown that, with a few exceptions like one-to-all
broadcast, all-to-one reduction, all-reduce, etc., the communication operations discussed in this chapter are asymptotically optimal for
 in the costs of these operations cannot be reduced asymptotically. In this section, wt large messages; that is, the terms associated with
we present asymptotically optimal algorithms for three global communication operations.
 roughly equal parts. Therefore, the earlier p  being large enough to be split into m Note that the algorithms of this section rely on
algorithms are still useful for shorter messages. A comparison of the cost of the algorithms in this section with those presented earlier in
 decreases wt  increases and the term associated with st this chapter for the same operations would reveal that the term associated with
m , there is a cut-off value for the message size p , and wt , st when the messages are split. Therefore, depending on the actual values of
and only the messages longer than the cut-off would benefit from the algorithms in this section.
One-to-All Broadcast
 is large enough m -node ensemble. If p  from one source node to all the nodes in a m  of size M Consider broadcasting a single message
 in timei  on nodei M ) can place Section 4.4  each, then a scatter operation ( p/ m  of size -1 p M , ..., 1 M , 0 M  parts p  can be split into M so that
 on all nodes. This -1 p M · ·· 1 M 0 M  = M  - 1). Note that the desired result of the one-to-all broadcast is to place p )( p/ m ( wt  + p  log st
 residing on each node after the scatter operation. This p/ m can be accomplished by an all-to-all broadcast of the messages of size
 - 1) on a hypercube. Thus, on a hypercube, one-to-all broadcast can be p )( p/ m ( wt  + p  log st all-to-all broadcast can be completed in time
performed in time
3  Equation 4.1
 term has been reduced by a factor of (log wt , this algorithm has double the startup cost, but the cost due to the Equation 4.1 Compared to
)/2. Similarly, one-to-all broadcast can be improved on linear array and mesh interconnection networks as well. p
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
All-to-One Reduction
All-to-one reduction is a dual of one-to-all broadcast. Therefore, an algorithm for all-to-one reduction can be obtained by reversing the
direction and the sequence of communication in one-to-all broadcast. We showed above how an optimal one-to-all broadcast algorithm
can be obtained by performing a scatter operation followed by an all-to-all broadcast. Therefore, using the notion of duality, we should be
able to perform an all-to-one reduction by performing all-to-all reduction (dual of all-to-all broadcast) followed by a gather operation (dual
of scatter). We leave the details of such an algorithm as an exercise for the reader (Problem 4.17).
All-Reduce
Since an all-reduce operation is semantically equivalent to an all-to-one reduction followed by a one-to-all broadcast, the asymptotically
optimal algorithms for these two operations presented above can be used to construct a similar algorithm for the all-reduce operation.
Breaking all-to-one reduction and one-to-all broadcast into their component operations, it can be shown that an all-reduce operation can
be accomplished by an all-to-all reduction followed by a gather followed by a scatter followed by an all-to-all broadcast. Since the
intermediate gather and scatter would simply nullify each other's effect, all-reduce just requires an all-to-all reduction and an all-to-all
 words. Then, an p/ m  components of size roughly p  nodes are logically split into p -word messages on each of the m broadcast. First, the
-word component of the final p/ m . After this step, each node is left with a distinct i p  th components oni all-to-all reduction combines all the
result. An all-to-all broadcast can construct the concatenation of these components on each node.
st  in time p/ m -node hypercube interconnection network allows all-to-one reduction and one-to-all broadcast involving messages of size p A
 - 1) each. Therefore, the all-reduce operation can be completed in time p )( p/ m ( wt  + p log
4  Equation 4.1
4.7.2 All-Port Communication
In a parallel architecture, a single node may have multiple communication ports with links to other nodes in the ensemble. For example,
 ports. In this book, d -dimensional hypercube has d each node in a two-dimensional wraparound mesh has four ports, and each node in a
 model. In single-port communication, a node can send data on single-port communication we generally assume what is known as the
only one of its ports at a time. Similarly, a node can receive data on only one port at a time. However, a node can send and receive data
 model all-port communication simultaneously, either on the same port or on separate ports. In contrast to the single-port model, an
permits simultaneous communication on all the channels connected to a node.
 in the expressions for the communication times of one-to-all wt -node hypercube with all-port communication, the coefficients of p On a
. Since the p and all-to-all broadcast and personalized communication are all smaller than their single-port counterparts by a factor of log
number of channels per node for a linear array or a mesh is constant, all-port communication does not provide any asymptotic
improvement in communication time on these architectures.
Despite the apparent speedup, the all-port communication model has certain limitations. For instance, not only is it difficult to program,
but it requires that the messages are large enough to be split efficiently among different channels. In several parallel algorithms, an
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
increase in the size of messages means a corresponding increase in the granularity of computation at the nodes. When the nodes are
working with large data sets, the internode communication time is dominated by the computation time if the computational complexity of
 computations 3 n the algorithm is higher than the communication complexity. For example, in the case of matrix multiplication, there are
 words of data transferred among the nodes. If the communication time is a small fraction of the total parallel run time, then 2 n for
improving the communication by using sophisticated techniques is not very advantageous in terms of the overall run time of the parallel
algorithm.
Another limitation of all-port communication is that it can be effective only if data can be fetched and stored in memory at a rate sufficient
-node hypercube, the memory p to sustain all the parallel communication. For example, to utilize all-port communication effectively on a
; that is, the memory p bandwidth must be greater than the communication bandwidth of a single channel by a factor of at least log
bandwidth must increase with the number of nodes to support simultaneous communication on all ports. Some modern parallel
computers, like the IBM SP, have a very natural solution for this problem. Each node of the distributed-memory parallel computer is a
NUMA shared-memory multiprocessor. Multiple ports are then served by separate memory banks and full memory and communication
bandwidth can be utilized if the buffers for sending and receiving data are placed appropriately across different memory banks.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.8 Summary
 summarizes the communication times for various collective communications operations discussed in this chapter. The time for Table 4.1
one-to-all broadcast, all-to-one reduction, and the all-reduce operations is the minimum of two expressions. This is because, depending
Table 4.1  are faster. Section 4.7  or the ones described in 4.3  and 4.1 , either the algorithms described in Sections m on the message size
 have Table 4.1 assumes that the algorithm most suitable for the given message size is chosen. The communication-time expressions in
been derived in the earlier sections of this chapter in the context of a hypercube interconnection network with cut-through routing.
(p) cross-section bandwidth Q However, these expressions and the corresponding algorithms are valid for any architecture with a
, except all-to-all personalized Table 4.1  for the expressions for all operations listed in wt ). In fact, the terms associated with Section 2.4.4 (
 mesh network) provided that the d - k communication and circular shift, would remain unchanged even on ring and mesh networks (or any
 gives the asymptotic Table 4.1 logical processes are mapped onto the physical nodes of the network appropriately. The last column of
cross-section bandwidth required to perform an operation in the time given by the second column of the table, assuming an optimal
(p) Q mapping of processes to nodes. For large messages, only all-to-all personalized communication and circular shift require the full
, when applying the expressions for the time of these operations on a Section 2.5.1 cross-section bandwidth. Therefore, as discussed in
 term must reflect the effective bandwidth. For example, the bisection width of a wt network with a smaller cross-section bandwidth, the
(1). Therefore, while performing all-to-all personalized communication on Q -node ring is p  and that of a Q -node square mesh is p
) p ( Q  of individual links, and on a ring, it would be wt  times the Q a square mesh, the effective per-word transfer time would be
 of individual links. wt times the
Table 4.1. Summary of communication times of various operations discussed in Sections
 and m  on a hypercube interconnection network. The message size for each operation is 4.7 – 4.1
. p the number of nodes is
B/W Requirement Hypercube Time Operation
One-to-all broadcast,
All-to-one reduction
(1) Q )) m wt  + p  log st , 2( p ) log m wt  + st min((
All-to-all broadcast,
All-to-all reduction
(1) Q  - 1) p ( m wt  + p  log st
(1) Q )) m wt  + p  log st , 2( p ) log m wt  + st min(( All-reduce
(1) Q  - 1) p ( m wt  + p  log st Scatter, Gather
) p ( Q  - 1) p )( m wt  + st ( All-to-all personalized
) p ( Q m wt  + st Circular shift
Table 4.2. MPI names of the various operations discussed in this chapter.
MPI Name Operation
MPI_Bcast One-to-all broadcast
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
MPI Name Operation
MPI_Reduce All-to-one reduction
MPI_Allgather All-to-all broadcast
MPI_Reduce_scatter All-to-all reduction
MPI_Allreduce All-reduce
MPI_Gather Gather
MPI_Scatter Scatter
MPI_Alltoall All-to-all personalized
The collective communications operations discussed in this chapter occur frequently in many parallel algorithms. In order to facilitate
speedy and portable design of efficient parallel programs, most parallel computer vendors provide pre-packaged software for performing
these collective communications operations. The most commonly used standard API for these operations is known as the Message
 gives the names of the MPI functions that correspond to the communications operations described Table 4.2 Passing Interface, or MPI.
in this chapter.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
4.9 Bibliographic Remarks
In this chapter, we studied a variety of data communication operations for the linear array, mesh, and hypercube interconnection
] discuss implementation issues for these operations on these and other architectures, such as SS89b topologies. Saad and Schultz [
shared-memory and a switch or bus interconnect. Most parallel computer vendors provide standard APIs for inter-process
] and the 96 + SOHL communications via message-passing. Two of the most common APIs are the message passing interface (MPI) [
]. 94 + GBD parallel virtual machine (PVM) [
The hypercube algorithm for a certain communication operation is often the best algorithm for other less-connected architectures too, if
they support cut-through routing. Due to the versatility of the hypercube architecture and the wide applicability of its algorithms, extensive
, Joh90 , JH89 , FF86 , BT97 , BR90 , 91 + BOS work has been done on implementing various communication operations on hypercubes [
]. The properties of a hypercube network that are used in deriving the algorithms for various SW87 , SS89a , RS90b , MdV87
]. SS88 communication operations on it are described by Saad and Schultz [
The all-to-all personalized communication problem in particular has been analyzed for the hypercube architecture by Boppana and
]. E-cube routing that guarantees congestion-free Tak87 ], and Take [ Sei89 ], Seidel [ JH91 ], Johnsson and Ho [ BR90 Raghavendra [
]. Nug88 communication in Algorithm 4.10 for all-to-all personalized communication is described by Nugent [
]. Our discussion of the circular RS90b  are described by Ranka and Sahni [ Section 4.3 The all-reduce and the prefix sums algorithms of
, has been scan ]. A generalized form of prefix sums, often referred to as BT97 shift operation is adapted from Bertsekas and Tsitsiklis [
, and scan vector model ] defines a Ble90 used by some researchers as a basic primitive in data-parallel programming. Blelloch [
describes how a wide variety of parallel programs can be expressed in terms of the scan primitive and its variations.
] and BT97 The hypercube algorithm for one-to-all broadcast using spanning binomial trees is described by Bertsekas and Tsitsiklis [
-word message to be broadcast into log m , we split the Section 4.7.1 ]. In the spanning tree algorithm described in JH89 Johnsson and Ho [
] show that the optimal size of the parts is JH89  for ease of presenting the algorithm. Johnsson and Ho [ p /log m  parts of size p
. These smaller messages are sent from p . In this case, the number of messages may be greater than log
-node p  subtrees in a circular fashion. With this strategy, one-to-all broadcast on a p the root of the spanning binomial tree to its log
. hypercube can be performed in time
Algorithms using the all-port communication model have been described for a variety of communication operations on the hypercube
], and Stout SS89a ], Saad and Schultz [ HJ87 ], Ho and Johnsson [ JH89 ], Johnsson and Ho [ BT97 architecture by Bertsekas and Tsitsiklis [
 in the wt -node hypercube with all-port communication, the coefficients of p ] show that on a JH89 ]. Johnsson and Ho [ SW87 and Wagar [
expressions for the communication times of one-to-all and all-to-all broadcast and personalized communication are all smaller than those
] show that all-port communication may not improve the GK91 . Gupta and Kumar [ p of their single-port counterparts by a factor of log
scalability of an algorithm on a parallel architecture over single-port communication.
The elementary operations described in this chapter are not the only ones used in parallel applications. A variety of other useful
], BPC Jaj92 , HS86 ], pointer jumping [ Akl89 operations for parallel computers have been described in literature, including selection [
, Ble90 ], and keyed-scan or multi-prefix [ Loa92 ], bit reversal [ Sch80 , Lev87 ], packing [ 83 + GGK ], fetch-and-op [ RS90b , Joh90 permutations [
]. Ran89
Sometimes data communication does not follow any predefined pattern, but is arbitrary, depending on the application. In such cases, a
simplistic approach of routing the messages along the shortest data paths between their respective sources and destinations leads to
] discuss VB81 ], and Valiant and Brebner [ Val82 ], Valiant [ LMR88 contention and imbalanced communication. Leighton, Maggs, and Rao [
efficient routing methods for arbitrary permutations of messages.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
 so that they work for any number of processes, not just the powers of 2. 4.3 , and 4.2 , 4.1  Modify Algorithms 4.1
 presents the recursive doubling algorithm for one-to-all broadcast, for all three networks (ring, Section 4.1 4.2
, a message is sent along the highest Figure 4.6 mesh, hypercube). Note that in the hypercube algorithm of
 - 1 to 0). The same d  goes down fromi , line 4, Algorithm 4.1 dimension first, and then sent to lower dimensions (in
algorithm can be used for mesh and ring and ensures that messages sent in different time steps do not interfere
with each other.
, Algorithm 3.1 Let's now change the algorithm so that the message is sent along the lowest dimension first (i.e., in
 - 1). So in the first time step, processor 0 will communicate with processor 1; in the d  goes up from 0 toi line 4,
second time step, processors 0 and 1 will communicate with 2 and 3, respectively; and so on.
What is the run time of this revised algorithm on hypercube? . 1
What is the run time of this revised algorithm on ring? . 2
 messages have to traverse the same link at the same time, then assume that the k For these derivations, if
. w kt effective per-word-transfer time for these messages is
 On a ring, all-to-all broadcast can be implemented in two different ways: (a) the standard ring algorithm as 4.3
. Figure 4.11  and (b) the hypercube algorithm as shown in Figure 4.9 shown in
What is the run time for case (a)? . 1
What is the run time for case (b)? . 2
 messages have to traverse the same link at the same time, then assume that the effective per-word-transfer k If
. wt  = 100 x st . Also assume that w kt time for these messages is
 is very large? m Which of the two methods, (a) or (b), above is better if the message size . 1
 is very small (may be one word)? m Which method is better if . 2
 for performing all-to-all reduction on a mesh. Algorithm 4.6  Write a procedure along the lines of 4.4
, describe a procedure to Figure 4.7  Given a balanced binary tree as shown in 4.5 (All-to-all broadcast on a tree)
 nodes. Assume that only p -word messages on m  for p /2) log mp wt  + st perform all-to-all broadcast that takes time (
-word messages between any two nodes m the leaves of the tree contain nodes, and that an exchange of two
 if the communication channel (or a part of it) is shared mk wt  + st connected by bidirectional channels takes time
 simultaneous messages. k by
 words, and needs to get the m  Consider the all-reduce operation in which each processor starts with an array of 4.6
global sum of the respective words in the array at each processor. This operation can be implemented on a ring
using one of the following three alternatives:
All-to-all broadcast of all the arrays followed by a local computation of the sum of the respective
elements of the array.
.i
Single node accumulation of the elements of the array, followed by a one-to-all broadcast of the result
array.
. ii
An algorithm that uses the pattern of the all-to-all broadcast, but simply adds numbers rather than
. iii
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
concatenating messages.
. wt , and st , m For each of the above cases, compute the run time in terms of . 1
 is very large. Which of the three alternatives (among (i), (ii) or (iii)) m  = 1, and wt  = 100, st Assume that
is better?
. 2
 is very small (say 1). Which of the three alternatives (among (i), (ii) m  = 1, and wt  = 100, st Assume that
or (iii)) is better?
. 3
 Give the procedures and their 4.7 (One-to-all personalized communication on a linear array and a mesh)
 nodes for the linear p -word messages on m communication times for one-to-all personalized communication of
array and the mesh architectures.
 For the mesh, the algorithm proceeds in two phases as usual and starts with the source distributing pieces of Hint:
 nodes in its row such that each of these nodes receives the data meant for all  words among the
 nodes in its column. the
 The dual of all-to-all broadcast is all-to-all reduction, in which each node is the 4.8 (All-to-all reduction)
p  nodes have a vector of p destination of an all-to-one reduction. For example, consider the scenario where
 th elements of all thei ) gets the sum of the p  <i  such that 0i  th node (for alli elements each, and the
vectors. Describe an algorithm to perform all-to-all reduction on a hypercube with addition as the associative
 is the time to perform one addition, how much time does addt  words and m operator. If each message contains
)? wt  and st , addt , p , m your algorithm take (in terms of
 such messages by the end of p  In all-to-all broadcast, each node starts with a single message and collects Hint:
 distinct messages (one meant for each node) but ends p the operation. In all-to-all reduction, each node starts with
up with a single message.
 show that for any node in a three-dimensional hypercube, there are exactly Figure 4.21  Parts (c), (e), and (f) of 4.9
three nodes whose shortest distance from the node is two links. Derive an exact expression for the number of
. l -node hypercube is p ) whose shortest distance from any given node in a l  and p nodes (in terms of
 is an p/ n  is the number of nodes and p  numbers if n  Give a hypercube algorithm to compute prefix sums of 4.10
 to send a message of unit st  to add two numbers and time addt integer greater than 1. Assuming that it takes time
length between two directly-connected nodes, give an exact expression for the total time taken by the algorithm.
 for the time  is zero, then the expression st  Show that if the message startup time 4.11
 4)  mesh is optimal within a small ( taken by all-to-all personalized communication on a
constant factor.
 to work without the end-to-end 4.5 – 4.1  Modify the linear array and the mesh algorithms in Sections 4.12
wraparound connections. Compare the new communication times with those of the unmodified procedures. What
is the maximum factor by which the time for any of the operations increases on either the linear array or the
mesh?
 Give optimal (within a small constant) algorithms for one-to-all and all-to-all broadcasts and 4.13 (3-D mesh)
 nodes with store-and-forward p  three-dimensional mesh of 1/3 p  x 1/3 p  x 1/3 p personalized communications on a
routing. Derive expressions for the total communication times of these procedures.
 nodes is proportional to the total number of p  Assume that the cost of building a parallel computer with 4.14
communication links within it. Let the cost effectiveness of an architecture be inversely proportional to the product
-node ensemble of this architecture and the communication time of a certain operation on it. p of the cost of a
s  to be zero, which architecture is more cost effective for each of the operations discussed in thi st Assuming
? chapter – a standard 3-D mesh or a sparse 3-D mesh
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 = 0. Under this model of communication, the wt  is a nonzero constant but st  Repeat Problem 4.14 when 4.15
message transfer time between two directly-connected nodes is fixed, regardless of the size of the message.
. st Also, if two packets are combined and transmitted as one message, the communication latency is still
 nodes simultaneously perform a k -to-all broadcast be an operation in which k  Let -to-all broadcast) 4.16 (k
-word messages. Give an algorithm for this operation that has a total communication m one-to-all broadcast of
-word messages cannot be split, m -node hypercube. Assume that the p  - 1) on a k ) + k / p  log( k ( m wt  + p  log st time of
. p k  is a power of 2, and 1 k
) p  - 1)/ p ( m wt  + p  log st  Give a detailed description of an algorithm for performing all-to-one reduction in time 2( 4.17
 each. p/ m  nearly equal parts of size p  into m -node hypercube by splitting the original messages of size p on a
-to-all k  If messages can be split and their parts can be routed independently, then derive an algorithm for 4.18
-node p broadcast such that its communication time is less than that of the algorithm in Problem 4.16 for a
hypercube.
-node p  can be performed on a m , then all-to-one reduction with message size p m  Show that, if 4.19
) in communication. m wt  + p  log st hypercube spending time 2(
 Express all-to-one reduction as a combination of all-to-all reduction and gather. Hint:
 nodes simultaneously k -to-all personalized communication, k  In -to-all personalized communication) 4.20 (k
-node ensemble with individual packets p ) in a p k perform a one-to-all personalized communication (1
) + k / p  (log( st  is a power of 2, then this operation can be performed on a hypercube in time k . Show that, if m of size
 - 1). p ( m wt  - 1) + k
 to perform a read and a write operation on a single word of data in a node's rt  Assuming that it takes time 4.21
) spends a total of Section 4.5.2 -node mesh ( p local memory, show that all-to-all personalized communication on a
 is the size of an individual message. m  in internal data movement on the nodes, where mp rt time
. m  array of messages of size  The internal data movement is equivalent to transposing a Hint:
-shift are congestion-free if E-cube routing q  data paths in a circular p -node hypercube, all the p  Show that in a 4.22
) is used. Section 4.5 (
-node hypercube. (2) Prove by induction on p )-shift on a q  - p -shift is isomorphic to a ( q /2, then a p  > q  (1) If Hint:
-node hypercube, then all p ) on a p  < q -shift (1 q hypercube dimension. If all paths are congestion-free for a
-node hypercube also. p these paths are congestion-free on a 2
 - p -node hypercube is log p -shift on a q  Show that the length of the longest path of any message in a circular 4.23
. j  is divisible by 2 q  such that j ) is the highest integer q ( g ), where q ( g
-node hypercube. (2) Prove by induction on hypercube dimension. p  - 1 on a p ) = log q ( g /2, then p  = q  (1) If Hint:
) increases by one each time the number of nodes is doubled. q ( g , q For a given
 Derive an expression for the parallel run time of the hypercube algorithms for one-to-all broadcast, all-to-all 4.24
broadcast, one-to-all personalized communication, and all-to-all personalized communication adapted unaltered
for a mesh with identical communication links (same channel width and channel rate). Compare the performance
of these adaptations with that of the best mesh algorithms.
, two common measures of the cost of a network are (1) the total number of Section 2.4.4  As discussed in 4.25
wires in a parallel computer (which is a product of number of communication links and channel width); and (2) the
 = 1. The wt bisection bandwidth. Consider a hypercube in which the channel width of each link is one, that is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
channel width of a mesh-connected computer with equal number of nodes and identical cost is higher, and is
' represent the factors by which the channel width of the mesh is s  and s determined by the cost metric used. Let
'. Using these, derive the s  and s increased in accordance with the two cost metrics. Derive the values of
communication time of the following operations on a mesh:
One-to-all broadcast . 1
All-to-all broadcast . 2
One-to-all personalized communication . 3
All-to-all personalized communication . 4
Compare these times with the time taken by the same operations on a hypercube with equal cost.
 nodes. For the four communication operations in Problem 4.25 p  Consider a completely-connected network of 4.26
derive an expression for the parallel run time of the hypercube algorithms on the completely-connected network.
Comment on whether the added connectivity of the network yields improved performance for these operations.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 5. Analytical Modeling of Parallel Programs
A sequential algorithm is usually evaluated in terms of its execution time, expressed as a function of the size of its input. The execution
time of a parallel algorithm depends not only on input size but also on the number of processing elements used, and their relative
computation and interprocess communication speeds. Hence, a parallel algorithm cannot be evaluated in isolation from a parallel
 is the combination of an algorithm and the parallel architecture on which it parallel system architecture without some loss in accuracy. A
is implemented. In this chapter, we study various metrics for evaluating the performance of parallel systems.
A number of measures of performance are intuitive. Perhaps the simplest of these is the wall-clock time taken to solve a given problem
on a given parallel platform. However, as we shall see, a single figure of merit of this nature cannot be extrapolated to other problem
instances or larger machine configurations. Other intuitive measures quantify the benefit of parallelism, i.e., how much faster the parallel
program runs with respect to the serial program. However, this characterization suffers from other drawbacks, in addition to those
mentioned above. For instance, what is the impact of using a poorer serial algorithm that is more amenable to parallel processing? For
these reasons, more complex measures for extrapolating performance to larger machine configurations or problems are often
necessary. With these objectives in mind, this chapter focuses on metrics for quantifying the performance of parallel programs.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.1 Sources of Overhead in Parallel Programs
Using twice as many hardware resources, one can reasonably expect a program to run twice as fast. However, in typical parallel
programs, this is rarely the case, due to a variety of overheads associated with parallelism. An accurate quantification of these
overheads is critical to the understanding of parallel program performance.
. In addition to performing essential computation (i.e., Figure 5.1 A typical execution profile of a parallel program is illustrated in
computation that would be performed by the serial program for solving the same problem instance), a parallel program may also spend
time in interprocess communication, idling, and excess computation (computation not performed by the serial formulation).
Figure 5.1. The execution profile of a hypothetical parallel program executing on eight
processing elements. Profile indicates times spent performing computation (both essential
and excess), communication, and idling.
,  Any nontrivial parallel system requires its processing elements to interact and communicate data (e.g. Interprocess Interaction
intermediate results). The time spent communicating data between processing elements is usually the most significant source of parallel
processing overhead.
d  Processing elements in a parallel system may become idle due to many reasons such as load imbalance, synchronization, an Idling
presence of serial components in a program. In many parallel applications (for example, when task generation is dynamic), it is
impossible (or at least difficult) to predict the size of the subtasks assigned to various processing elements. Hence, the problem cannot
be subdivided statically among the processing elements while maintaining uniform workload. If different processing elements have
different workloads, some processing elements may be idle during part of the time that others are working on the problem. In some
parallel programs, processing elements must synchronize at certain points during parallel program execution. If all processing elements
are not ready for synchronization at the same time, then the ones that are ready sooner will be idle until all the rest are ready. Parts of an
algorithm may be unparallelizable, allowing only a single processing element to work on it. While one processing element works on the
serial part, all the other processing elements must wait.
o  The fastest known sequential algorithm for a problem may be difficult or impossible to parallelize, forcing us t Excess Computation
use a parallel algorithm based on a poorer but easily parallelizable (that is, one with a higher degree of concurrency) sequential
algorithm. The difference in computation performed by the parallel program and the best serial program is the excess computation
overhead incurred by the parallel program.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
A parallel algorithm based on the best serial algorithm may still perform more aggregate computation than the serial algorithm. An
example of such a computation is the Fast Fourier Transform algorithm. In its serial version, the results of certain computations can be
reused. However, in the parallel version, these results cannot be reused because they are generated by different processing elements.
 discusses these algorithms in Chapter 13 Therefore, some computations are performed multiple times on different processing elements.
detail.
Since different parallel algorithms for solving the same problem incur varying overheads, it is important to quantify these overheads with
a view to establishing a figure of merit for each algorithm.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.2 Performance Metrics for Parallel Systems
It is important to study the performance of parallel programs with a view to determining the best algorithm, evaluating hardware platforms,
and examining the benefits from parallelism. A number of metrics have been used based on the desired outcome of performance analysis.
5.2.1 Execution Time
The serial runtime of a program is the time elapsed between the beginning and the end of its execution on a sequential computer. The
 is the time that elapses from the moment a parallel computation starts to the moment the last processing element finishes parallel runtime
. P T  and the parallel runtime by S T execution. We denote the serial runtime by
5.2.2 Total Parallel Overhead
. We define overhead function The overheads incurred by a parallel program are encapsulated into a single expression referred to as the
 of a parallel system as the total time collectively spent by all the processing elements over and total overhead overhead function or
above that required by the fastest known sequential algorithm for solving the same problem on a single processing element. We denote
. o T the overhead function of a parallel system by the symbol
 units of this time are spent performing useful S T  . P pT The total time spent in solving a problem summed over all processing elements is
) is given by o T work, and the remainder is overhead. Therefore, the overhead function (
1  Equation 5.
5.2.3 Speedup
When evaluating a parallel system, we are often interested in knowing how much performance gain is achieved by parallelizing a given
application over a sequential implementation. Speedup is a measure that captures the relative benefit of solving a problem in parallel. It is
defined as the ratio of the time taken to solve a problem on a single processing element to the time required to solve the same problem on
. S  identical processing elements. We denote speedup by the symbol p a parallel computer with
Example 5.1 Adding n numbers using n processing elements
 processing elements. Initially, each processing element is assigned n  numbers by using n Consider the problem of adding
one of the numbers to be added and, at the end of the computation, one of the processing elements stores the sum of all
 steps by propagating partial sums n  is a power of two, we can perform this operation in log n the numbers. Assuming that
 = 16. The processing elements n  illustrates the procedure for Figure 5.2 up a logical binary tree of processing elements.
are labeled from 0 to 15. Similarly, the 16 numbers to be added are labeled from 0 to 15. The sum of the numbers with
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
.  is denoted by j  toi consecutive labels from
 denotes the Figure 5.2. Computing the globalsum of 16 partial sums using 16 processing elements.
. j  toi sum of numbers with consecutive labels from
 consists of one addition and the communication of a single word. The addition can be Figure 5.2 Each step shown in
. wt  + st , and the communication of a single word can be performed in time ct performed in some constant time, say
Therefore, the addition and communication operations take a constant amount of time. Thus,
2  Equation 5.
) time on a single processing element, its speedup is n ( Q Since the problem can be solved in
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
3  Equation 5.
For a given problem, more than one sequential algorithm may be available, but all of these may not be equally suitable for parallelization.
When a serial computer is used, it is natural to use the sequential algorithm that solves the problem in the least amount of time. Given a
parallel algorithm, it is fair to judge its performance with respect to the fastest sequential algorithm for solving the same problem on a
single processing element. Sometimes, the asymptotically fastest sequential algorithm to solve a problem is not known, or its runtime has
a large constant that makes it impractical to implement. In such cases, we take the fastest known algorithm that would be a practical
choice for a serial computer to be the best sequential algorithm. We compare the performance of a parallel algorithm to solve a problem
 as the ratio of the serial runtime of S speedup with that of the best sequential algorithm to solve the same problem. We formally define the
 processing p the best sequential algorithm for solving a problem to the time taken by the parallel algorithm to solve the same problem on
 processing elements used by the parallel algorithm are assumed to be identical to the one used by the sequential algorithm. p elements. The
Example 5.2 Computing speedups of parallel programs
5 ). Assume that a serial version of bubble sort of 10 Section 9.3.1 Consider the example of parallelizing bubble sort (
records takes 150 seconds and a serial quicksort can sort the same list in 30 seconds. If a parallel version of bubble sort,
also called odd-even sort, takes 40 seconds on four processing elements, it would appear that the parallel odd-even sort
algorithm results in a speedup of 150/40 or 3.75. However, this conclusion is misleading, as in reality the parallel algorithm
results in a speedup of 30/40 or 0.75 with respect to the best serial algorithm.
 units of time to S T . If the best sequential algorithm takes p Theoretically, speedup can never exceed the number of processing elements,
 processing elements if none of the p  can be obtained on p solve a given problem on a single processing element, then a speedup of
 is possible only if each processing element spends less than p . A speedup greater than p  / S T processing elements spends more than time
 processing elements and solve the problem in p  solving the problem. In this case, a single processing element could emulate the p  / S T time
 units of time. This is a contradiction because speedup, by definition, is computed with respect to the best sequential S T fewer than
 on a single processing element. S T  is the serial runtime of the algorithm, then the problem cannot be solved in less than time S T algorithm. If
). This usually happens superlinear speedup  is sometimes observed (a phenomenon known as p In practice, a speedup greater than
when the work performed by a serial algorithm is greater than its parallel formulation or due to hardware features that put the serial
implementation at a disadvantage. For example, the data for a problem might be too large to fit into the cache of a single processing
element, thereby degrading its performance due to the use of slower memory elements. But when partitioned among several processing
elements, the individual data-partitions would be small enough to fit into their respective processing elements' caches. In the remainder of
this book, we disregard superlinear speedup due to hierarchical memory.
Example 5.3 Superlinearity effects from caches
Consider the execution of a parallel program on a two-processor parallel system. The program attempts to solve a
. With this size and available cache of 64 KB on one processor, the program has a cache hit W problem instance of size
rate of 80%. Assuming the latency to cache of 2 ns and latency to DRAM of 100 ns, the effective memory access time is 2
x 0.8 + 100 x 0.2, or 21.6 ns. If the computation is memory bound and performs one FLOP/memory access, this
corresponds to a processing rate of 46.3 MFLOPS. Now consider a situation when each of the two processors is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
/2). At this problem size, the cache hit ratio is expected to be W effectively executing half of the problem instance (i.e., size
higher, since the effective problem size is smaller. Let us assume that the cache hit ratio is 90%, 8% of the remaining data
comes from local DRAM, and the other 2% comes from the remote DRAM (communication overhead). Assuming that
remote data access takes 400 ns, this corresponds to an overall access time of 2 x 0.9 + 100 x 0.08 + 400 x 0.02, or 17.8
ns. The corresponding execution rate at each processor is therefore 56.18, for a total execution rate of 112.36 MFLOPS.
The speedup in this case is given by the increase in speed over serial formulation, i.e., 112.36/46.3 or 2.43! Here, because
of increased cache hit ratio resulting from lower problem size per processor, we notice superlinear speedup.
Example 5.4 Superlinearity effects due to exploratory decomposition
Consider an algorithm for exploring leaf nodes of an unstructured tree. Each leaf has a label associated with it and the
objective is to find a node with a specified label, in this case 'S'. Such computations are often used to solve combinatorial
, we illustrate such a Figure 5.3 ). In Section 11.6 problems, where the label 'S' could imply the solution to the problem (
tree. The solution node is the rightmost leaf in the tree. A serial formulation of this problem based on depth-first tree
. Now ct  to visit a node, the time for this traversal is 14 ct traversal explores the entire tree, i.e., all 14 nodes. If it takes time
consider a parallel formulation in which the left subtree is explored by processing element 0 and the right subtree by
processing element 1. If both processing elements explore the tree at the same speed, the parallel formulation explores
only the shaded nodes before the solution is found. Notice that the total work done by the parallel algorithm is only nine
 (one root ct . The corresponding parallel time, assuming the root node expansion is serial, is 5 ct node expansions, i.e., 9
node expansion, followed by four node expansions by each processing element). The speedup of this two-processor
 , or 2.8! ct /5 ct execution is therefore 14
Figure 5.3. Searching an unstructured tree for a node with a given label, 'S', on two processing elements
using depth-first traversal. The two-processor version with processor 0 searching the left subtree and
processor 1 searching the right subtree expands only the shaded nodes before the solution is found. The
corresponding serial formulation expands the entire tree. It is clear that the serial algorithm does more
work than the parallel algorithm.
The cause for this superlinearity is that the work performed by parallel and serial algorithms is different. Indeed, if the
two-processor algorithm was implemented as two processes on the same processing element, the algorithmic
superlinearity would disappear for this problem instance. Note that when exploratory decomposition is used, the relative
amount of work performed by serial and parallel algorithms is dependent upon the location of the solution, and it is often
not possible to find a serial algorithm that is optimal for all instances. Such effects are further analyzed in greater detail in
. Chapter 11
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
5.2.4 Efficiency
. In practice, ideal behavior is not achieved p  processing elements can deliver a speedup equal to p Only an ideal parallel system containing
because while executing a parallel algorithm, the processing elements cannot devote 100% of their time to the computations of the
 numbers is spent idling n , part of the time required by the processing elements to compute the sum of Example 5.1 algorithm. As we saw in
 is a measure of the fraction of time for which a processing element is usefully employed; Efficiency (and communicating in real systems).
 and efficiency p it is defined as the ratio of speedup to the number of processing elements. In an ideal parallel system, speedup is equal to
 and efficiency is between zero and one, depending on the effectiveness with which the p is equal to one. In practice, speedup is less than
. Mathematically, it is given by E processing elements are utilized. We denote efficiency by the symbol
4  Equation 5.
Example 5.5 Efficiency of adding n numbers on n processing elements
 processing n  numbers on n  and the preceding definition, the efficiency of the algorithm for adding Equation 5.3 From
elements is
We also illustrate the process of deriving the parallel runtime, speedup, and efficiency while preserving various constants associated with
the parallel platform.
Example 5.6 Edge detection on images
 pixel image, the problem of detecting edges corresponds to applying a3x 3 template to each pixel. The n  x n Given an
process of applying the template corresponds to multiplying pixel values with corresponding template values and summing
 along with typical templates Figure 5.4(a) across the template (a convolution operation). This process is illustrated in
, the entire ct ). Since we have nine multiply-add operations for each pixel, if each multiply-add takes time Figure 5.4(b) (
 on a serial computer. 2 n ct operation takes time 9
Figure 5.4. Example of edge detection: (a) an 8 x 8 image; (b) typical templates for detecting edges; and
(c) partitioning of the image across four processors with shaded regions indicating image data that must
be communicated from neighboring processors to processor 1.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
A simple parallel algorithm for this problem partitions the image equally across the processing elements and each
processing element applies the template to its own subimage. Note that for applying the template to the boundary pixels, a
processing element must get data that is assigned to the adjoining processing element. Specifically, if a processing
 pixels from the n ), it must access a single layer of p/ n  x ( n element is assigned a vertically sliced subimage of dimension
 pixels from the processing element to the right (note that one of these n processing element to the left and a single layer of
accesses is redundant for the two processing elements assigned the subimages at the extremities). This is illustrated in
. Figure 5.4(c)
 pixels with each of the two n On a message passing machine, the algorithm executes in two steps: (i) exchange a layer of
-word messages n adjoining processing elements; and (ii) apply template on local subimage. The first step involves two
). The second step takes time n wt  + st (assuming each pixel takes a word to communicate RGB data). This takes time 2(
. The total time for the algorithm is therefore given by: p/ 2 n ct 9
The corresponding values of speedup and efficiency are given by:
and
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
5.2.5 Cost
 of solving a problem on a parallel system as the product of parallel runtime and the number of processing elements cost We define the
used. Cost reflects the sum of the time that each processing element spends solving the problem. Efficiency can also be expressed as the
p ratio of the execution time of the fastest known sequential algorithm for solving a problem to the cost of solving the same problem on
processing elements.
The cost of solving a problem on a single processing element is the execution time of the fastest known sequential algorithm. A parallel
 terms) as a Q  if the cost of solving a problem on a parallel computer has the same asymptotic growth (in cost-optimal system is said to be
function of the input size as the fastest-known sequential algorithm on a single processing element. Since efficiency is the ratio of
(1). Q sequential cost to parallel cost, a cost-optimal parallel system has an efficiency of
 -optimal system. P pT , and a cost-optimal system is also known as a processor-time product  or work Cost is sometimes referred to as
Example 5.7 Cost of adding n numbers on n processing elements
n ( Q  processing elements has a processor-time product of n  numbers on n  for adding Example 5.1 The algorithm given in
), the algorithm is not cost optimal. n ( Q ). Since the serial runtime of this operation is n log
Cost optimality is a very important practical concept although it is defined in terms of asymptotics. We illustrate this using the following
example.
Example 5.8 Performance of non-cost optimal algorithms
. Since the serial runtime of a 2 ) n  processing elements to sort the list in time (log n Consider a sorting algorithm that uses
, n  and 1/log n /log n , the speedup and efficiency of this algorithm are given by n  log n (comparison-based) sort is
. Therefore, this algorithm is not cost optimal but only by a factor 2 ) n (log n  product of this algorithm is P pT respectively. The
. An n  is much less than p . Let us consider a realistic scenario in which the number of processing elements n of log
. This follows from the p/ 2 ) n (log n  processing elements gives us a parallel time less than n  < p  tasks to n assignment of these
p ; and 2 ) n (log n , then one processing element would take time 2 ) n  processing elements take time (log n fact that if
. Consider the n /log p . The corresponding speedup of this formulation is p/ 2 ) n (log n processing elements would take time
n /log p  = 10) on 32 processing elements. The speedup expected is only n  = 1024, log n problem of sorting 1024 numbers (
 = 20 and the speedup is only 1.6. Clearly, there is a n , log 6  = 10 n  increases. For n or 3.2. This number gets worse as
 is smaller p significant cost associated with not being cost-optimal even by a very small factor (note that a factor of log
). This emphasizes the practical importance of cost-optimality. than even
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.3 The Effect of Granularity on Performance
 illustrated an instance of an algorithm that is not cost-optimal. The algorithm discussed in this example uses as many Example 5.7
processing elements as the number of inputs, which is excessive in terms of the number of processing elements. In practice, we assign
larger pieces of input data to processing elements. This corresponds to increasing the granularity of computation on the processing
scaling down elements. Using fewer than the maximum possible number of processing elements to execute a parallel algorithm is called
a parallel system in terms of the number of processing elements. A naive way to scale down a parallel system is to design a parallel
algorithm for one input element per processing element, and then use fewer processing elements to simulate a large number of
n ), we can use the parallel algorithm designed for n  < p  processing elements ( p  inputs and only n processing elements. If there are
 virtual p/ n  physical processing elements simulate p  virtual processing elements and having each of the n processing elements by assuming
processing elements.
, the computation at each processing element increases by a factor of p/ n As the number of processing elements decreases by a factor of
 processing elements. If virtual processing elements are mapped p/ n  because each processing element now performs the work of p/ n
. The total p/ n appropriately onto physical processing elements, the overall communication time does not grow by more than a factor of
, and the processor-time product does not increase. Therefore, if a parallel system with p/ n parallel runtime increases, at most, by a factor of
 processing elements preserves n )to simulate n  < p  processing elements (where p  processing elements is cost-optimal, using n
cost-optimality.
A drawback of this naive method of increasing computational granularity is that if a parallel system is not cost-optimal to begin with, it may
still not be cost-optimal after the granularity of computation increases. This is illustrated by the following example for the problem of adding
 numbers. n
Example 5.9 Adding n numbers on p processing elements
 are powers of 2. We p  and n  and both n  < p  processing elements such that p  numbers on n Consider the problem of adding
 processing elements. The steps p  processing elements on n  and simulate Example 5.1 use the same algorithm as in
 is simulated by thei  = 4. Virtual processing element p  = 16 and n  for Figure 5.5 leading to the solution are shown in
n  of the log p ; the numbers to be added are distributed similarly. The first log p  modi physical processing element labeled
 processing elements. In the remaining steps, no p  steps on p ) log p/ n steps of the original algorithm are simulated in (
communication is required because the processing elements that communicate in the original algorithm are simulated by
) time in p ) log p/ n (( Q the same processing element; hence, the remaining numbers are added locally. The algorithm takes
 numbers to add, taking time p/ n the steps that require communication, after which a single processing element is left with
), p  log n ( Q ). Consequently, its cost is p ) log p/ n (( Q ). Thus, the overall parallel execution time of this parallel system is p/ n ( Q
) cost of adding n numbers sequentially. Therefore, the parallel system is not n ( Q which is asymptotically higher than the
cost-optimal.
Figure 5.5. Four processing elements simulating 16 processing elements to compute the sum of 16
 . Four j  toi  denotes the sum of numbers with consecutive labels from numbers (first two steps).
processing elements simulating 16 processing elements to compute the sum of 16 numbers (last three
steps).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processing elements to p ). When using n (log Q -processor machine in time n  numbers can be added on an n  showed that Example 5.1
 this task was Example 5.9 ). However, in n ) log p/ n (( Q ), the expected parallel runtime is n  < p  virtual processing elements ( n simulate
) instead. The reason is that every communication step of the original algorithm does not have to be p ) log p/ n (( Q performed in time
simulated; at times, communication takes place between virtual processing elements that are simulated by the same physical processing
Figure 5.5(c) element. For these operations, there is no associated overhead. For example, the simulation of the third and fourth steps (
) did not require any communication. However, this reduction in communication was not enough to make the algorithm cost-optimal. (d) and
 processing elements) can be performed cost-optimally with a p  numbers on n  illustrates that the same problem (adding Example 5.10
smarter assignment of data to processing elements.
Example 5.10 Adding n numbers cost-optimally
 = 4. In p  = 16 and n  for Figure 5.6  processing elements is illustrated in p  numbers using n An alternate method for adding
). Now the problem is p/ n ( Q  numbers in time p/ n the first step of this algorithm, each processing element locally adds its
) by the method p (log Q  processing elements, which can be done in time p  partial sums on p reduced to adding the
. The parallel runtime of this algorithm is Example 5.1 described in
5  Equation 5.
), which is the same as the serial runtime. Hence, this n ( Q ), the cost is p  log p ( W  = n ). As long as p  log p  + n ( Q and its cost is
parallel system is cost-optimal.
Figure 5.6. A cost-optimal way of computing the sum of 16 numbers using four processing elements.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
These simple examples demonstrate that the manner in which the computation is mapped onto processing elements may determine
whether a parallel system is cost-optimal. Note, however, that we cannot make all non-cost-optimal systems cost-optimal by scaling down
the number of processing elements.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.4 Scalability of Parallel Systems
Very often, programs are designed and tested for smaller problems on fewer processing elements. However, the real problems these
programs are intended to solve are much larger, and the machines contain larger number of processing elements. Whereas code
development is simplified by using scaled-down versions of the machine and the problem, their performance and correctness (of
programs) is much more difficult to establish based on scaled-down systems. In this section, we will investigate techniques for evaluating
the scalability of parallel programs using analytical tools.
Example 5.11 Why is performance extrapolation so difficult?
-point Fast Fourier Transform (FFT) on 64 processing elements. n Consider three parallel algorithms for computing an
 is increased to 18 K. Keeping the number of processing elements constant, n  illustrates speedup as the value of Figure 5.7
, one would infer from observed speedups that binary exchange and 3-D transpose algorithms are the n at smaller values of
 that the 2-D transpose Figure 5.7 best. However, as the problem is scaled up to 18 K points or more, it is evident from
.) Chapter 13 algorithm yields best speedup. (These algorithms are discussed in greater detail in
Figure 5.7. A comparison of the speedups obtained by the binary-exchange, 2-D transpose and 3-D
 for Chapter 13  = 2 (see h t  = 25, and s t  = 4, w t  = 2, c t transpose algorithms on 64 processing elements with
details).
Similar results can be shown relating to the variation in number of processing elements as the problem size is held constant.
Unfortunately, such parallel performance traces are the norm as opposed to the exception, making performance prediction based on
limited observed data very difficult.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
5.4.1 Scaling Characteristics of Parallel Programs
The efficiency of a parallel program can be written as:
), we can rewrite this expression as Equation 5.1 Using the expression for parallel overhead (
6  Equation 5.
. This is because every program must contain some serial component. If this p  is an increasing function of o T The total overhead function
, then during this time all the other processing elements must be idle. This corresponds serialt serial component of the program takes time
. In addition, due to p  grows at least linearly with o T . Therefore, the total overhead function serialt  - 1) x p to a total overhead function of (
Equation 5.6 communication, idling, and excess computation, this function may grow superlinearly in the number of processing elements.
 remains S T gives us several interesting insights into the scaling of parallel programs. First, for a given problem size (i.e. the value of
 that the overall Equation 5.6  increases. In such a scenario, it is clear from o T constant), as we increase the number of processing elements,
efficiency of the parallel program goes down. This characteristic of decreasing efficiency with increasing number of processing elements
for a given problem size is common to all parallel programs.
Example 5.12 Speedup and efficiency as functions of the number of processing elements
. Example 5.10  processing elements. We use the same algorithm as in p  numbers on n Consider the problem of adding
However, to illustrate actual speedups, we work with constants here instead of asymptotics. Assuming unit time for adding
p  time. The second phase involves log p/ n two numbers, the first phase (local summations) of the algorithm takes roughly
steps with a communication and an addition at each step. If a single communication takes unit time as well, the time for this
. Therefore, we can derive parallel time, speedup, and efficiency as: p phase is 2 log
7  Equation 5.
8  Equation 5.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
9  Equation 5.
S  shows the Figure 5.8 . p  and n These expressions can be used to calculate the speedup and efficiency for any pair of
 shows the corresponding efficiencies. Table 5.1 . p  and n  curves for a few different values of p versus
Figure 5.8. Speedup versus the number of processing elements for adding a list of numbers.
Amdahl's  illustrate that the speedup tends to saturate and efficiency drops as a consequence of Table 5.1  and Figure 5.8
 (Problem 5.1). Furthermore, a larger instance of the same problem yields higher speedup and efficiency for the same law
. p number of processing elements, although both speedup and efficiency continue to drop with increasing
Let us investigate the effect of increasing the problem size keeping the number of processing elements constant. We know that the total
 grows sublinearly o T . In many cases, p  and the number of processing elements S T  is a function of both problem size o T overhead function
 . In such cases, we can see that efficiency increases if the problem size is increased keeping the number of processing S T with respect to
elements constant. For such algorithms, it should be possible to keep the efficiency fixed by increasing both the size of the problem and
, the efficiency of adding 64 numbers using four processing Table 5.1 the number of processing elements simultaneously. For instance, in
elements is 0.80. If the number of processing elements is increased to 8 and the size of the problem is scaled up to add 192 numbers, the
 to 512 results in the same efficiency. This ability to maintain efficiency at a fixed value by n  to 16 and p efficiency remains 0.80. Increasing
simultaneously increasing the number of processing elements and the size of the problem is exhibited by many parallel systems. We call
 of a parallel system is a measure of its capacity to increase speedup in scalability  parallel systems. The scalable such systems
proportion to the number of processing elements. It reflects a parallel system's ability to utilize increasing processing resources effectively.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processing elements. p  numbers on n  for adding p  and n Table 5.1. Efficiency as a function of
 = 32 p  = 16 p  = 8 p  = 4 p  = 1 p n
0.17 0.33 0.57 0.80 1.0 64
0.38 0.60 0.80 0.92 1.0 192
0.50 0.71 0.87 0.95 1.0 320
0.62 0.80 0.91 0.97 1.0 512
(1). Therefore, scalability and cost-optimality of parallel Q  that a cost-optimal parallel system has an efficiency of Section 5.2.5 Recall from
systems are related. A scalable parallel system can always be made cost-optimal if the number of processing elements and the size of the
 processing p  numbers on n  shows that the parallel system for adding Example 5.10 computation are chosen appropriately. For instance,
 shows that the same parallel system is scalable if n is increased in proportion Example 5.13 ). p  log p ( W  = n elements is cost-optimal when
 is increased. p ) as p  log p ( Q to
Example 5.13 Scalability of adding n numbers
, the efficiency is Table 5.1 ). As shown in p  log p ( W  = n  processing elements p  numbers on n For the cost-optimal addition of
. If the number of processing elements is p  log p  = 8 n  is p  and n  = 4. At this point, the relation between p  = 64 and n 0.80 for
 = 192 for eight processing n  shows that the efficiency is indeed 0.80 with Table 5.1  = 192. p  log p increased to eight, then 8
 = 512. Thus, this parallel system remains cost-optimal p  log p  = 8 n  = 16, the efficiency is 0.80 for p elements. Similarly, for
. p  log p  is increased as 8 n at an efficiency of 0.80 if
5.4.2 The Isoefficiency Metric of Scalability
We summarize the discussion in the section above with the following two observations:
For a given problem size, as we increase the number of processing elements, the overall efficiency of the parallel system goes
down. This phenomenon is common to all parallel systems.
. 1
In many cases, the efficiency of a parallel system increases if the problem size is increased while keeping the number of
processing elements constant.
. 2
, respectively. Following from these two observations, we define a scalable (b)  and Figure 5.9(a) These two phenomena are illustrated in
parallel system as one in which the efficiency can be kept constant as the number of processing elements is increased, provided that the
problem size is also increased. It is useful to determine the rate at which the problem size must increase with respect to the number of
processing elements to keep the efficiency fixed. For different parallel systems, the problem size must increase at different rates in order
to maintain a fixed efficiency as the number of processing elements is increased. This rate determines the degree of scalability of the
parallel system. As we shall show, a lower rate is more desirable than a higher growth rate in problem size. Let us now investigate metrics
for quantitatively determining the degree of scalability of a parallel system. However, before we do that, we must define the notion of
 precisely. problem size
Figure 5.9. Variation of efficiency: (a) as the number of processing elements is increased for a given problem size; and
(b) as the problem size is increased for a given number of processing elements. The phenomenon illustrated in graph
(b) is not common to all parallel systems.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
e  When analyzing parallel systems, we frequently encounter the notion of the size of the problem being solved. Thus far, w Problem Size
 informally, without giving a precise definition. A naive way to express problem size is as a parameter of problem size have used the term
 matrices. A drawback of this definition is that the interpretation of n  x n  in case of a matrix operation involving n the input size; for instance,
problem size changes from one problem to another. For example, doubling the input size results in an eight-fold increase in the execution
) algorithm is the best matrix 3 n ( Q time for matrix multiplication and a four-fold increase for matrix addition (assuming that the conventional
multiplication algorithm, and disregarding more complicated algorithms with better asymptotic complexities).
A consistent definition of the size or the magnitude of the problem should be such that, regardless of the problem, doubling the problem
size always means performing twice the amount of computation. Therefore, we choose to express problem size in terms of the total
 matrix multiplication n  x n ) for 3 n ( Q number of basic operations required to solve the problem. By this definition, the problem size is
problem  matrix addition. In order to keep it unique for a given problem, we define n  x n ) for 2 n ( Q (assuming the conventional algorithm) and
 as the number of basic computation steps in the best sequential algorithm to solve the problem on a single processing element, size
. Because it is defined in terms of sequential time complexity, the Section 5.2.3 where the best sequential algorithm is defined as in
. W problem size is a function of the size of the input. The symbol we use to denote problem size is
In the remainder of this chapter, we assume that it takes unit time to perform one basic computation step of an algorithm. This assumption
does not impact the analysis of any parallel system because the other hardware-related constants, such as message startup time,
per-word transfer time, and per-hop time, can be normalized with respect to the time taken by a basic computation step. With this
 of the fastest known algorithm to solve the problem on a sequential S T  is equal to the serial runtime W assumption, the problem size
computer.
The Isoefficiency Function
Parallel execution time can be expressed as a function of problem size, overhead function, and the number of processing elements. We
can write parallel runtime as:
0  Equation 5.1
The resulting expression for speedup is
1  Equation 5.1
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Finally, we write the expression for efficiency as
2  Equation 5.1
 increases o T  is increased, the efficiency decreases because the total overhead p , if the problem size is kept constant and Equation 5.12 In
 is increased keeping the number of processing elements fixed, then for scalable parallel systems, the efficiency increases. This W . If p with
. For these parallel systems, efficiency can be maintained at a desired value (between 0 p ) for a fixed W ( Q  grows slower than o T is because
 is also increased. W , provided p and 1) for increasing
 in order to maintain a fixed efficiency. For instance, in p  must be increased at different rates with respect to W For different parallel systems,
 increases. Such parallel p  to keep the efficiency from dropping as p  might need to grow as an exponential function of W some cases,
systems are poorly scalable. The reason is that on these parallel systems it is difficult to obtain good speedups for a large number of
, then the p  needs to grow only linearly with respect to W processing elements unless the problem size is enormous. On the other hand, if
parallel system is highly scalable. That is because it can easily deliver speedups proportional to the number of processing elements for
reasonable problem sizes.
 is Equation 5.12  in W / o T For scalable parallel systems, efficiency can be maintained at a fixed value (between 0 and 1) if the ratio
 of efficiency, E maintained at a constant value. For a desired value
3  Equation 5.1
 can be Equation 5.13 , p  and W  is a function of o T ) be a constant depending on the efficiency to be maintained. Since E /(1 - E  = K Let
rewritten as
4  Equation 5.1
 by algebraic manipulations. This function dictates the p  can usually be obtained as a function of W , the problem size Equation 5.14 From
 of the parallel system. isoefficiency function  increases. We call this function the p  required to keep the efficiency fixed as W growth rate of
The isoefficiency function determines the ease with which a parallel system can maintain a constant efficiency and hence achieve
speedups increasing in proportion to the number of processing elements. A small isoefficiency function means that small increments in the
problem size are sufficient for the efficient utilization of an increasing number of processing elements, indicating that the parallel system is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
highly scalable. However, a large isoefficiency function indicates a poorly scalable parallel system. The isoefficiency function does not exist
 increases, no matter p for unscalable parallel systems, because in such systems the efficiency cannot be kept at any constant value as
how fast the problem size is increased.
Example 5.14 Isoefficiency function of adding numbers
, as given p  log p  processing elements is approximately 2 p  numbers on n The overhead function for the problem of adding
, we get Equation 5.14  in p  log p  by 2 o T . Substituting 5.1  and 5.9 by Equations
5  Equation 5.1
). This means that, if the number of p  log p ( Q Thus, the asymptotic isoefficiency function for this parallel system is
p ')/( p ' log p ) must be increased by a factor of ( n ', the problem size (in this case, p  to p processing elements is increased from
 processing elements. In other words, increasing the number of processing p ) to get the same efficiency as on p log
) to increase the speedup by a factor p  log p ')/( p ' log p  be increased by a factor of ( n  requires that p '/ p elements by a factor of
. p '/ p of
) communication overhead  numbers, the overhead due to communication (hereafter referred to as the n In the simple example of adding
 only. In general, communication overhead can depend on both the problem size and the number of processing elements. p is a function of
. In such a case, it can W  and p A typical overhead function can have several distinct terms of different orders of magnitude with respect to
. For example, consider a hypothetical p be cumbersome (or even impossible) to obtain the isoefficiency function as a closed function of
. It is 3/4 W 3/4 Kp  + 3/2 Kp  = W  can be rewritten as Equation 5.14 . For this overhead function, 3/4 W 3/4 p  + 3/2 p  = o T parallel system for which
. p  in terms of W hard to solve this equation for
 increase, the efficiency is nondecreasing as W  and p  remains fixed. As W / o T Recall that the condition for constant efficiency is that the ratio
 and compute the o T  against each term of W  has multiple terms, we balance o T . If W  grow faster than o T long as none of the terms of
 that requires the problem size to grow at the highest rate with o T respective isoefficiency functions for individual terms. The component of
 further illustrates this technique of Example 5.15  determines the overall asymptotic isoefficiency function of the parallel system. p respect to
isoefficiency analysis.
Example 5.15 Isoefficiency function of a parallel system with a complex overhead function
, we get Equation 5.14  in o T . Using only the first term of 3/4 W 3/4 p  + 3/2 p  = o T Consider a parallel system for which
6  Equation 5.1
: p  and W  yields the following relation between Equation 5.14 Using only the second term,
7  Equation 5.1
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
To ensure that the efficiency does not decrease as the number of processing elements increases, the first and second
), respectively. The asymptotically 3 p ( Q ) and 2 / 3 p ( Q terms of the overhead function require the problem size to grow as
), gives the overall asymptotic isoefficiency function of this parallel system, since it subsumes 3 p ( Q higher of the two rates,
the rate dictated by the other term. The reader may indeed verify that if the problem size is increased at this rate, the
. p (1) and that any rate lower than this causes the efficiency to fall with increasing Q efficiency is
In a single expression, the isoefficiency function captures the characteristics of a parallel algorithm as well as the parallel architecture on
which it is implemented. After performing isoefficiency analysis, we can test the performance of a parallel program on a few processing
elements and then predict its performance on a larger number of processing elements. However, the utility of isoefficiency analysis is not
 shows how the Section 5.4.5 limited to predicting the impact on performance of an increasing number of processing elements.
 that isoefficiency Chapter 13 isoefficiency function characterizes the amount of parallelism inherent in a parallel algorithm. We will see in
analysis can also be used to study the behavior of a parallel system with respect to changes in hardware parameters such as the speed of
 illustrates how isoefficiency analysis can be used even for parallel Chapter 11 processing elements and communication channels.
algorithms for which we cannot derive a value of parallel runtime.
5.4.3 Cost-Optimality and the Isoefficiency Function
, we stated that a parallel system is cost-optimal if the product of the number of processing elements and the parallel Section 5.2.5 In
execution time is proportional to the execution time of the fastest known sequential algorithm on a single processing element. In other
words, a parallel system is cost-optimal if and only if
8  Equation 5.1
, we get the following: Equation 5.10  from the right-hand side of P T Substituting the expression for
9  Equation 5.1
0  Equation 5.2
 suggest that a parallel system is cost-optimal if and only if its overhead function does not asymptotically exceed 5.20  and 5.19 Equations
 for maintaining a fixed efficiency while increasing the number Equation 5.14 the problem size. This is very similar to the condition given by
 that the Equation 5.20 ), then it follows from p (f  yields an isoefficiency function Equation 5.14 of processing elements in a parallel system. If
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
)) must be satisfied to ensure the cost-optimality of a parallel system as it is scaled up. The following example further p (f ( W  = W relation
illustrates the relationship between cost-optimality and the isoefficiency function.
Example 5.16 Relationship between cost-optimality and isoefficiency
Example  processing elements, presented in p  numbers on n Consider the cost-optimal solution to the problem of adding
); that p  log p ( Q , its isoefficiency function is Equation 5.14 ). From p  log p ( Q  = o T , and n W . For this parallel system, 5.10
 we also derived the Example 5.10 ) to maintain a constant efficiency. In p  log p ( Q is, the problem size must increase as
). p  log p ( W  = W condition for cost-optimality as
5.4.4 A Lower Bound on the Isoefficiency Function
We discussed earlier that a smaller isoefficiency function indicates higher scalability. Accordingly, an ideally-scalable parallel system must
 processing elements can be W  units of work, no more than W have the lowest possible isoefficiency function. For a problem consisting of
) as the number of p ( Q used cost-optimally; additional processing elements will be idle. If the problem size grows at a rate slower than
. Even for an ideal parallel system with W processing elements increases, then the number of processing elements will eventually exceed
 will be idle. Thus, W  = p no communication, or other overhead, the efficiency will drop because processing elements added beyond
) is the asymptotic lower p ( W ) to maintain fixed efficiency; hence, p ( Q asymptotically, the problem size must increase at least as fast as
). p ( Q bound on the isoefficiency function. It follows that the isoefficiency function of an ideally scalable parallel system is
5.4.5 The Degree of Concurrency and the Isoefficiency Function
) is imposed on the isoefficiency function of a parallel system by the number of operations that can be performed p ( W A lower bound of
degree of concurrently. The maximum number of tasks that can be executed simultaneously at any time in a parallel algorithm is called its
. The degree of concurrency is a measure of the number of operations that an algorithm can perform in parallel for a problem concurrency
) is the degree of concurrency of a parallel algorithm, then for a problem of W ( C ; it is independent of the parallel architecture. If W of size
) processing elements can be employed effectively. W ( C , no more than W size
Example 5.17 Effect of concurrency on isoefficiency function
). The total amount of Section 8.3.1  variables by using Gaussian elimination ( n  equations in n Consider solving a system of
). But then variables must be eliminated one after the other, and eliminating each variable requires 3 n ( Q computation is
) for this 3 n ( Q  = W ) processing elements can be kept busy at any time. Since 2 n ( Q ) computations. Thus, at most 2 n ( Q
) processing elements can be used efficiently. On 2/3 W ( Q ) and at most 2/3 W ( Q ) is W problem, the degree of concurrency C(
) to use them all. Thus, the 3/2 p ( W  processing elements, the problem size should be at least p the other hand, given
). 3/2 p ( Q isoefficiency function of this computation due to concurrency is
). If W ( Q )) only if the degree of concurrency of the parallel algorithm is p ( Q The isoefficiency function due to concurrency is optimal (that is,
), then the isoefficiency function due to concurrency is worse (that is, greater) W ( Q the degree of concurrency of an algorithm is less than
). In such cases, the overall isoefficiency function of a parallel system is given by the maximum of the isoefficiency functions due p ( Q than
to concurrency, communication, and other overheads.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.5 Minimum Execution Time and Minimum Cost-Optimal Execution Time
We are often interested in knowing how fast a problem can be solved, or what the minimum possible execution time of a parallel algorithm
is, provided that the number of processing elements is not a constraint. As we increase the number of processing elements for a given
problem size, either the parallel runtime continues to decrease and asymptotically approaches a minimum value, or it starts rising after
 by differentiating the W  for a given attaining a minimum value (Problem 5.12). We can determine the minimum parallel runtime
) is differentiable with respect to p , W  ( P T  and equating the derivative to zero (assuming that the function p  with respect to P T expression for
 is minimum is determined by the following equation: P T ). The number of processing elements for which p
1  Equation 5.2
 can be determined by . The value of Equation 5.21  be the value of the number of processing elements that satisfies 0 p Let
n  for the problem of adding  . In the following example, we derive the expression for P T  in the expression for p  for 0 p substituting
numbers.
Example 5.18 Minimum execution time for adding n numbers
 processing p  numbers on n , the parallel run time for the problem of adding Example 5.12 Under the assumptions of
elements can be approximated by
2  Equation 5.2
 as p  to zero we get the solutions for Equation 5.22  of the right-hand side of p Equating the derivative with respect to
follows:
3  Equation 5.2
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, we get Equation 5.22 /2 in n  = p Substituting
4  Equation 5.2
) serial complexity of the problem. Hence, n ( Q ), which is higher than the n  log n ( Q  is 0 p  = p , the processor-time product for Example 5.18 In
 that yields minimum parallel runtime. We now derive an important result that gives p the parallel system is not cost-optimal for the value of
a lower bound on parallel runtime if the problem is solved cost-optimally.
 be the minimum time in which a problem can be solved by a cost-optimal parallel system. From the discussion Let
, we conclude that if the isoefficiency function of Section 5.4.3 regarding the equivalence of cost-optimality and the isoefficiency function in
)). In other words, given a problem p (f ( W  = W  can be solved cost-optimally if and only if W )), then a problem of size p (f ( Q a parallel system is
) for a cost-optimal parallel system p/ W ( Q )). Since the parallel runtime is W ( -1f ( O  = p , a cost-optimal solution requires that W of size
 cost-optimally is W ), the lower bound on the parallel runtime for solving a problem of size Equation 5.18 (
5  Equation 5.2
Example 5.19 Minimum cost-optimal execution time for adding n numbers
, then p  log p ) = p (f  = n  = W ). If p  log p ( Q ) of this parallel system is p (f , the isoefficiency function Example 5.14 As derived in
/log n ) = n ( -1f  = p , then p  log p ) = p  (f  = n . If p  log n . Ignoring the double logarithmic term, log p  + log log p  = log n log
). As a consequence of the relation between cost-optimality and the isoefficiency n /log n ( Q ) = W ( -1f . Hence, n /log n p
/log n ( Q function, the maximum number of processing elements that can be used to solve this problem cost-optimally is
, we get Equation 5.2  in n /log n  = p ). Using n
6  Equation 5.2
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). Thus, for this 5.26  and 5.24 ) (Equations n (log Q  numbers are n  for adding  and It is interesting to observe that both
problem, a cost-optimal solution is also the asymptotically fastest solution. The parallel execution time cannot be reduced asymptotically by
 greater than that suggested by the isoefficiency function for a given problem size (due to the equivalence between p using a value of
cost-optimality and the isoefficiency function). This is not true for parallel systems in general, however, and it is quite possible that
. The following example illustrates such a parallel system.
Example 5.20 A parallel system with
, for which Example 5.15 Consider the hypothetical parallel system of
7  Equation 5.2
, the parallel runtime for this system is Equation 5.10 From
8  Equation 5.2
, Example 5.18 Using the methodology of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, we get Equation 5.28  in 0 p  by the value of p ). Substituting W ( Q  = 0 p From the preceding analysis,
9  Equation 5.2
), which implies that the 3 p ( Q , the overall isoefficiency function for this parallel system is Example 5.15 According to
) in 1/3 W ( Q  = p ). Substituting 1/3 W ( Q maximum number of processing elements that can be used cost-optimally is
, we get Equation 5.28
0  Equation 5.3
.  is asymptotically greater than  shows that 5.30  and 5.29 A comparison of Equations
 is asymptotically equal to In this section, we have seen examples of both types of parallel systems: those for which
. Most parallel systems presented in this book are of  is asymptotically greater than , and those for which
the first type. Parallel systems for which the runtime can be reduced by an order of magnitude by using an asymptotically higher number of
processing elements than indicated by the isoefficiency function are rare.
While deriving the minimum execution time for any parallel system, it is important to be aware that the maximum number of processing
 is greater 0 p ) of the parallel algorithm. It is quite possible that W ( C elements that can be utilized is bounded by the degree of concurrency
 is given by  is meaningless, and 0 p ) for a parallel system (Problems 5.13 and 5.14). In such cases, the value of W ( C than
1  Equation 5.3
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.6 Asymptotic Analysis of Parallel Programs
At this point, we have accumulated an arsenal of powerful tools for quantifying the performance and scalability of an algorithm. Let us
illustrate the use of these tools for evaluating a set of parallel programs for solving a given problem. Often, we ignore constants and
concern ourselves with the asymptotic behavior of quantities. In many cases, this can yield a clearer picture of relative merits and
demerits of various parallel programs.
Table 5.2. Comparison of four different algorithms for sorting a given list of numbers. The table
P pT shows number of processing elements, parallel runtime, speedup, efficiency and the
product.
A4 A3 A2 A1 Algorithm
p
n
n n log 2
n 1 P T
n log n  log n S
1 1 E
P pT
n
n  log n 2
n
n  log n 1.5
). Let us look at four n  log n  ( O  numbers. The fastest serial programs for this problem run in time n Consider the problem of sorting a list of
different parallel algorithms A1, A2, A3, and A4, for sorting a given list. The parallel runtime of the four algorithms along with the number
. The objective of this exercise is to determine which of these four algorithms is Table 5.2 of processing elements they can use is given in
 is the best. By this metric, algorithm A1 is the P T the best. Perhaps the simplest metric is one of speed; the algorithm with the lowest
best, followed by A3, A4, and A2. This is also reflected in the fact that the speedups of the set of algorithms are also in this order.
 processing elements as are required by algorithm A1. Furthermore, resource 2 n However, in practical situations, we will rarely have
utilization is an important aspect of practical program design. So let us look at how efficient each of these algorithms are. This metric of
evaluating the algorithm presents a starkly different image. Algorithms A2 and A4 are the best, followed by A3 and A1. The last row of
 presents the cost of the four algorithms. From this row, it is evident that the costs of algorithms A1 and A3 are higher than the Table 5.2
 and therefore neither of these algorithms is cost optimal. However, algorithms A2 and A4 are cost optimal. n  log n serial runtime of
This set of algorithms illustrate that it is important to first understand the objectives of parallel algorithm analysis and to use appropriate
metrics. This is because use of different metrics may often result in contradictory outcomes.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.7 Other Scalability Metrics
A number of other metrics of scalability of parallel systems have been proposed. These metrics are specifically suited to different system
requirements. For example, in real time applications, the objective is to scale up a system to accomplish a task in a specified time bound.
One such application is multimedia decompression, where MPEG streams must be decompressed at the rate of 25 frames/second.
Consequently, a parallel system must decode a single frame in 40 ms (or with buffering, at an average of 1 frame in 40 ms over the
buffered frames). Other such applications arise in real-time control, where a control vector must be generated in real-time. Several
scalability metrics consider constraints on physical architectures. In many applications, the maximum size of a problem is constrained not
by time, efficiency, or underlying models, but by the memory available on the machine. In such cases, metrics make assumptions on the
growth function of available memory (with number of processing elements) and estimate how the performance of the parallel system
changes with such scaling. In this section, we examine some of the related metrics and how they can be used in various parallel
applications.
f  This metric is defined as the speedup obtained when the problem size is increased linearly with the number o Scaled Speedup
processing elements. If the scaled-speedup curve is close to linear with respect to the number of processing elements, then the parallel
system is considered scalable. This metric is related to isoefficiency if the parallel algorithm under consideration has linear or near-linear
isoefficiency function. In this case the scaled-speedup metric provides results very close to those of isoefficiency analysis, and the
scaled-speedup is linear or near-linear with respect to the number of processing elements. For parallel systems with much worse
isoefficiencies, the results provided by the two metrics may be quite different. In this case, the scaled-speedup versus number of
processing elements curve is sublinear.
Two generalized notions of scaled speedup have been examined. They differ in the methods by which the problem size is scaled up with
the number of processing elements. In one method, the size of the problem is increased to fill the available memory on the parallel
computer. The assumption here is that aggregate memory of the system increases with the number of processing elements. In the other
 subject to an upper-bound on execution time. p method, the size of the problem grows with
Example 5.21 Memory and time-constrained scaled speedup for matrix-vector
products
 is the time for a single ct , where 2 n ct  with a vector is n  x n The serial runtime of multiplying a matrix of dimension
multiply-add operation. The corresponding parallel runtime using a simple parallel algorithm is given by:
 is given by: S and the speedup
2  Equation 5.3
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). Let us consider the two cases of problem scaling. In the case 2 n ( Q The total memory requirement of the algorithm is
of memory constrained scaling, we assume that the memory of the parallel system grows linearly with the number of
 = m ). This is a reasonable assumption for most current parallel platforms. Since p ( Q  = m processing elements, i.e.,
' is given by: S  Therefore, the scaled speedup. , for some constant c p  x c  = 2 n ), we have 2 n ( Q
or
. In the limiting case,
). We p ( O  = 2 n ). Since this is constrained to be constant, p/ 2 n ( O  = P T In the case of time constrained scaling, we have
notice that this case is identical to the memory constrained case. This happened because the memory and runtime of
the algorithm are asymptotically identical.
Example 5.22 Memory and time-constrained scaled speedup for matrix-matrix
products
, as before, is the time for a single ct , where 3 n ct  is n  x n The serial runtime of multiplying two matrices of dimension
multiply-add operation. The corresponding parallel runtime using a simple parallel algorithm is given by:
 is given by: S and the speedup
3  Equation 5.3
). Let us consider the two cases of problem scaling. In the case 2 n ( Q The total memory requirement of the algorithm is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
of memory constrained scaling, as before, we assume that the memory of the parallel system grows linearly with the
. Therefore, the c , for some constant p  x c  = 2 n ), we have 2 n ( Q  = m ). Since p ( Q  = m number of processing elements, i.e.,
' is given by: S scaled speedup
3 n ), or p ( O  = 3 n ). Since this is constrained to be constant, p/ 3 n ( O  = P T In the case of time constrained scaling, we have
). c  (for some constant p  x c =
" is given by: S Therefore, the time-constrained speedup
This example illustrates that memory-constrained scaling yields linear speedup, whereas time-constrained speedup
yields sublinear speedup in the case of matrix multiplication.
 can be used to quantify the performance of a parallel system on af  The experimentally determined serial fractionf Serial Fraction
fixed-size problem. Consider a case when the serial runtime of a computation can be divided into a totally parallel and a totally serial
component, i.e.,
 correspond to totally serial and totally parallel components. From this, we can write: par T  and ser T Here,
Here, we have assumed that all of the other parallel overheads such as excess computation and communication are captured in the
. From these equations, it follows that: ser T serial component
4  Equation 5.3
 of a parallel program is defined as:f The serial fraction
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, we have: Equation 5.34 Therefore, from
 , we have P T / W  = S Since
 , we get:f Solving for
5  Equation 5.3
 increases with the number of processingf  are better since they result in higher efficiencies. Iff It is easy to see that smaller values of
elements, then it is considered as an indicator of rising communication overhead, and thus an indicator of poor scalability.
Example 5.23 Serial component of the matrix-vector product
, we have 5.32  and 5.35 From Equations
6  Equation 5.3
Simplifying the above expression, we get
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
It is useful to note that the denominator of this equation is the serial runtime of the algorithm and the numerator
corresponds to the overhead in parallel execution.
In addition to these metrics, a number of other metrics of performance have been proposed in the literature. We refer interested readers
to the bibliography for references to these.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
5.8 Bibliographic Remarks
To use today's massively parallel computers effectively, larger problems must be solved as more processing elements are added.
However, when the problem size is fixed, the objective is to attain the best compromise between efficiency and parallel runtime.
]. In Wor90 , TL90 , NW88 , KF90 , GK93a , FK89 Performance issues for fixed-size problems have been addressed by several researchers [
most situations, additional computing power derived from increasing the number of processing elements can be used to solve bigger
problems. In some situations, however, different ways of increasing the problem size may apply, and a variety of constraints may guide
]. Time-constrained scaling and SHG93 the scaling up of the workload with respect to the number of processing elements [
], and Worley SN93 , SN90 ], Sun and Ni [ Gus92 , Gus88 , GMB88  [ et al. memory-constrained scaling have been explored by Gustafson
] (Problem 5.9). Wor91 , Wor88 , Wor90[
An important scenario is one in which we want to make the most efficient use of the parallel system; in other words, we want the overall
. This is possible only for scalable parallel systems, which are exactly those p performance of the parallel system to increase linearly with
 by simply increasing the problem size. For such systems, it is natural p for which a fixed efficiency can be maintained for arbitrarily large
]. Isoefficiency analysis has been found to be very KRS88 , KR87b , CD87 , GGK93 to use the isoefficiency function or related metrics [
, KS91b , KR89 , KR87b , KN91 , HX98 , GKS92 , GK93b , GK91 useful in characterizing the scalability of a variety of parallel algorithms [
] have demonstrated the relevance of the isoefficiency function KG94 , GK93a ]. Gupta and Kumar [ WS91 , WS89 , TL90 , SKAT91b , RS90b
), then the problem size cannot be p ( Q in the fixed time case as well. They have shown that if the isoefficiency function is greater than
increased indefinitely while maintaining a fixed execution time, no matter how many processing elements are used. A number of other
, TL90 , NW88 , MS88 , FK89 , EZL89 researchers have analyzed the performance of parallel systems with concern for overall efficiency [
]. ZRV89 , Zho89
 problems. Their definition is related to the concept of parallel efficient (PE) ] define the concept of KRS88 Kruskal, Rudolph, and Snir [
isoefficiency function. Problems in the class PE have algorithms with a polynomial isoefficiency function at some efficiency. The class PE
makes an important distinction between algorithms with polynomial isoefficiency functions and those with worse isoefficiency functions.
 proved the invariance of the class PE over a variety of parallel computational models and interconnection schemes. An et al. Kruskal
important consequence of this result is that an algorithm with a polynomial isoefficiency on one architecture will have a polynomial
] show that GK93b isoefficiency on many other architectures as well. There can be exceptions, however; for instance, Gupta and Kumar [
the fast Fourier transform algorithm has a polynomial isoefficiency on a hypercube but an exponential isoefficiency on a mesh.
. PC* includes problems with efficient parallel algorithms on a PRAM. A PC* ] define a class of problems called VS86 Vitter and Simons [
 (the polynomial-time class) is in PC* if it has a parallel algorithm on a PRAM that can use a polynomial (in terms of P problem in class
. Any problem in PC* has at least one parallel algorithm input size) number of processing elements and achieve a minimal efficiency
, its isoefficiency function exists and is a polynomial. such that, for an efficiency
]. Besides the ones KG94 A discussion of various scalability and performance measures can be found in the survey by Kumar and Gupta [
, CR91 , CR89 , BW89 cited so far, a number of other metrics of performance and scalability of parallel systems have been proposed [
]. VC89 , SZ96 , SR91 , SG91 , NA91 , MR , Mol87 , Kun86 , Hil90 , Fla90
] show that if the overhead function satisfies certain mathematical properties, then there exists a unique Fla90 , FK89 Flatt and Kennedy [
 on which their analysis depends o T . A property of W  is minimum for a given P T  of the number of processing elements for which 0 p value
] show that there exist parallel systems that do not obey this condition, and in such GK93a ). Gupta and Kumar [ p ( Q  > o T heavily is that
cases the point of peak performance is determined by the degree of concurrency of the algorithm being used.
] develop a model to describe and analyze a parallel computation on an MIMD computer in terms of the number MR Marinescu and Rice [
. They consider the case p ) as a function of p ( g  into which the computation is divided and the number of events p of threads of control
, they conclude that with increasing o T ). Under these assumptions on p ( g q  = o T  and hence q where each event is of a fixed duration
 = o T ), and it asymptotically approaches zero if p ( Q  = o T number of processing elements, the speedup saturates at some value if
] generalize these results for a wider class of overhead functions. They show that the GK93a  2. Gupta and Kumar [ m ),where m p ( Q
), and the speedup attains a maximum value and then drops monotonically p ( Q o T speedup saturates at some maximum value if
). p ( Q  > o T  if p with
] have proposed a criterion of optimality of a parallel system so that a balance is struck TL90 ] and Tang and Li [ EZL89  [ et al. Eager
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
between efficiency and speedup. They propose that a good choice of operating point on the execution time versus efficiency curve is that
 per processing element or, in other words, efficiency is 0.5. where the incremental benefit of adding processing elements is roughly
 is minimum. 2 ) P T ( p  product is maximum or ES ), this is also equivalent to operating at a point where the p ( Q  = o T They conclude that for
]. GK93a This conclusion is a special case of the more general case presented by Gupta and Kumar [
] address the important problem of PD89 ], and Park and Dowdy [ MS88 ], Ma and Shea [ LDP89  [ et al. ], Leuze BB90 Belkhale and Banerjee [
optimal partitioning of the processing elements of a parallel computer among several applications of different scalabilities executing
simultaneously.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
 is an upper bound on S W / W , prove that S W  has a serial component W  If a problem of size ]) Amd67 5.1 (Amdahl's law [
its speedup, no matter how many processing elements are used.
, in which the dark node represents the Figure 5.10(a)  Consider the search tree shown in 5.2 (Superlinear speedup)
solution.
Figure 5.10. Superlinear(?) speedup in parallel depth first search.
Section If a sequential search of the tree is performed using the standard depth-first search (DFS) algorithm (
), how much time does it take to find the solution if traversing each arc of the tree takes one unit of time? 11.2.1
. a
Assume that the tree is partitioned between two processing elements that are assigned to do the search job,
. If both processing elements perform a DFS on their respective halves of the tree, Figure 5.10(b) as shown in
how much time does it take for the solution to be found? What is the speedup? Is there a speedup anomaly?
If so, can you explain the anomaly?
. b
 Parallel algorithms can often be represented by dependency graphs. 5.3 (The DAG model of parallel computation)
. If a program can be broken into several tasks, then each Figure 5.11 Four such dependency graphs are shown in
node of the graph represents one task. The directed edges of the graph represent the dependencies between the tasks
or the order in which they must be performed to yield correct results. A node of the dependency graph can be
scheduled for execution as soon as the tasks at all the nodes that have incoming edges to that node have finished
, the nodes on the second level from the root can begin execution only after Figure 5.11(b) execution. For example, in
 (DAG); that is, directed acyclic graph the task at the root is finished. Any deadlock-free dependency graph must be a
it is devoid of any cycles. All the nodes that are scheduled for execution can be worked on in parallel provided enough
 - 1 for graphs n  = 2 N  is an integer, then n  is the number of nodes in a graph, and N processing elements are available. If
 = 4 and graphs (c) n  + 1)/2 for graph (d) (graphs (a) and (b) are drawn for n ( n  = N  for graph (c), and 2 n  = N (a) and (b),
 = 8). Assuming that each task takes one unit of time and that interprocessor communication n and (d) are drawn for
time is zero, for the algorithms represented by each of these graphs:
Compute the degree of concurrency. . 1
Compute the maximum possible speedup if an unlimited number of processing elements is available. . 2
Compute the values of speedup, efficiency, and the overhead function if the number of processing elements
is (i) the same as the degree of concurrency and (ii) equal to half of the degree of concurrency.
. 3
Figure 5.11. Dependency graphs for Problem 5.3.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 units of work. Prove W  processing elements solving a problem consisting of p  Consider a parallel system containing 5.4
), then the problem cannot be solved p ( Q that if the isoefficiency function of the system is worse (greater) than
), W ( Q  < p ). Also prove the converse that if the problem can be solved cost-optimally only for W  = ( p cost-optimally with
then the isoefficiency function of the parallel system is worse than linear.
 is defined as the speedup obtained when the problem size is increased Scaled speedup 5.5 (Scaled speedup)
 is chosen as a base problem size for a single processing W linearly with the number of processing elements; that is, if
element, then
7  Equation 5.3
), plot the speedup curves, assuming that Example 5.1  processing elements ( p  numbers on n For the problem of adding
 = 1, 4, 16, 64, and 256. Assume that it takes 10 time p  = 1 is that of adding 256 numbers. Use p the base problem for
units to communicate a number between two processing elements, and that it takes one unit of time to add two
numbers. Now plot the standard speedup curve for the base problem size and compare it with the scaled speedup
curve.
. p  - 1) + 11 log p/ n  The parallel runtime is ( Hint:
 Plot a third speedup curve for Problem 5.5, in which the problem size is scaled up according to the isoefficiency 5.6
 . P T ). Use the same expression for p  log p ( Q function, which is
 The scaled speedup under this method of scaling is given by the following equation: Hint:
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processing elements corresponding to the p  numbers on n  Plot the efficiency curves for the problem of adding 5.7
standard speedup curve (Problem 5.5), the scaled speedup curve (Problem 5.5), and the speedup curve when the
problem size is increased according to the isoefficiency function (Problem 5.6).
 A drawback of increasing the number of processing elements without increasing the total workload is that the 5.8
speedup does not increase linearly with the number of processing elements, and the efficiency drops monotonically.
Based on your experience with Problems 5.5 and 5.7, discuss whether or not scaled speedup increases linearly with
the number of processing elements in general. What can you say about the isoefficiency function of a parallel system
whose scaled speedup curve matches the speedup curve determined by increasing the problem size according to the
isoefficiency function?
 = 1, 4, 16, 64, 256, 1024, and 4096, p  from Problem 5.5 for P T  Using the expression for 5.9 (Time-constrained scaling)
what is the largest problem that can be solved if the total execution time is not to exceed 512 time units? In general, is
it possible to solve an arbitrarily large problem in a fixed amount of time, provided that an unlimited number of
processing elements is available? Why?
 processing n  numbers on n ) of Example 5.1  Consider the problem of computing the prefix sums ( 5.10 (Prefix sums)
elements. What is the parallel runtime, speedup, and efficiency of this algorithm? Assume that adding two numbers
takes one unit of time and that communicating one number between two processing elements takes 10 units of time. Is
the algorithm cost-optimal?
n  Design a cost-optimal version of the prefix sums algorithm (Problem 5.10) for computing all prefix-sums of 5.11
. Assuming that adding two numbers takes one unit of time and that n  < p  processing elements where p numbers on
 , E , S  , P T communicating one number between two processing elements takes 10 units of time, derive expressions for
cost, and the isoefficiency function.
) for a given problem size, then the parallel execution time will continue to p  ( o T  Prove that if 5.12 [GK93a]
 first TP ), then p ( Q  > o T  is increased and will asymptotically approach a constant value. Also prove that if p decrease as
; hence, it has a distinct minimum. p decreases and then increases with
 = P T  processing elements is given by p  The parallel runtime of a parallel implementation of the FFT algorithm with 5.13
 = 0). The maximum number of st  with Equation 13.4  ( n  for an input sequence of length p ) log p/ n ( wt  + n ) log p/ n (
 that p  (the value of 0 p . What are the values of n -point FFT is n processing elements that the algorithm can use for an
 = 10? wt  for ) and Equation 5.21 satisfies
 Consider two parallel systems with the same overhead function, but with different degrees of ] GK93a 5.14 [
 curve for p  versus P T . Plot the p 2/3 W  + 0.1 3/2 p 1/3 W concurrency. Let the overhead function of both parallel systems be
 for the second 2/3 W  for the first algorithm and 1/3 W  2048. If the degree of concurrency is p , and 1 6  = 10 W
 for both parallel systems. Also compute the cost and efficiency for both the algorithm, compute the values of
 curve where their respective minimum runtimes are achieved. p  versus P T parallel systems at the point on the
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 6. Programming Using the
Message-Passing Paradigm
Numerous programming languages and libraries have been developed for explicit parallel programming. These differ in their view of the
address space that they make available to the programmer, the degree of synchronization imposed on concurrent activities, and the
 is one of the oldest and most widely used approaches for message-passing programming paradigm multiplicity of programs. The
programming parallel computers. Its roots can be traced back in the early days of parallel processing and its wide-spread adoption can
be attributed to the fact that it imposes minimal requirements on the underlying hardware.
In this chapter, we first describe some of the basic concepts of the message-passing programming paradigm and then explore various
message-passing programming techniques using the standard and widely-used Message Passing Interface.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
6.1 Principles of Message-Passing Programming
There are two key attributes that characterize the message-passing programming paradigm. The first is that it assumes a partitioned
address space and the second is that it supports only explicit parallelization.
s  processes, each with its own exclusive addres p The logical view of a machine supporting the message-passing paradigm consists of
e space. Instances of such a view come naturally from clustered workstations and non-shared address space multicomputers. There ar
; two immediate implications of a partitioned address space. First, each data element must belong to one of the partitions of the space
s hence, data must be explicitly partitioned and placed. This adds complexity to programming, but encourages locality of access that i
l critical for achieving high performance on non-UMA architecture, since a processor can access its local data much faster than non-loca
s data on such architectures. The second implication is that all interactions (read-only or read/write) require cooperation of two processe
f – the process that has the data and the process that wants to access the data. This requirement for cooperation adds a great deal o
n complexity for a number of reasons. The process that has the data must participate in the interaction even if it has no logical connectio
c to the events at the requesting process. In certain circumstances, this requirement leads to unnatural programs. In particular, for dynami
a and/or unstructured interactions the complexity of the code written for this type of paradigm can be very high for this reason. However,
s primary advantage of explicit two-way interactions is that the programmer is fully aware of all the costs of non-local interactions, and i
g more likely to think about algorithms (and mappings) that minimize interactions. Another major advantage of this type of programmin
. paradigm is that it can be efficiently implemented on a wide variety of architectures
The message-passing programming paradigm requires that the parallelism is coded explicitly by the programmer. That is, the
programmer is responsible for analyzing the underlying serial algorithm/application and identifying ways by which he or she can
decompose the computations and extract concurrency. As a result, programming using the message-passing paradigm tends to be hard
and intellectually demanding. However, on the other hand, properly written message-passing programs can often achieve very high
performance and scale to a very large number of processes.
loosely  or asynchronous  Message-passing programs are often written using the Structure of Message-Passing Programs
 paradigms. In the asynchronous paradigm, all concurrent tasks execute asynchronously. This makes it possible to synchronous
implement any parallel algorithm. However, such programs can be harder to reason about, and can have non-deterministic behavior due
to race conditions. Loosely synchronous programs are a good compromise between these two extremes. In such programs, tasks or
subsets of tasks synchronize to perform interactions. However, between these interactions, tasks execute completely asynchronously.
Since the interaction happens synchronously, it is still quite easy to reason about the program. Many of the known parallel algorithms
can be naturally implemented using loosely synchronous programs.
 processes. This p In its most general form, the message-passing paradigm supports execution of a different program on each of the
provides the ultimate flexibility in parallel programming, but makes the job of writing parallel programs effectively unscalable. For this
 (SPMD) approach. In SPMD programs single program multiple data reason, most message-passing programs are written using the
the code executed by different processes is identical except for a small number of processes (e.g., the "root" process). This does not
mean that the processes work in lock-step. In an extreme case, even in an SPMD program, each process could execute a different code
(the program contains a large case statement with code for each process). But except for this degenerate case, most processes execute
the same code. SPMD programs can be loosely synchronous or completely asynchronous.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
6.2 The Building Blocks: Send and Receive Operations
Since interactions are accomplished by sending and receiving messages, the basic operations in the message-passing programming
. In their simplest form, the prototypes of these operations are defined as follows: receive  and send paradigm are
send(void *sendbuf, int nelems, int dest)
receive(void *recvbuf, int nelems, int source)
 is the nelems  points to a buffer that stores the data to be received, recvbuf  points to a buffer that stores the data to be sent, sendbuf The
 is the identifier of the source  is the identifier of the process that receives the data, and dest number of data units to be sent and received,
process that sends the data.
However, to stop at this point would be grossly simplifying the programming and performance ramifications of how these functions are
implemented. To motivate the need for further investigation, let us start with a simple example of a process sending a piece of data to
another process as illustrated in the following code-fragment:
1 P0 P1
2
3 a = 100; receive(&a, 1, 0)
4 send(&a, 1, 1); printf("%d\n", a);
5 a=0;
In this simple example, process P0 sends a message to process P1 which receives and prints the message. The important thing to note is
. The semantics of the send operation require that the value send  to 0 immediately following the a that process P0 changes the value of
 at the time of the send operation must be the value that is a received by process P1 must be 100 as opposed to 0. That is, the value of
. P1 received by process
It may seem that it is quite straightforward to ensure the semantics of the send and receive operations. However, based on how the send
and receive operations are implemented this may not be the case. Most message passing platforms have additional hardware support for
sending and receiving messages. They may support DMA (direct memory access) and asynchronous message transfer using network
interface hardware. Network interfaces allow the transfer of messages from buffer memory to desired location without CPU intervention.
Similarly, DMA allows copying of data from one memory location to another (e.g., communication buffers) without CPU support (once they
have been programmed). As a result, if the send operation programs the communication hardware and returns before the communication
 instead of 100! a  might receive the value 0 in P1 operation has been accomplished, process
While this is undesirable, there are in fact reasons for supporting such send operations for performance reasons. In the rest of this section,
we will discuss send and receive operations in the context of such a hardware environment, and motivate various implementation details
and message-passing protocols that help in ensuring the semantics of the send and receive operations.
6.2.1 Blocking Message Passing Operations
A simple solution to the dilemma presented in the code fragment above is for the send operation to return only when it is semantically safe
to do so. Note that this is not the same as saying that the send operation returns only after the receiver has received the data. It simply
means that the sending operation blocks until it can guarantee that the semantics will not be violated on return irrespective of what
happens in the program subsequently. There are two mechanisms by which this can be achieved.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Blocking Non-Buffered Send/Receive
In the first case, the send operation does not return until the matching receive has been encountered at the receiving process. When this
happens, the message is sent and the send operation returns upon completion of the communication operation. Typically, this process
involves a handshake between the sending and receiving processes. The sending process sends a request to communicate to the
receiving process. When the receiving process encounters the target receive, it responds to the request. The sending process upon
. Since there are no buffers used at either Figure 6.1 receiving this response initiates a transfer operation. The operation is illustrated in
. non-buffered blocking operation sending or receiving ends, this is also referred to as a
Figure 6.1. Handshake for a blocking non-buffered send/receive operation. It is easy to see that
in cases where sender and receiver do not reach communication point at similar times, there
can be considerable idling overheads.
, we illustrate three scenarios in which the send is reached before Figure 6.1  In Idling Overheads in Blocking Non-Buffered Operations
the receive is posted, the send and receive are posted around the same time, and the receive is posted before the send is reached. In
cases (a) and (c), we notice that there is considerable idling at the sending and receiving process. It is also clear from the figures that a
blocking non-buffered protocol is suitable when the send and receive are posted at roughly the same time. However, in an asynchronous
environment, this may be impossible to predict. This idling overhead is one of the major drawbacks of this protocol.
:  Consider the following simple exchange of messages that can lead to a deadlock Deadlocks in Blocking Non-Buffered Operations
1 P0 P1
2
3 send(&a, 1, 1); send(&a, 1, 0);
4 receive(&b, 1, 1); receive(&b, 1, 0);
 available to both processes P0 and P1. However, if the send and receive operations are a The code fragment makes the values of
implemented using a blocking non-buffered protocol, the send at P0 waits for the matching receive at P1 whereas the send at process P1
waits for the corresponding receive at P0, resulting in an infinite wait.
As can be inferred, deadlocks are very easy in blocking protocols and care must be taken to break cyclic waits of the nature outlined. In
 as send  and a receive the above example, this can be corrected by replacing the operation sequence of one of the processes by a
opposed to the other way around. This often makes the code more cumbersome and buggy.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Blocking Buffered Send/Receive
A simple solution to the idling and deadlocking problem outlined above is to rely on buffers at the sending and receiving ends. We start
with a simple case in which the sender has a buffer pre-allocated for communicating messages. On encountering a send operation, the
sender simply copies the data into the designated buffer and returns after the copy operation has been completed. The sender process
can now continue with the program knowing that any changes to the data will not impact program semantics. The actual communication
can be accomplished in many ways depending on the available hardware resources. If the hardware supports asynchronous
communication (independent of the CPU), then a network transfer can be initiated after the message has been copied into the buffer. Note
that at the receiving end, the data cannot be stored directly at the target location since this would violate program semantics. Instead, the
data is copied into a buffer at the receiver as well. When the receiving process encounters a receive operation, it checks to see if the
. Figure 6.2(a) message is available in its receive buffer. If so, the data is copied into the target location. This operation is illustrated in
Figure 6.2. Blocking buffered transfer protocols: (a) in the presence of communication
hardware with buffers at send and receive ends; and (b) in the absence of communication
hardware, sender interrupts receiver and deposits data in buffer at receiver end.
In the protocol illustrated above, buffers are used at both sender and receiver and communication is handled by dedicated hardware.
Sometimes machines do not have such communication hardware. In this case, some of the overhead can be saved by buffering only on
one side. For example, on encountering a send operation, the sender interrupts the receiver, both processes participate in a
communication operation and the message is deposited in a buffer at the receiver end. When the receiver eventually encounters a receive
. It is not difficult to Figure 6.2(b) operation, the message is copied from the buffer into the target location. This protocol is illustrated in
conceive a protocol in which the buffering is done only at the sender and the receiver initiates a transfer by interrupting the sender.
It is easy to see that buffered protocols alleviate idling overheads at the cost of adding buffer management overheads. In general, if the
parallel program is highly synchronous (i.e., sends and receives are posted around the same time), non-buffered sends may perform
better than buffered sends. However, in general applications, this is not the case and buffered sends are desirable unless buffer capacity
becomes an issue.
Example 6.1 Impact of finite buffers in message passing
Consider the following code fragment:
1 P0 P1
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
2
3 for (i = 0; i < 1000; i++) { for (i = 0; i < 1000; i++) {
4 produce_data(&a); receive(&a, 1, 0);
5 send(&a, 1, 1); consume_data(&a);
6 } }
In this code fragment, process P0 produces 1000 data items and process P1 consumes them. However, if process P1
was slow getting to this loop, process P0 might have sent all of its data. If there is enough buffer space, then both
processes can proceed; however, if the buffer is not sufficient (i.e., buffer overflow), the sender would have to be
blocked until some of the corresponding receive operations had been posted, thus freeing up buffer space. This can
often lead to unforeseen overheads and performance degradation. In general, it is a good idea to write programs that
have bounded buffer requirements.
o  While buffering alleviates many of the deadlock situations, it is still possible t Deadlocks in Buffered Send and Receive Operations
write code that deadlocks. This is due to the fact that as in the non-buffered case, receive calls are always blocking (to ensure semantic
consistency). Thus, a simple code fragment such as the following deadlocks since both processes wait to receive data but nobody sends
it.
1 P0 P1
2
3 receive(&a, 1, 1); receive(&a, 1, 0);
4 send(&b, 1, 1); send(&b, 1, 0);
Once again, such circular waits have to be broken. However, deadlocks are caused only by waits on receive operations in this case.
6.2.2 Non-Blocking Message Passing Operations
In blocking protocols, the overhead of guaranteeing semantic correctness was paid in the form of idling (non-buffered) or buffer
management (buffered). Often, it is possible to require the programmer to ensure semantic correctness and provide a fast send/receive
operation that incurs little overhead. This class of non-blocking protocols returns from the send or receive operation before it is
semantically safe to do so. Consequently, the user must be careful not to alter data that may be potentially participating in a
 operation, which indicates whether the check-status communication operation. Non-blocking operations are generally accompanied by a
semantics of a previously initiated transfer may be violated or not. Upon return from a non-blocking send or receive operation, the process
is free to perform any computation that does not depend upon the completion of the operation. Later in the program, the process can
check whether or not the non-blocking operation has completed, and, if necessary, wait for its completion.
, non-blocking operations can themselves be buffered or non-buffered. In the non-buffered case, a process Figure 6.3 As illustrated in
wishing to send data to another simply posts a pending message and returns to the user program. The program can then do other useful
work. At some point in the future, when the corresponding receive is posted, the communication operation is initiated. When this operation
Figure is completed, the check-status operation indicates that it is safe for the programmer to touch this data. This transfer is indicated in
. 6.4(a)
Figure 6.3. Space of possible protocols for send and receive operations.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 6.4. Non-blocking non-buffered send and receive operations (a) in absence of
communication hardware; (b) in presence of communication hardware.
, it is easy to see that the idling time when the process is waiting for the corresponding receive in a 6.1(a)  and 6.4(a) Comparing Figures
blocking operation can now be utilized for computation, provided it does not update the data being sent. This alleviates the major
bottleneck associated with the former at the expense of some program restructuring. The benefits of non-blocking operations are further
. In this case, the communication Figure 6.4(b) enhanced by the presence of dedicated communication hardware. This is illustrated in
overhead can be almost entirely masked by non-blocking operations. In this case, however, the data being received is unsafe for the
duration of the receive operation.
Non-blocking operations can also be used with a buffered protocol. In this case, the sender initiates a DMA operation and returns
immediately. The data becomes safe the moment the DMA operation has been completed. At the receiving end, the receive operation
initiates a transfer from the sender's buffer to the receiver's target location. Using buffers with non-blocking operation has the effect of
reducing the time during which the data is unsafe.
Typical message-passing libraries such as Message Passing Interface (MPI) and Parallel Virtual Machine (PVM) implement both blocking
and non-blocking operations. Blocking operations facilitate safe and easier programming and non-blocking operations are useful for
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
performance optimization by masking communication overhead. One must, however, be careful using non-blocking protocols since errors
can result from unsafe access to data that is in the process of being communicated.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
6.3 MPI: the Message Passing Interface
Many early generation commercial parallel computers were based on the message-passing architecture due to its lower cost relative to
shared-address-space architectures. Since message-passing is the natural programming paradigm for these machines, this resulted in
the development of many different message-passing libraries. In fact, message-passing became the modern-age form of assembly
language, in which every hardware vendor provided its own library, that performed very well on its own hardware, but was incompatible
with the parallel computers offered by other vendors. Many of the differences between the various vendor-specific message-passing
libraries were only syntactic; however, often enough there were some serious semantic differences that required significant
re-engineering to port a message-passing program from one library to another.
The message-passing interface, or MPI as it is commonly known, was created to essentially solve this problem. MPI defines a standard
library for message-passing that can be used to develop portable message-passing programs using either C or Fortran. The MPI
standard defines both the syntax as well as the semantics of a core set of library routines that are very useful in writing
message-passing programs. MPI was developed by a group of researchers from academia and industry, and has enjoyed wide support
by almost all the hardware vendors. Vendor implementations of MPI are available on almost all commercial parallel computers.
The MPI library contains over 125 routines, but the number of key concepts is much smaller. In fact, it is possible to write fully-functional
. These routines are used to initialize and terminate the MPI Table 6.1 message-passing programs by using only the six routines shown in
library, to get information about the parallel computing environment, and to send and receive messages.
In this section we describe these routines as well as some basic concepts that are essential in writing correct and efficient
message-passing programs using MPI.
Table 6.1. The minimal set of MPI routines.
Initializes MPI. MPI_Init
Terminates MPI. MPI_Finalize
Determines the number of processes. MPI_Comm_size
Determines the label of the calling process. MPI_Comm_rank
Sends a message. MPI_Send
Receives a message. MPI_Recv
6.3.1 Starting and Terminating the MPI Library
 more than once MPI_Init  is called prior to any calls to other MPI routines. Its purpose is to initialize the MPI environment. Calling MPI_Init
 is called at the end of the computation, and it performs various MPI_Finalize during the execution of a program will lead to an error.
. MPI_Init  has been called, not even MPI_Finalize clean-up tasks to terminate the MPI environment. No MPI calls may be performed after
 must be called by all the processes, otherwise MPI's behavior will be undefined. The exact calling MPI_Finalize  and MPI_Init Both
sequences of these two routines for C are as follows:
int MPI_Init(int *argc, char ***argv)
int MPI_Finalize()
 are the command-line arguments of the C program. An MPI implementation is expected to MPI_Init  of argv  and argc The arguments
 array any command-line arguments that should be processed by the implementation before returning back to the argv remove from the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 has been called. MPI_Init  accordingly. Thus, command-line processing should be performed only after argc program, and to decrement
; otherwise they return an implementation-defined error MPI_SUCCESS  return MPI_Finalize  and MPI_Init Upon successful execution,
code.
The bindings and calling sequences of these two functions are illustrative of the naming practices and argument conventions followed by
. MPI_SUCCESS ". The return code for successful completion is MPI_ MPI. All MPI routines, data-types, and constants are prefixed by "
. This header file must be included in each MPI "mpi.h" This and other MPI constants and data-structures are defined for C in the file
program.
6.3.2 Communicators
. A communication domain is a set of processes that are communication domain A key concept used throughout MPI is that of the
,that are MPI_Comm allowed to communicate with each other. Information about communication domains is stored in variables of type
. These communicators are used as arguments to all message transfer MPI routines and they uniquely identify the communicators called
processes participating in the message transfer operation. Note that each process can belong to many different (possibly overlapping)
communication domains.
The communicator is used to define a set of processes that can communicate with each other. This set of processes form a
. In general, all the processes may need to communicate with each other. For this reason, MPI defines a communication domain
 which includes all the processes involved in the parallel execution. However, in many MPI_COMM_WORLD default communicator called
cases we want to perform communication only within (possibly overlapping) groups of processes. By using a different communicator for
each such group, we can ensure that no messages will ever interfere with messages destined to any other group. How to create and use
 as the communicator MPI_COMM_WORLD such communicators is described at a later point in this chapter. For now, it suffices to use
argument to all the MPI functions that require a communicator.
6.3.3 Getting Information
 functions are used to determine the number of processes and the label of the calling MPI_Comm_rank  and MPI_Comm_size The
process, respectively. The calling sequences of these routines are as follows:
int MPI_Comm_size(MPI_Comm comm, int *size)
int MPI_Comm_rank(MPI_Comm comm, int *rank)
. So, when comm  the number of processes that belong to the communicator size  returns in the variable MPI_Comm_size The function
 the number of size  will return in MPI_Comm_size(MPI_COMM_WORLD, &size) there is a single process per processor, the call
. The rank of a process rank processors used by the program. Every process that belongs to a communicator is uniquely identified by its
is an integer that ranges from zero up to the size of the communicator minus one. A process can determine its rank in a communicator by
. Up on return, the variable rank  function that takes two arguments: the communicator and an integer variable MPI_Comm_rank using the
 stores the rank of the process. Note that each process that calls either one of these functions must belong in the supplied rank
communicator, otherwise an error will occur.
Example 6.2 Hello World
We can use the four MPI functions just described to write a program that prints out a "Hello World" message from each
processor.
1 #include <mpi.h>
2
3 main(int argc, char *argv[])
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
4 {
5 int npes, myrank;
6
7 MPI_Init(&argc, &argv);
8 MPI_Comm_size(MPI_COMM_WORLD, &npes);
9 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
10 printf("From process %d out of %d, Hello World!\n",
11 myrank, npes);
12 MPI_Finalize();
13 }
6.3.4 Sending and Receiving Messages
, respectively. The calling sequences MPI_Recv  and MPI_Send The basic functions for sending and receiving messages in MPI are the
of these routines are as follows:
int MPI_Send(void *buf, int count, MPI_Datatype datatype,
 int dest, int tag, MPI_Comm comm)
int MPI_Recv(void *buf, int count, MPI_Datatype datatype,
 int source, int tag, MPI_Comm comm, MPI_Status *status)
. This buffer consists of consecutive entries of the type specified by the buf  sends the data stored in the buffer pointed by MPI_Send
. The correspondence between MPI datatypes count . The number of entries in the buffer is given by the parameter datatype parameter
. Note that for all C datatypes, an equivalent MPI datatype is provided. However, MPI Table 6.2 and those provided by C is shown in
. MPI_PACKED  and MPI_BYTE allows two additional datatypes that are not part of the C language. These are
 corresponds to a collection of data items that has been created by packing MPI_PACKED  corresponds to a byte (8 bits) and MPI_BYTE
, as well as in other MPI routines, is specified in terms of the MPI_Send non-contiguous data. Note that the length of the message in
number of entries being sent and not in terms of the number of bytes. Specifying the length in terms of the number of entries has the
advantage of making the MPI code portable, since the number of bytes used to store various datatypes can be different for different
architectures.
 argument is the rank dest  arguments. The comm  and dest  is uniquely specified by the MPI_Send The destination of the message sent by
tag . Each message has an integer-valued comm of the destination process in the communication domain specified by the communicator
 can take values ranging from zero up to the tag associated with it. This is used to distinguish different types of messages. The message-
 is implementation specific, it is at least 32,767. MPI_TAG_UB . Even though the value of MPI_TAG_UB MPI defined constant
comm  in the communication domain specified by the source  receives a message sent by a process whose rank is given by the MPI_Recv
 argument. If there are many messages with identical tag from tag argument. The tag of the sent message must be that specified by the
. tag  and source the same process, then any one of these messages is received. MPI allows specification of wildcard arguments for both
, then any process of the communication domain can be the source of the message. Similarly, if MPI_ANY_SOURCE  is set to source If
, then messages with any tag are accepted. The received message is stored in continuous locations in the MPI_ANY_TAG  is set to tag
 are used to specify the length of the supplied buffer. The MPI_Recv  arguments of datatype  and count . The buf buffer pointed to by
received message should be of length equal to or less than this length. This allows the receiving process to not know the exact size of
the message being sent. If the received message is larger than the supplied buffer, then an overflow error will occur, and the routine will
. MPI_ERR_TRUNCATE return the error
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Table 6.2. Correspondence between the datatypes supported by MPI and those supported by C.
C Datatype MPI Datatype
signed char MPI_CHAR
signed short int MPI_SHORT
signed int MPI_INT
signed long int MPI_LONG
unsigned char MPI_UNSIGNED_CHAR
unsigned short int MPI_UNSIGNED_SHORT
unsigned int MPI_UNSIGNED
unsigned long int MPI_UNSIGNED_LONG
float MPI_FLOAT
double MPI_DOUBLE
long double MPI_LONG_DOUBLE
MPI_BYTE
MPI_PACKED
 is status  operation. In C, MPI_Recv  variable can be used to get information about the status After a message has been received, the
 data-structure. This is implemented as a structure with three fields, as follows: MPI_Status stored using the
typedef struct MPI_Status {
 int MPI_SOURCE;
 int MPI_TAG;
 int MPI_ERROR;
};
 store the source and the tag of the received message. They are particularly useful when MPI_TAG  and MPI_SOURCE
 stores the error-code of the received MPI_ERROR  arguments. tag  and source  are used for the MPI_ANY_TAG  and MPI_ANY_SOURCE
message.
The status argument also returns information about the length of the received message. This information is not directly accessible from
 function. The calling sequence of this function is as follows: MPI_Get_count  variable, but it can be retrieved by calling the status the
int MPI_Get_count(MPI_Status *status, MPI_Datatype datatype,
 int *count)
, and returns the datatype  and the type of the received data in MPI_Recv  returned by status  takes as arguments the MPI_Get_count
 variable. count number of entries that were actually received in the
 is a blocking MPI_Recv  returns only after the requested message has been received and copied into the buffer. That is, MPI_Recv The
 returns only MPI_Send . In the first implementation, MPI_Send receive operation. However, MPI allows two different implementations for
 have been issued and the message has been sent to the receiver. In the second implementation, MPI_Recv after the corresponding
 to be executed. In MPI_Recv  first copies the message into a buffer and then returns, without waiting for the corresponding MPI_Send
 can be safely reused and overwritten. MPI programs MPI_Send  argument of buf either implementation, the buffer that is pointed by the
. Such programs are called MPI_Send must be able to run correctly regardless of which of the two methods is used for implementing
 and just think of it as MPI_Send . In writing safe MPI programs, sometimes it is helpful to forget about the alternate implementation of safe
being a blocking send operation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 place some restrictions on how we can mix and match send and MPI_Recv  and MPI_Send  The semantics of Avoiding Deadlocks
receive operations. For example, consider the following piece of code in which process 0 sends two messages with different tags to
process 1, and process 1 receives them in the reverse order.
1 int a[10], b[10], myrank;
2 MPI_Status status;
3 ...
4 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
5 if (myrank == 0) {
6 MPI_Send(a, 10, MPI_INT, 1, 1, MPI_COMM_WORLD);
7 MPI_Send(b, 10, MPI_INT, 1, 2, MPI_COMM_WORLD);
8 }
9 else if (myrank == 1) {
10 MPI_Recv(b, 10, MPI_INT, 0, 2, MPI_COMM_WORLD);
11 MPI_Recv(a, 10, MPI_INT, 0, 1, MPI_COMM_WORLD);
12 }
13 ...
 is implemented using buffering, then this code will run correctly provided that sufficient buffer space is available. However, if MPI_Send If
 is implemented by blocking until the matching receive has been issued, then neither of the two processes will be able to MPI_Send
tag  (i.e., the one with MPI_Recv ) will wait until process one issues the matching myrank == 0 proceed. This is because process zero (i.e.,
 equal to tag  (i.e., the one with MPI_Send equal to 1), and at the same time process one will wait until process zero performs the matching
2). This code fragment is not safe, as its behavior is implementation dependent. It is up to the programmer to ensure that his or her
matching the order in which the program will run correctly on any MPI implementation. The problem in this program can be corrected by
. Similar deadlock situations can also occur when a process sends a message to itself. Even send and receive operations are issued
though this is legal, its behavior is implementation dependent and must be avoided.
 can also lead to deadlocks in situations when each processor needs to send and receive a MPI_Recv  and MPI_Send Improper use of
 + 1 (modulo thei  sends a message to processi message in a circular fashion. Consider the following piece of code, in which process
 - 1 (module the number of processes).i number of processes) and receives a message from process
1 int a[10], b[10], npes, myrank;
2 MPI_Status status;
3 ...
4 MPI_Comm_size(MPI_COMM_WORLD, &npes);
5 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
6 MPI_Send(a, 10, MPI_INT, (myrank+1)%npes, 1, MPI_COMM_WORLD);
7 MPI_Recv(b, 10, MPI_INT, (myrank-1+npes)%npes, 1, MPI_COMM_WORLD);
8 ...
 will get buffered, allowing MPI_Send  is implemented using buffering, the program will work correctly, since every call to MPI_Send When
 blocks until the matching receive MPI_Send  to be performed, which will transfer the required data. However, if MPI_Recv the call of the
 operation. Note MPI_Recv has been issued, all processes will enter an infinite wait state, waiting for the neighboring process to issue a
that the deadlock still remains even when we have only two processes. Thus, when pairs of processes need to exchange data, the
above method leads to an unsafe program. The above example can be made safe, by rewriting it as follows:
1 int a[10], b[10], npes, myrank;
2 MPI_Status status;
3 ...
4 MPI_Comm_size(MPI_COMM_WORLD, &npes);
5 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
6 if (myrank%2 == 1) {
7 MPI_Send(a, 10, MPI_INT, (myrank+1)%npes, 1, MPI_COMM_WORLD);
8 MPI_Recv(b, 10, MPI_INT, (myrank-1+npes)%npes, 1, MPI_COMM_WORLD);
9 }
10 else {
11 MPI_Recv(b, 10, MPI_INT, (myrank-1+npes)%npes, 1, MPI_COMM_WORLD);
12 MPI_Send(a, 10, MPI_INT, (myrank+1)%npes, 1, MPI_COMM_WORLD);
13 }
14 ...
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
This new implementation partitions the processes into two groups. One consists of the odd-numbered processes and the other of the
even-numbered processes. The odd-numbered processes perform a send followed by a receive, and the even-numbered processes
,the target process (which has an even MPI_Send perform a receive followed by a send. Thus, when an odd-numbered process calls
 to receive that message, before attempting to send its own message. MPI_Recv number) will call
g  The above communication pattern appears frequently in many message-passin Sending and Receiving Messages Simultaneously
 function that both sends and receives a message. MPI_Sendrecv programs, and for this reason MPI provides the
 as MPI_Sendrecv . You can think of MPI_Recv  and MPI_Send  does not suffer from the circular deadlock problems of MPI_Sendrecv
 is the following: MPI_Sendrecv allowing data to travel for both send and receive simultaneously. The calling sequence of
int MPI_Sendrecv(void *sendbuf, int sendcount,
 MPI_Datatype senddatatype, int dest, int sendtag,
 void *recvbuf, int recvcount, MPI_Datatype recvdatatype,
 int source, int recvtag, MPI_Comm comm,
 MPI_Status *status)
. The send and receive MPI_Recv  and MPI_Send  are essentially the combination of the arguments of MPI_Sendrecv The arguments of
buffers must be disjoint, and the source and destination of the messages can be the same or different. The safe version of our earlier
 is as follows. MPI_Sendrecv example using
1 int a[10], b[10], npes, myrank;
2 MPI_Status status;
3 ...
4 MPI_Comm_size(MPI_COMM_WORLD, &npes);
5 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
6 MPI_SendRecv(a, 10, MPI_INT, (myrank+1)%npes, 1,
7 b, 10, MPI_INT, (myrank-1+npes)%npes, 1,
8 MPI_COMM_WORLD, &status);
9 ...
 be disjoint may force us to use a temporary buffer. MPI_Sendrecv In many programs, the requirement for the send and receive buffers of
This increases the amount of memory required by the program and also increases the overall run time due to the extra copy. This
 MPI function. This function performs a blocking send and receive, but it MPI_Sendrecv_replace problem can be solved by using that
uses a single buffer for both the send and receive operation. That is, the received data replaces the data that was sent out of the buffer.
The calling sequence of this function is the following:
int MPI_Sendrecv_replace(void *buf, int count,
 MPI_Datatype datatype, int dest, int sendtag,
 int source, int recvtag, MPI_Comm comm,
 MPI_Status *status)
Note that both the send and receive operations must transfer data of the same datatype.
6.3.5 Example: Odd-Even Sort
We will now use the MPI functions described in the previous sections to write a complete message-passing program that will sort a list of
n  that the odd-even sorting algorithm sorts a sequence of Section 9.3.1 numbers using the odd-even sorting algorithm. Recall from
 phases. During each of these phases, the odd-or even-numbered processes perform a p  processes in a total of p elements using
. To Program 6.1 compare-split step with their right neighbors. The MPI program for performing the odd-even sort in parallel is shown in
. p  is divisible by n simplify the presentation, this program assumes that
Program 6.1 Odd-Even Sorting
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[View full width]
 1 #include <stdlib.h>
/* Include MPI's header file */  2 #include <mpi.h>
 3
 4 main(int argc, char *argv[])
 5 {
/* The total number of elements to be sorted */  6 int n;
/* The total number of processes */  7 int npes;
/* The rank of the calling process */  8 int myrank;
/* The local number of elements, and the array that stores them */  9 int nlocal;
/* The array that stores the local elements */  10 int *elmnts;
/* The array that stores the received elements */  11 int *relmnts;
/* The rank of the process during odd-phase communication */  12 int oddrank;
/* The rank of the process during even-phase communication */  13 int evenrank;
/* Working space during the compare-split operation */  14 int *wspace;
 15 int i;
 16 MPI_Status status;
 17
/* Initialize MPI and get system information */  18
 19 MPI_Init(&argc, &argv);
 20 MPI_Comm_size(MPI_COMM_WORLD, &npes);
 21 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
 22
 23 n = atoi(argv[1]);
/* Compute the number of elements to be stored locally. */  24 nlocal = n/npes;
 25
/* Allocate memory for the various arrays */  26
 27 elmnts = (int *)malloc(nlocal*sizeof(int));
 28 relmnts = (int *)malloc(nlocal*sizeof(int));
 29 wspace = (int *)malloc(nlocal*sizeof(int));
 30
/* Fill-in the elmnts array with random elements */  31
 32 srandom(myrank);
 33 for (i=0; i<nlocal; i++)
 34 elmnts[i] = random();
 35
/* Sort the local elements using the built-in quicksort routine */  36
 37 qsort(elmnts, nlocal, sizeof(int), IncOrder);
 38
/* Determine the rank of the processors that myrank needs to communicate during  39
the */
/* odd and even phases of the algorithm */  40
 41 if (myrank%2 == 0) {
 42 oddrank = myrank-1;
 43 evenrank = myrank+1;
 44 }
 45 else {
 46 oddrank = myrank+1;
 47 evenrank = myrank-1;
 48 }
 49
/* Set the ranks of the processors at the end of the linear */  50
 51 if (oddrank == -1 || oddrank == npes)
 52 oddrank = MPI_PROC_NULL;
 53 if (evenrank == -1 || evenrank == npes)
 54 evenrank = MPI_PROC_NULL;
 55
/* Get into the main loop of the odd-even sorting algorithm */  56
 57 for (i=0; i<npes-1; i++) {
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
/* Odd phase */  58 if (i%2 == 1)
 59 MPI_Sendrecv(elmnts, nlocal, MPI_INT, oddrank, 1, relmnts,
 60 nlocal, MPI_INT, oddrank, 1, MPI_COMM_WORLD, &status);
/* Even phase */  61 else
 62 MPI_Sendrecv(elmnts, nlocal, MPI_INT, evenrank, 1, relmnts,
 63 nlocal, MPI_INT, evenrank, 1, MPI_COMM_WORLD, &status);
 64
 65 CompareSplit(nlocal, elmnts, relmnts, wspace,
 66 myrank < status.MPI_SOURCE);
 67 }
 68
 69 free(elmnts); free(relmnts); free(wspace);
 70 MPI_Finalize();
 71 }
 72
/* This is the CompareSplit function */  73
 74 CompareSplit(int nlocal, int *elmnts, int *relmnts, int *wspace,
 75 int keepsmall)
 76 {
 77 int i, j, k;
 78
 79 for (i=0; i<nlocal; i++)
/* Copy the elmnts array into the wspace array */  80 wspace[i] = elmnts[i];
 81
/* Keep the nlocal smaller elements */  82 if (keepsmall) {
 83 for (i=j=k=0; k<nlocal; k++) {
 84 if (j == nlocal || (i < nlocal && wspace[i] < relmnts[j]))
 85 elmnts[k] = wspace[i++];
 86 else
 87 elmnts[k] = relmnts[j++];
 88 }
 89 }
/* Keep the nlocal larger elements */  90 else {
 91 for (i=k=nlocal-1, j=nlocal-1; k>=0; k--) {
 92 if (j == 0 || (i >= 0 && wspace[i] >= relmnts[j]))
 93 elmnts[k] = wspace[i--];
 94 else
 95 elmnts[k] = relmnts[j--];
 96 }
 97 }
 98 }
 99
/* The IncOrder function that is called by qsort is defined as follows */ 100
101 int IncOrder(const void *e1, const void *e2)
102 {
103 return (*((int *)e1) - *((int *)e2));
104 }
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
6.4 Topologies and Embedding
MPI views the processes as being arranged in a one-dimensional topology and uses a linear ordering to number the processes. However,
in many parallel programs, processes are naturally arranged in higher-dimensional topologies (e.g., two- or three-dimensional). In such
programs, both the computation and the set of interacting processes are naturally identified by their coordinates in that topology. For
) may need to send j , i example, in a parallel program in which the processes are arranged in a two-dimensional topology, process (
). To implement these programs in MPI, we need to map each MPI process to a l , k message to (or receive message from) process (
process in that higher-dimensional topology.
 illustrates some possible mappings of eight MPI processes onto a 4 x 4 two-dimensional Figure 6.5 Many such mappings are possible.
) in the grid col , row  corresponds to process ( rank , an MPI process with rank Figure 6.5(a) topology. For example, for the mapping shown in
 (where '%' is C's module operator). As an illustration, the process with rank 7 is mapped to col = rank%4  and row = rank/4 such that
process (1, 3) in the grid.
Figure 6.5. Different ways to map a set of processes to a two-dimensional grid. (a) and (b) show
a row- and column-wise mapping of these processes, (c) shows a mapping that follows a
space-filling curve (dotted line), and (d) shows a mapping in which neighboring processes are
directly connected in a hypercube.
In general, the goodness of a mapping is determined by the pattern of interaction among the processes in the higher-dimensional
topology, the connectivity of physical processors, and the mapping of MPI processes to physical processors. For example, consider a
y  and x program that uses a two-dimensional topology and each process needs to communicate with its neighboring processes along the
directions of this topology. Now, if the processors of the underlying parallel system are connected using a hypercube interconnection
 is better, since neighboring processes in the grid are also neighboring processors in the Figure 6.5(d) network, then the mapping shown in
hypercube topology.
However, the mechanism used by MPI to assign ranks to the processes in a communication domain does not use any information about
the interconnection network, making it impossible to perform topology embeddings in an intelligent manner. Furthermore, even if we had
that information, we will need to specify different mappings for different interconnection networks, diminishing the architecture independent
advantages of MPI. A better approach is to let the library itself compute the most appropriate embedding of a given topology to the
processors of the underlying parallel computer. This is exactly the approach facilitated by MPI. MPI provides a set of routines that allows
the programmer to arrange the processes in different topologies without having to explicitly specify how these processes are mapped onto
the processors. It is up to the MPI library to find the most appropriate mapping that reduces the cost of sending and receiving messages.
6.4.1 Creating and Using Cartesian Topologies
MPI provides routines that allow the specification of virtual process topologies of arbitrary connectivity in terms of a graph. Each node in
the graph corresponds to a process and two nodes are connected if they communicate with each other. Graphs of processes can be used
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
to specify any desired topology. However, most commonly used topologies in message-passing programs are one-, two-, or
. For this reason, MPI provides a set of specialized routines for Cartesian topologies higher-dimensional grids, that are also referred to as
specifying and manipulating this type of multi-dimensional grid topologies.
. Its calling sequence is as follows. MPI_Cart_create MPI's function for describing Cartesian topologies is called
int MPI_Cart_create(MPI_Comm comm_old, int ndims, int *dims,
 int *periods, int reorder, MPI_Comm *comm_cart)
 and creates a virtual process topology. The comm_old This function takes the group of processes that belong to the communicator
. Any subsequent MPI routines that MPI_Cart_create  that is created by comm_cart topology information is attached to a new communicator
 as the communicator argument. Note that all the processes comm_cart want to take advantage of this new Cartesian topology must use
 communicator must call this function. The shape and properties of the topology are specified by the arguments comm_old that belong to the
 specify the size along dims  specifies the number of dimensions of the topology. The array ndims . The argument periods , and dims , ndims
 is used periods  th dimension of the topology. The arrayi  th element of this array stores the size of thei each dimension of the topology. The
 is true (non-zero in C), then the topology has periods[i] to specify whether or not the topology has wraparound connections. In particular, if
 is used to determine if the processes in the reorder , otherwise it does not. Finally, the argument i wraparound connections along dimension
 is false, then the rank of each process in the new group is identical to reorder new group (i.e., communicator) are to be reordered or not. If
 may reorder the processes if that leads to a better embedding of the virtual topology MPI_Cart_create its rank in the old group. Otherwise,
 array is smaller than the number of processes in the dims onto the parallel computer. If the total number of processes specified in the
, then some processes will not be part of the Cartesian topology. For this set of processes, the value comm_old communicator specified by
 (an MPI defined constant). Note that it will result in an error if the total number of processes MPI_COMM_NULL  will be set to comm_cart of
 communicator. comm_old  is greater than the number of processes in the dims specified by
l  When a Cartesian topology is used, each process is better identified by its coordinates in this topology. However, al Process Naming
MPI functions that we described for sending and receiving messages require that the source and the destination of each message be
, for performing MPI_Cart_coord  and MPI_Cart_rank specified using the rank of the process. For this reason, MPI provides two functions,
coordinate-to-rank and rank-to-coordinate translations, respectively. The calling sequences of these routines are the following:
int MPI_Cart_rank(MPI_Comm comm_cart, int *coords, int *rank)
int MPI_Cart_coord(MPI_Comm comm_cart, int rank, int maxdims,
 int *coords)
MPI_Cart_coords . The rank  array and returns its rank in coords  takes the coordinates of the process as argument in the MPI_Cart_rank The
 should maxdims . Note that maxdims , of length coords  and returns its Cartesian coordinates in the array rank takes the rank of the process
. comm_cart be at least as large as the number of dimensions in the Cartesian topology specified by the communicator
Frequently, the communication performed among processes in a Cartesian topology is that of shifting data along a dimension of the
, that can be used to compute the rank of the source and destination processes for MPI_Cart_shift topology. MPI provides the function
such operation. The calling sequence of this function is the following:
int MPI_Cart_shift(MPI_Comm comm_cart, int dir, int s_step,
 int *rank_source, int *rank_dest)
 argument, and is one of the dimensions of the topology. The size of the shift step is specified dir The direction of the shift is specified in the
. If the Cartesian topology was created with rank_dest  and rank_source  argument. The computed ranks are returned in s_step in the
 value is MPI_PROC_NULL  entry was set to true), then the shift wraps around. Otherwise, a periods[dir] wraparound connections (i.e., the
 for those processes that are outside the topology. rank_dest  and/or rank_source returned for
6.4.2 Example: Cannon's Matrix-Matrix Multiplication
, B  and A To illustrate how the various topology functions are used we will implement Cannon's algorithm for multiplying two matrices
. Cannon's algorithm views the processes as being arranged in a virtual two-dimensional square array. It uses Section 8.2.2 described in
 is the total p  is the size of each matrix and n  x n  in a block fashion. That is, if C , and the result matrix B , A this array to distribute the matrices
 is a perfect square). p  (assuming that number of process, then each matrix is divided into square blocks of size
 blocks of each matrix. After an initial data alignment phase, the algorithm j, i C , and j, i B , j, i A  in the grid is assigned the j, i P Now, process
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, and then sends the block B  and A  steps. In each step, every process multiplies the locally available blocks of matrices proceeds in
 to the upward process. B  to the leftward process, and the block of A of
. The n  shows the MPI function that implements Cannon's algorithm. The dimension of the matrices is supplied in the parameter Program 6.2
, respectively. The size of these arrays is C , and B , A  point to the locally stored portions of the matrices c , and b , a parameters
 is a multiple of n  is a perfect square and that p  is the number of processes. This routine assumes that p , where
 function. Note that the MatrixMatrixMultiply  stores the communicator describing the processes that call the comm . The parameter
remaining programs in this chapter will be provided in the form of a function, as opposed to complete stand-alone programs.
Program 6.2 Cannon's Matrix-Matrix Multiplication with MPI's Topologies
 1 MatrixMatrixMultiply(int n, double *a, double *b, double *c,
 2 MPI_Comm comm)
 3 {
 4 int i;
 5 int nlocal;
 6 int npes, dims[2], periods[2];
 7 int myrank, my2drank, mycoords[2];
 8 int uprank, downrank, leftrank, rightrank, coords[2];
 9 int shiftsource, shiftdest;
10 MPI_Status status;
11 MPI_Comm comm_2d;
12
/* Get the communicator related information */ 13
14 MPI_Comm_size(comm, &npes);
15 MPI_Comm_rank(comm, &myrank);
16
/* Set up the Cartesian topology */ 17
18 dims[0] = dims[1] = sqrt(npes);
19
/* Set the periods for wraparound connections */ 20
21 periods[0] = periods[1] = 1;
22
/* Create the Cartesian topology, with rank reordering */ 23
24 MPI_Cart_create(comm, 2, dims, periods, 1, &comm_2d);
25
/* Get the rank and coordinates with respect to the new topology */ 26
27 MPI_Comm_rank(comm_2d, &my2drank);
28 MPI_Cart_coords(comm_2d, my2drank, 2, mycoords);
29
/* Compute ranks of the up and left shifts */ 30
31 MPI_Cart_shift(comm_2d, 0, -1, &rightrank, &leftrank);
32 MPI_Cart_shift(comm_2d, 1, -1, &downrank, &uprank);
33
/* Determine the dimension of the local matrix block */ 34
35 nlocal = n/dims[0];
36
/* Perform the initial matrix alignment. First for A and then for B */ 37
38 MPI_Cart_shift(comm_2d, 0, -mycoords[0], &shiftsource, &shiftdest);
39 MPI_Sendrecv_replace(a, nlocal*nlocal, MPI_DOUBLE, shiftdest,
40 1, shiftsource, 1, comm_2d, &status);
41
42 MPI_Cart_shift(comm_2d, 1, -mycoords[1], &shiftsource, &shiftdest);
43 MPI_Sendrecv_replace(b, nlocal*nlocal, MPI_DOUBLE,
44 shiftdest, 1, shiftsource, 1, comm_2d, &status);
45
/* Get into the main computation loop */ 46
47 for (i=0; i<dims[0]; i++) {
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
/*c=c+a*b*/ 48 MatrixMultiply(nlocal, a, b, c);
49
/* Shift matrix a left by one */ 50
51 MPI_Sendrecv_replace(a, nlocal*nlocal, MPI_DOUBLE,
52 leftrank, 1, rightrank, 1, comm_2d, &status);
53
/* Shift matrix b up by one */ 54
55 MPI_Sendrecv_replace(b, nlocal*nlocal, MPI_DOUBLE,
56 uprank, 1, downrank, 1, comm_2d, &status);
57 }
58
/* Restore the original distribution of a and b */ 59
60 MPI_Cart_shift(comm_2d, 0, +mycoords[0], &shiftsource, &shiftdest);
61 MPI_Sendrecv_replace(a, nlocal*nlocal, MPI_DOUBLE,
62 shiftdest, 1, shiftsource, 1, comm_2d, &status);
63
64 MPI_Cart_shift(comm_2d, 1, +mycoords[1], &shiftsource, &shiftdest);
65 MPI_Sendrecv_replace(b, nlocal*nlocal, MPI_DOUBLE,
66 shiftdest, 1, shiftsource, 1, comm_2d, &status);
67
/* Free up communicator */ 68 MPI_Comm_free(&comm_2d);
69 }
70
/* This function performs a serial matrix-matrix multiplication c = a*b */ 71
72 MatrixMultiply(int n, double *a, double *b, double *c)
73 {
74 int i, j, k;
75
76 for (i=0; i<n; i++)
77 for (j=0; j<n; j++)
78 for (k=0; k<n; k++)
79 c[i*n+j] += a[i*n+k]*b[k*n+j];
80 }
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
6.5 Overlapping Communication with Computation
The MPI programs we developed so far used blocking send and receive operations whenever they needed to perform point-to-point
communication. Recall that a blocking send operation remains blocked until the message has been copied out of the send buffer (either
into a system buffer at the source process or sent to the destination process). Similarly, a blocking receive operation returns only after
the message has been received and copied into the receive buffer. For example, consider Cannon's matrix-matrix multiplication program
f . During each iteration of its main computational loop (lines 47– 57), it first computes the matrix multiplication o Program 6.2 described in
 which blocks until the specified MPI_Sendrecv_replace , using b  and a , and then shifts the blocks of b  and a the sub-matrices stored in
) time for 1.5 p / 3 n  ( O matrix block has been sent and received by the corresponding processes. In each iteration, each process spends
. Now, since the blocks of matrices B  and A ) time for shifting the blocks of matrices p/ 2 n ( O performing the matrix-matrix multiplication and
 do not change as they are shifted among the processors, it will be preferable if we can overlap the transmission of these blocks B  and A
with the computation for the matrix-matrix multiplication, as many recent distributed-memory parallel computers have dedicated
communication controllers that can perform the transmission of messages without interrupting the CPUs.
6.5.1 Non-Blocking Communication Operations
In order to overlap communication with computation, MPI provides a pair of functions for performing non-blocking send and receive
 starts a send operation but does not complete, that is, it returns MPI_Isend . MPI_Irecv  and MPI_Isend operations. These functions are
 starts a receive operation but returns before the data has been received MPI_Irecv before the data is copied out of the buffer. Similarly,
and copied into the buffer. With the support of appropriate hardware, the transmission and reception of messages can proceed
concurrently with the computations performed by the program upon the return of the above functions.
However, at a later point in the program, a process that has started a non-blocking send or receive operation must make sure that this
operation has completed before it proceeds with its computations. This is because a process that has started a non-blocking send
operation may want to overwrite the buffer that stores the data that are being sent, or a process that has started a non-blocking receive
operation may want to use the data it requested. To check the completion of non-blocking send and receive operations, MPI provides a
. The first tests whether or not a non-blocking operation has finished and the second waits (i.e., MPI_Wait  and MPI_Test pair of functions
gets blocked) until a non-blocking operation actually finishes.
 are the following: MPI_Irecv  and MPI_Isend The calling sequences of
int MPI_Isend(void *buf, int count, MPI_Datatype datatype,
 int dest, int tag, MPI_Comm comm, MPI_Request *request)
int MPI_Irecv(void *buf, int count, MPI_Datatype datatype,
 int source, int tag, MPI_Comm comm, MPI_Request *request)
Note that these functions have similar arguments as the corresponding blocking send and receive functions. The main difference is that
 and return a pointer to it in the request object  functions allocate a MPI_Irecv  and MPI_Isend . request they take an additional argument
 functions to identify the operation whose MPI_Wait  and MPI_Test  variable. This request object is used as an argument in the request
status we want to query or to wait for its completion.
 argument similar to the blocking receive function, but the status information status  function does not take a MPI_Irecv Note that the
 functions. MPI_Wait  and MPI_Test associated with the receive operation is returned by the
int MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)
int MPI_Wait(MPI_Request *request, MPI_Status *status)
flag = {true}  has finished. It returns request  tests whether or not the non-blocking send or receive operation identified by its MPI_Test
 (a zero value in C). In the case that the non-blocking operation has {false} (non-zero value in C) if it completed, otherwise it returns
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 object is set status . Also the MPI_REQUEST_NULL  is set to request  is deallocated and request finished, the request object pointed to by
 object is status  is not modified and the value of the request to contain information about the operation. If the operation has not finished,
 completes. In that case it deal-locates the request  function blocks until the non-blocking operation identified by MPI_Wait undefined. The
 object. status , and returns information about the completed operation in the MPI_REQUEST_NULL  object, sets it to request
For the cases that the programmer wants to explicitly deallocate a request object, MPI provides the following function.
int MPI_Request_free(MPI_Request *request)
Note that the deallocation of the request object does not have any effect on the associated non-blocking send or receive operation. That
is, if it has not yet completed it will proceed until its completion. Hence, one must be careful before explicitly deallocating a request
object, since without it, we cannot check whether or not the non-blocking operation has completed.
A non-blocking communication operation can be matched with a corresponding blocking operation. For example, a process can send a
message using a non-blocking send operation and this message can be received by the other process using a blocking receive
operation.
r  By using non-blocking communication operations we can remove most of the deadlocks associated with thei Avoiding Deadlocks
 the following piece of code is not safe. Section 6.3 blocking counterparts. For example, as we discussed in
 1 int a[10], b[10], myrank;
 2 MPI_Status status;
 3 ...
 4 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
 5 if (myrank == 0) {
 6 MPI_Send(a, 10, MPI_INT, 1, 1, MPI_COMM_WORLD);
 7 MPI_Send(b, 10, MPI_INT, 1, 2, MPI_COMM_WORLD);
 8 }
 9 else if (myrank == 1) {
10 MPI_Recv(b, 10, MPI_INT, 0, 2, &status, MPI_COMM_WORLD);
11 MPI_Recv(a, 10, MPI_INT, 0, 1, &status, MPI_COMM_WORLD);
12 }
13 ...
However, if we replace either the send or receive operations with their non-blocking counterparts, then the code will be safe, and will
correctly run on any MPI implementation.
 1 int a[10], b[10], myrank;
 2 MPI_Status status;
 3 MPI_Request requests[2];
 4 ...
 5 MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
 6 if (myrank == 0) {
 7 MPI_Send(a, 10, MPI_INT, 1, 1, MPI_COMM_WORLD);
 8 MPI_Send(b, 10, MPI_INT, 1, 2, MPI_COMM_WORLD);
 9 }
10 else if (myrank == 1) {
11 MPI_Irecv(b, 10, MPI_INT, 0, 2, &requests[0], MPI_COMM_WORLD);
12 MPI_Irecv(a, 10, MPI_INT, 0, 1, &requests[1], MPI_COMM_WORLD);
13 }
14 ...
This example also illustrates that the non-blocking operations started by any process can finish in any order depending on the
transmission or reception of the corresponding messages. For example, the second receive operation will finish before the first does.
Example: Cannon's Matrix-Matrix Multiplication (Using Non-Blocking Operations)
 shows the MPI program that implements Cannon's algorithm using non-blocking send and receive operations. The various Program 6.3
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Program 6.2 parameters are identical to those of
Program 6.3 Non-Blocking Cannon's Matrix-Matrix Multiplication
 1 MatrixMatrixMultiply_NonBlocking(int n, double *a, double *b,
 2 double *c, MPI_Comm comm)
 3 {
 4 int i, j, nlocal;
 5 double *a_buffers[2], *b_buffers[2];
 6 int npes, dims[2], periods[2];
 7 int myrank, my2drank, mycoords[2];
 8 int uprank, downrank, leftrank, rightrank, coords[2];
 9 int shiftsource, shiftdest;
10 MPI_Status status;
11 MPI_Comm comm_2d;
12 MPI_Request reqs[4];
13
/* Get the communicator related information */ 14
15 MPI_Comm_size(comm, &npes);
16 MPI_Comm_rank(comm, &myrank);
17
/* Set up the Cartesian topology */ 18
19 dims[0] = dims[1] = sqrt(npes);
20
/* Set the periods for wraparound connections */ 21
22 periods[0] = periods[1] = 1;
23
/* Create the Cartesian topology, with rank reordering */ 24
25 MPI_Cart_create(comm, 2, dims, periods, 1, &comm_2d);
26
/* Get the rank and coordinates with respect to the new topology */ 27
28 MPI_Comm_rank(comm_2d, &my2drank);
29 MPI_Cart_coords(comm_2d, my2drank, 2, mycoords);
30
/* Compute ranks of the up and left shifts */ 31
32 MPI_Cart_shift(comm_2d, 0, -1, &rightrank, &leftrank);
33 MPI_Cart_shift(comm_2d, 1, -1, &downrank, &uprank);
34
/* Determine the dimension of the local matrix block */ 35
36 nlocal = n/dims[0];
37
/* Setup the a_buffers and b_buffers arrays */ 38
39 a_buffers[0] = a;
40 a_buffers[1] = (double *)malloc(nlocal*nlocal*sizeof(double));
41 b_buffers[0] = b;
42 b_buffers[1] = (double *)malloc(nlocal*nlocal*sizeof(double));
43
/* Perform the initial matrix alignment. First for A and then for B */ 44
45 MPI_Cart_shift(comm_2d, 0, -mycoords[0], &shiftsource, &shiftdest);
46 MPI_Sendrecv_replace(a_buffers[0], nlocal*nlocal, MPI_DOUBLE,
47 shiftdest, 1, shiftsource, 1, comm_2d, &status);
48
49 MPI_Cart_shift(comm_2d, 1, -mycoords[1], &shiftsource, &shiftdest);
50 MPI_Sendrecv_replace(b_buffers[0], nlocal*nlocal, MPI_DOUBLE,
51 shiftdest, 1, shiftsource, 1, comm_2d, &status);
52
/* Get into the main computation loop */ 53
54 for (i=0; i<dims[0]; i++) {
55 MPI_Isend(a_buffers[i%2], nlocal*nlocal, MPI_DOUBLE,
56 leftrank, 1, comm_2d, &reqs[0]);
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
57 MPI_Isend(b_buffers[i%2], nlocal*nlocal, MPI_DOUBLE,
58 uprank, 1, comm_2d, &reqs[1]);
59 MPI_Irecv(a_buffers[(i+1)%2], nlocal*nlocal, MPI_DOUBLE,
60 rightrank, 1, comm_2d, &reqs[2]);
61 MPI_Irecv(b_buffers[(i+1)%2], nlocal*nlocal, MPI_DOUBLE,
62 downrank, 1, comm_2d, &reqs[3]);
63
/* c = c + a*b */ 64
65 MatrixMultiply(nlocal, a_buffers[i%2], b_buffers[i%2], c);
66
67 for (j=0; j<4; j++)
68 MPI_Wait(&reqs[j], &status);
69 }
70
/* Restore the original distribution of a and b */ 71
72 MPI_Cart_shift(comm_2d, 0, +mycoords[0], &shiftsource, &shiftdest);
73 MPI_Sendrecv_replace(a_buffers[i%2], nlocal*nlocal, MPI_DOUBLE,
74 shiftdest, 1, shiftsource, 1, comm_2d, &status);
75
76 MPI_Cart_shift(comm_2d, 1, +mycoords[1], &shiftsource, &shiftdest);
77 MPI_Sendrecv_replace(b_buffers[i%2], nlocal*nlocal, MPI_DOUBLE,
78 shiftdest, 1, shiftsource, 1, comm_2d, &status);
79
/* Free up communicator */ 80 MPI_Comm_free(&comm_2d);
81
82 free(a_buffers[1]);
83 free(b_buffers[1]);
84 }
) and this non-blocking one. The first difference is that the Program 6.2 There are two main differences between the blocking program (
 and A , that are used as the buffer of the blocks of b_buffers  and a_buffers non-blocking program requires the use of the additional arrays
 that are being received while the computation involving the previous blocks is performed. The second difference is that in the main B
 to the processes left and B  and A computational loop, it first starts the non-blocking send operations to send the locally stored blocks of
up the grid, and then starts the non-blocking receive operations to receive the blocks for the next iteration from the processes right and
down the grid. Having initiated these four non-blocking operations, it proceeds to perform the matrix-matrix multiplication of the blocks it
 to wait for the send and receive operations to complete. MPI_Wait currently stores. Finally, before it proceeds to the next iteration, it uses
. This is to B  and one for A Note that in order to overlap communication with computation we have to use two auxiliary arrays – one for
 that are used in the computation, which proceeds concurrently with B  and A ensure that incoming messages never overwrite the blocks of
the data transfer. Thus, increased performance (by overlapping communication with computation) comes at the expense of increased
memory requirements. This is a trade-off that is often made in message-passing programs, since communication overheads can be quite
high for loosely coupled distributed memory parallel computers.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
6.6 Collective Communication and Computation Operations
MPI provides an extensive set of functions for performing many commonly used collective communication operations. In particular, the
 are supported by MPI. All of the collective communication Chapter 4 majority of the basic communication operations described in
functions provided by MPI take as an argument a communicator that defines the group of processes that participate in the collective
operation. All the processes that belong to this communicator participate in the operation, and all of them must call the collective
communication function. Even though collective communication operations do not act like barriers (i.e., it is possible for a processor to
virtual go past its call for the collective communication operation even before other processes have reached it), it acts like a
synchronization step in the following sense: the parallel program should be written such that it behaves correctly even if a global
synchronization is performed before and after the collective call. Since the operations are virtually synchronous, they do not require tags.
In some of the collective functions data is required to be sent from a single process (source-process) or to be received by a single
process (target-process). In these functions, the source- or target-process is one of the arguments supplied to the routines. All the
processes in the group (i.e., communicator) must specify the same source- or target-process. For most collective communication
operations, MPI provides two different variants. The first transfers equal-size data to or from each process, and the second transfers data
that can be of different sizes.
6.6.1 Barrier
 function. MPI_Barrier The barrier synchronization operation is performed in MPI using the
int MPI_Barrier(MPI_Comm comm)
MPI_Barrier  is the communicator that defines the group of processes that are synchronized. The call to MPI_Barrier The only argument of
returns only after all the processes in the group have called this function.
6.6.2 Broadcast
 function. MPI_Bcast  is performed in MPI using the Section 4.1 The one-to-all broadcast operation described in
int MPI_Bcast(void *buf, int count, MPI_Datatype datatype,
 int source, MPI_Comm comm)
 to all the other processes in the group. The data received by each source  of process buf  sends the data stored in the buffer MPI_Bcast
. The amount of data sent by the datatype  entries of type count . The data that is broadcast consist of buf process is stored in the buffer
 fields must match datatype  and count  process must be equal to the amount of data that is being received by each process; i.e., the source
on all processes.
6.6.3 Reduction
 function. MPI_Reduce  is performed in MPI using the Section 4.1 The all-to-one reduction operation described in
int MPI_Reduce(void *sendbuf, void *recvbuf, int count,
 MPI_Datatype datatype, MPI_Op op, int target,
 MPI_Comm comm)
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, and op  of each process in the group, using the operation specified in sendbuf  combines the elements stored in the buffer MPI_Reduce
 must have the same recvbuf  and sendbuf . Both the target  of the process with rank recvbuf returns the combined values in the buffer
 of the target  array, even if they are not the recvbuf . Note that all processes must provide a datatype  items of type count number of
 is more than one, then the combine operation is applied element-wise on each entry of the sequence. count reduction operation. When
. comm , and target , op , datatype , count  with the same value for MPI_Reduce All the processes must call
. MPI also allows programmers sendbuf MPI provides a list of predefined operations that can be used to combine the elements stored in
. For example, in Table 6.3 to define their own operations, which is not covered in this book. The predefined operations are shown in
 argument. Not all of these op  value must be used for the MPI_MAX , the sendbuf order to compute the maximum of the elements stored in
) is not MPI_BOR  = op operations can be applied to all possible data-types supported by MPI. For example, a bit-wise OR operation (i.e.,
 shows the various data-types that Table 6.3 . The last column of MPI_REAL  and MPI_FLOAT defined for real-valued data-types such as
can be used with each operation.
Table 6.3. Predefined reduction operations.
Datatypes Meaning Operation
C integers and floating point Maximum MPI_MAX
C integers and floating point Minimum MPI_MIN
C integers and floating point Sum MPI_SUM
C integers and floating point Product MPI_PROD
C integers Logical AND MPI_LAND
C integers and byte Bit-wise AND MPI_BAND
C integers Logical OR MPI_LOR
C integers and byte Bit-wise OR MPI_BOR
C integers Logical XOR MPI_LXOR
C integers and byte Bit-wise XOR MPI_BXOR
Data-pairs max-min value-location MPI_MAXLOC
Data-pairs min-min value-location MPI_MINLOC
 isl 's andi v ) such that v is the maximum among all l , v ) and returns the pair ( il , i v  combines pairs of values ( MPI_MAXLOC The operation
) such that v is the l , v  combines pairs of values and returns the pair ( MPI_MINLOC . Similarly, i v  = v 's such thatil the smallest among all
 is MPI_MINLOC  or MPI_MAXLOC . One possible application of i v  = v 's such thatil  is the smallest among alll 's andi v minimum among all
to compute the maximum or minimum of a list of numbers each residing on a different process and also the rank of the first process that
 require datatypes that MPI_MINLOC  and MPI_MAXLOC . Since both Figure 6.6 stores this maximum or minimum, as illustrated in
. In C, these datatypes are Table 6.4 correspond to pairs of values, a new set of MPI datatypes have been defined as shown in
implemented as structures containing the corresponding types.
 operators. MPI_MAXLOC  and MPI_MINLOC Figure 6.6. An example use of the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 operation that returns the MPI_Allreduce When the result of the reduction operation is needed by all the processes, MPI provides the
. Section 4.3 result to all the processes. This function provides the functionality of the all-reduce operation described in
 reduction MPI_MINLOC  and MPI_MAXLOC Table 6.4. MPI datatypes for data-pairs used with the
operations.
C Datatype MPI Datatype
s int pair of MPI_2INT
int  and short MPI_SHORT_INT
int  and long MPI_LONG_INT
int  and long double MPI_LONG_DOUBLE_INT
int  and float MPI_FLOAT_INT
int  and double MPI_DOUBLE_INT
int MPI_Allreduce(void *sendbuf, void *recvbuf, int count,
 MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)
 argument since all processes receive the result of the operation. target Note that there is no
6.6.4 Prefix
 function. MPI_Scan The prefix-sum operation described in Section 4.3 is performed in MPI using the
int MPI_Scan(void *sendbuf, void *recvbuf, int count,
 MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)
. recvbuf  at each process and returns the result in the buffer sendbuf  performs a prefix reduction of the data stored in the buffer MPI_Scan
 will store, at the end of the operation, the reduction of the send buffers of the processesi The receive buffer of the process with rank
) as well as the restrictions on the various op . The type of supported operations (i.e., i whose ranks range from 0 up to and including
. MPI_Reduce  are the same as those for the reduction operation MPI_Scan arguments of
6.6.5 Gather
 function. MPI_Gather The gather operation described in Section 4.4 is performed in MPI using the
int MPI_Gather(void *sendbuf, int sendcount,
 MPI_Datatype senddatatype, void *recvbuf, int recvcount,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 MPI_Datatype recvdatatype, int target, MPI_Comm comm)
 is the number p  process. As a result, if target  to the sendbuf  process, sends the data stored in the array target Each process, including the
 of the recvbuf  buffers. The data is stored in the array p , the target process receives a total of comm of processors in the communication
sendcount  *i  starting at location recvbuf  are stored in thei target process, in a rank order. That is, the data from process with rank
). recvdatatype  is of the same type as recvbuf (assuming that the array
 and sendcount  must be called with the MPI_Gather The data sent by each process must be of the same size and type. That is,
 arguments having the same values at each process. The information about the receive buffer, its length and type applies senddatatype
 specifies the number of elements received recvcount only for the target process and is ignored for all the other processes. The argument
 and their datatypes sendcount  must be the same as recvcount by each process and not the total number of elements it receives. So,
must be matching.
 function in which the data are gathered to all the processes and not only at the target process. MPI_Allgather MPI also provides the
int MPI_Allgather(void *sendbuf, int sendcount,
 MPI_Datatype senddatatype, void *recvbuf, int recvcount,
 MPI_Datatype recvdatatype, MPI_Comm comm)
 array recvbuf ; however, each process must now supply a MPI_Gather The meanings of the various parameters are similar to those for
that will store the gathered data.
In addition to the above versions of the gather operation, in which the sizes of the arrays sent by each process are the same, MPI also
 variants. The vector vector provides versions in which the size of the arrays can be different. MPI refers to these operations as the
, respectively. MPI_Allgatherv  and MPI_Gatherv  operations are provided by the functions MPI_Allgather  and MPI_Gather variants of the
int MPI_Gatherv(void *sendbuf, int sendcount,
 MPI_Datatype senddatatype, void *recvbuf,
 int *recvcounts, int *displs,
 MPI_Datatype recvdatatype, int target, MPI_Comm comm)
int MPI_Allgatherv(void *sendbuf, int sendcount,
 MPI_Datatype senddatatype, void *recvbuf,
 int *recvcounts, int *displs, MPI_Datatype recvdatatype,
 MPI_Comm comm)
 parameter with the recvcount These functions allow a different number of data elements to be sent by each process by replacing the
 is equal to the size of the recvcounts . Note that the size of recvcounts[i]  is equal toi . The amount of data sent by process recvcounts array
 the data sent by recvbuf , which is also of the same size, is used to determine where in displs . The array parameter comm communicator
. Note that, as displs[i]  starting at location recvbuf  are stored ini each process will be stored. In particular, the data sent by process
 parameter can be different for different processes. sendcount opposed to the non-vector variants, the
6.6.6 Scatter
 function. MPI_Scatter  is performed in MPI using the Section 4.4 The scatter operation described in
int MPI_Scatter(void *sendbuf, int sendcount,
 MPI_Datatype senddatatype, void *recvbuf, int recvcount,
 MPI_Datatype recvdatatype, int source, MPI_Comm comm)
 to each processes, including itself. The data that are received are sendbuf  process sends a different part of the send buffer source The
 location of the sendcount  *i  starting from the senddatatype  contiguous elements of type sendcount  receivesi . Process recvbuf stored in
 must be called by all the MPI_Scatter ). senddatatype  is of the same type as sendbuf  of the source process (assuming that sendbuf
 arguments. Note again comm , and source , recvdatatype , recvcount , senddatatype , sendcount processes with the same values for the
 is the number of elements sent to each individual process. sendcount that
, that allows different MPI_Scatterv Similarly to the gather operation, MPI provides a vector variant of the scatter operation, called
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
amounts of data to be sent to different processes.
int MPI_Scatterv(void *sendbuf, int *sendcounts, int *displs,
 MPI_Datatype senddatatype, void *recvbuf, int recvcount,
 MPI_Datatype recvdatatype, int source, MPI_Comm comm)
 that determines the number of elements to be sent sendcounts  has been replaced by the array sendcount As we can see, the parameter
 is used to determine displs . Also, the array i  elements to process sendcounts[i]  process sends target to each process. In particular, the
i , the data sent to process senddatatype  is of the same type is sendbuf  these elements will be sent from. In particular, if sendbuf where in
 arrays are of size equal to the number of processes in the displs  and sendcounts . Both the sendbuf  of array displs[i] start at location
. sendbuf  to send overlapping regions of MPI_Scatterv  array we can use displs communicator. Note that by appropriately setting the
6.6.7 All-to-All
 function. MPI_Alltoall  is performed in MPI by using the Section 4.5 The all-to-all personalized communication operation described in
int MPI_Alltoall(void *sendbuf, int sendcount,
 MPI_Datatype senddatatype, void *recvbuf, int recvcount,
 MPI_Datatype recvdatatype, MPI_Comm comm)
i  array to each other process, including itself. Each process sends to process sendbuf Each process sends a different portion of the
 array. The data that are sendbuf  location of its sendcount  *i  starting from the senddatatype  contiguous elements of type sendcount
 and stores them recvdatatype  elements of type recvcount i  array. Each process receives from process recvbuf received are stored in the
 must be called by all the processes with the same values for the MPI_Alltoall . recvcount  *i  array starting at location recvbuf in its
 are the number of recvcount  and sendcount  arguments. Note that comm , and recvdatatype , recvcount , senddatatype , sendcount
elements sent to, and received from, each individual process.
 that allows different MPI_Alltoallv MPI also provides a vector variant of the all-to-all personalized communication operation called
amounts of data to be sent to and received from each process.
int MPI_Alltoallv(void *sendbuf, int *sendcounts, int *sdispls
 MPI_Datatype senddatatype, void *recvbuf, int *recvcounts,
 int *rdispls, MPI_Datatype recvdatatype, MPI_Comm comm)
 is used to specify sdispls  is used to specify the number of elements sent to each process, and the parameter sendcounts The parameter
 of sdispls[i] , starting at location i  in which these elements are stored. In particular, each process sends to process sendbuf the location in
 is used to specify the number of elements received by recvcounts  contiguous elements. The parameter sendcounts[i] , sendbuf the array
 in which these elements are stored. In particular, each recvbuf  is used to specify the location in rdispls each process, and the parameter
. rdispls[i]  starting at location recvbuf  elements that are stored in contiguous locations of recvcounts[i] i process receives from process
 arguments. comm , and recvdatatype , senddatatype  must be called by all the processes with the same values for the MPI_Alltoallv
6.6.8 Example: One-Dimensional Matrix-Vector Multiplication
. Ab  = x , i.e., b  with a vector A  matrix n  x n Our first message-passing program using collective communications will be to multiply a dense
, one way of performing this multiplication in parallel is to have each process compute different portions of the Section 8.1 As discussed in
. This algorithm can x  consecutive elements of p/ n  processes is responsible for computing p . In particular, each one of the x product-vector
 rows that correspond to p/ n  in a row-wise fashion, such that each process receives the A be implemented in MPI by distributing the matrix
. x  is distributed in a fashion similar to b  it computes. Vector x the portion of the product-vector
. The dimension of the matrices is supplied in the A  shows the MPI program that uses a row-wise distribution of matrix Program 6.4
 points to x , respectively, and the parameter b  and vector A  point to the locally stored portions of matrix b  and a , the parameters n parameter
 is a multiple of the number of processors. n the local portion of the output matrix-vector product. This program assumes that
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Program 6.4 Row-wise Matrix-Vector Multiplication
 1 RowMatrixVectorMultiply(int n, double *a, double *b, double *x,
 2 MPI_Comm comm)
 3 {
 4 int i, j;
/* Number of locally stored rows of A */  5 int nlocal;
/* Will point to a buffer that stores the entire vector b */  6 double *fb;
 7 int npes, myrank;
 8 MPI_Status status;
 9
/* Get information about the communicator */ 10
11 MPI_Comm_size(comm, &npes);
12 MPI_Comm_rank(comm, &myrank);
13
/* Allocate the memory that will store the entire vector b */ 14
15 fb = (double *)malloc(n*sizeof(double));
16
17 nlocal = n/npes;
18
/* Gather the entire vector b on each processor using MPI's ALLGATHER operation */ 19
20 MPI_Allgather(b, nlocal, MPI_DOUBLE, fb, nlocal, MPI_DOUBLE,
21 comm);
22
/* Perform the matrix-vector multiplication involving the locally stored submatrix */ 23
24 for (i=0; i<nlocal; i++) {
25 x[i] = 0.0;
26 for (j=0; j<n; j++)
27 x[i] += a[i*n+j]*fb[j];
28 }
29
30 free(fb);
31 }
, i x . That is, for each element x  is to parallelize the task of performing the dot-product for each element of x An alternate way of computing
, all the processes will compute a part of it, and the result will be obtained by adding up these partial dot-products. This x of vector
, A  consecutive columns of p/ n  in a column-wise fashion. Each process gets A algorithm can be implemented in MPI by distributing matrix
x  that correspond to these columns. Furthermore, at the end of the computation we want the product-vector b and the elements of vector
 shows the MPI program that implements this column-wise distribution of the Program 6.5 . b to be distributed in a fashion similar to vector
matrix.
Program 6.5 Column-wise Matrix-Vector Multiplication
 1 ColMatrixVectorMultiply(int n, double *a, double *b, double *x,
 2 MPI_Comm comm)
 3 {
 4 int i, j;
 5 int nlocal;
 6 double *px;
 7 double *fx;
 8 int npes, myrank;
 9 MPI_Status status;
10
/* Get identity and size information from the communicator */ 11
12 MPI_Comm_size(comm, &npes);
13 MPI_Comm_rank(comm, &myrank);
14
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
15 nlocal = n/npes;
16
/* Allocate memory for arrays storing intermediate results. */ 17
18 px = (double *)malloc(n*sizeof(double));
19 fx = (double *)malloc(n*sizeof(double));
20
/* Compute the partial-dot products that correspond to the local columns of A.*/ 21
22 for (i=0; i<n; i++) {
23 px[i] = 0.0;
24 for (j=0; j<nlocal; j++)
25 px[i] += a[i*nlocal+j]*b[j];
26 }
27
/* Sum-up the results by performing an element-wise reduction operation */ 28
29 MPI_Reduce(px, fx, n, MPI_DOUBLE, MPI_SUM, 0, comm);
30
/* Redistribute fx in a fashion similar to that of vector b */ 31
32 MPI_Scatter(fx, nlocal, MPI_DOUBLE, x, nlocal, MPI_DOUBLE, 0,
33 comm);
34
35 free(px); free(fx);
36 }
Comparing these two programs for performing matrix-vector multiplication we see that the row-wise version needs to perform only a
 operation. In general, a MPI_Scatter  and a MPI_Reduce  operation whereas the column-wise program needs to perform a MPI_Allgather
). However, many times, an application Problem 6.6 row-wise distribution is preferable as it leads to small communication overhead (see
T A , but the computation of Ax . In that case, the row-wise distribution can be used to compute x T A  but also Ax needs to compute not only
). It is much cheaper T A  is a column-wise distribution of its transpose A  requires the column-wise distribution (a row-wise distribution of x
to use the program for the column-wise distribution than to transpose the matrix and then use the row-wise program. We must also note
that using a dual of the all-gather operation, it is possible to develop a parallel formulation for column-wise distribution that is as fast as
). However, this dual operation is not available in MPI. Problem 6.7 the program using row-wise distribution (see
6.6.9 Example: Single-Source Shortest-Path
Our second message-passing program that uses collective communication operations computes the shortest paths from a source-vertex
. This program is Section 10.3  to all the other vertices in a graph using Dijkstra's single-source shortest-path algorithm described in s
shown in Program 6.6.
 stores the vertex from which we want to source  stores the total number of vertices in the graph, and the parameter n The parameter
 points to the locally stored portion of the weighted adjacency matrix of the wgt compute the single-source shortest path. The parameter
 to the locally stored vertices. source  points to a vector that will store the length of the shortest paths from lengths graph. The parameter
 is the communicator to be used by the MPI routines. Note that this routine assumes that the number of comm Finally, the parameter
vertices is a multiple of the number of processors.
Program 6.6 Dijkstra's Single-Source Shortest-Path
[View full width]
 1 SingleSource(int n, int source, int *wgt, int *lengths, MPI_Comm comm)
 2 {
 3 int i, j;
/* The number of vertices stored locally */  4 int nlocal;
*/ o /* Used to mark the vertices belonging to V  5 int *marker;
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
/* The index number of the first vertex that is stored locally */  6 int firstvtx;
/* The index number of the last vertex that is stored locally */  7 int lastvtx;
 8 int u, udist;
 9 int lminpair[2], gminpair[2];
10 int npes, myrank;
11 MPI_Status status;
12
13 MPI_Comm_size(comm, &npes);
14 MPI_Comm_rank(comm, &myrank);
15
16 nlocal = n/npes;
17 firstvtx = myrank*nlocal;
18 lastvtx = firstvtx+nlocal-1;
19
/* Set the initial distances from source to all the other vertices */ 20
21 for (j=0; j<nlocal; j++)
22 lengths[j] = wgt[source*nlocal + j];
23
/* This array is used to indicate if the shortest part to a vertex has been found 24
or not. */
is one, then the shortest path to v has been found, otherwise it ] v  [ /* if marker 25
has not. */
26 marker = (int *)malloc(nlocal*sizeof(int));
27 for (j=0; j<nlocal; j++)
28 marker[j] = 1;
29
/* The process that stores the source vertex, marks it as being seen */ 30
31 if (source >= firstvtx && source <= lastvtx)
32 marker[source-firstvtx] = 0;
33
/* The main loop of Dijkstra's algorithm */ 34
35 for (i=1; i<n; i++) {
Step 1: Find the local vertex that is at the smallest distance from source */ /* 36
/* set it to an architecture dependent large number */ 37 lminpair[0] = MAXINT;
38 lminpair[1] = -1;
39 for (j=0; j<nlocal; j++) {
40 if (marker[j] && lengths[j] < lminpair[0]) {
41 lminpair[0] = lengths[j];
42 lminpair[1] = firstvtx+j;
43 }
44 }
45
*/ c Step 2: Compute the global minimum vertex, and insert it into V /* 46
47 MPI_Allreduce(lminpair, gminpair, 1, MPI_2INT, MPI_MINLOC,
48 comm);
49 udist = gminpair[0];
50 u = gminpair[1];
51
/* The process that stores the minimum vertex, marks it as being seen */ 52
53 if (u == lminpair[1])
54 marker[u-firstvtx] = 0;
55
Step 3: Update the distances given that u got inserted */ /* 56
57 for (j=0; j<nlocal; j++) {
58 if (marker[j] && udist + wgt[u*nlocal+j] < lengths[j])
59 lengths[j] = udist + wgt[u*nlocal+j];
60 }
61 }
62
63 free(marker);
64 }
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The main computational loop of Dijkstra's parallel single-source shortest path algorithm performs three steps. First, each process finds
 that has the smallest distance from the source. Second, the vertex that has the smallest distance over all o V the locally stored vertex in
. Third, all processes update their distance arrays to reflect the inclusion of the new c V processes is determined, and it is included in
. c V vertex in
] value. v[ lengths  with the smaller v  and determining the one vertex o V The first step is performed by scanning the locally stored vertices in
[1] lminpair [0] stores the distance of the vertex, and lminpair . In particular, lminpair The result of this computation is stored in the array
stores the vertex itself. The reason for using this storage scheme will become clear when we consider the next step, in which we must
compute the vertex that has the smallest overall distance from the source. We can find the overall shortest distance by performing a
[0]. However, in addition to the shortest distance, we also need to know the vertex lminpair min-reduction on the distance values stored in
 which returns both the MPI_MINLOC that is at that shortest distance. For this reason, the appropriate reduction operation is the
 to lminpair  we use the two-element array MPI_MINLOC minimum as well as an index value associated with that minimum. Because of
store the distance as well as the vertex that achieves this distance. Also, because the result of the reduction operation is needed by all
 operation to perform the reduction. The result of the reduction MPI_Allreduce the processes to perform the third step, we use the
 array. The third and final step during each iteration is performed by scanning the local vertices that gminpair operation is returned in the
 and updating their shortest distances from the source vertex. o V belong in
 to each processor and in each iteration it uses the W  consecutive columns of p/ n  assigns Program 6.6 Avoiding Load Imbalances
) and i , a  operation for the pairs ( MPI_MINLOC . Recall that the c V  reduction operation to select the vertex v to be included in MPI_MINLOC
) will return the one that has the smaller index (since both of them have the same value). Consequently, among the vertices that are j , a (
equally close to the source vertex, it favors the smaller numbered vertices. This may lead to load imbalances, because vertices stored in
o V  faster than vertices in higher-ranked processes (especially when many vertices in c V lower-ranked processes will tend to be included in
 will be larger in higher-ranked processes, o V are at the same minimum distance from the source). Consequently, the size of the set
dominating the overall runtime.
th p  gets everyi  using a cyclic distribution. In this distribution process W One way of correcting this problem is to distribute the columns of
 vertices to each process but these vertices have indices that span almost the p/ n . This scheme also assigns i vertex starting from vertex
 does not lead to load-imbalance MPI_MINLOC entire graph. Consequently, the preference given to lower-numbered vertices by
problems.
6.6.10 Example: Sample Sort
 elements using the sample n  of A The last problem requiring collective communications that we will consider is that of sorting a sequence
. Program 6.7 . The program is shown in Section 9.5 sort algorithm described in
 function takes as input the sequence of elements stored at each process and returns a pointer to an array that stores SampleSort The
 function are integers and SampleSort the sorted sequence as well as the number of elements in this sequence. The elements of this
 and a pointer to the array n they are sorted in increasing order. The total number of elements to be sorted is specified by the parameter
 will store the number of elements nsorted . On return, the parameter elmnts that stores the local portion of these elements is specified by
 is a multiple of the number of processes. n in the returned sorted array. This routine assumes that
Program 6.7 Samplesort
[View full width]
 1 int *SampleSort(int n, int *elmnts, int *nsorted, MPI_Comm comm)
 2 {
 3 int i, j, nlocal, npes, myrank;
 4 int *sorted_elmnts, *splitters, *allpicks;
 5 int *scounts, *sdispls, *rcounts, *rdispls;
 6
/* Get communicator-related information */  7
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 8 MPI_Comm_size(comm, &npes);
 9 MPI_Comm_rank(comm, &myrank);
10
11 nlocal = n/npes;
12
/* Allocate memory for the arrays that will store the splitters */ 13
14 splitters = (int *)malloc(npes*sizeof(int));
15 allpicks = (int *)malloc(npes*(npes-1)*sizeof(int));
16
/* Sort local array */ 17
18 qsort(elmnts, nlocal, sizeof(int), IncOrder);
19
/* Select local npes-1 equally spaced elements */ 20
21 for (i=1; i<npes; i++)
22 splitters[i-1] = elmnts[i*nlocal/npes];
23
/* Gather the samples in the processors */ 24
25 MPI_Allgather(splitters, npes-1, MPI_INT, allpicks, npes-1,
26 MPI_INT, comm);
27
/* sort these samples */ 28
29 qsort(allpicks, npes*(npes-1), sizeof(int), IncOrder);
30
/* Select splitters */ 31
32 for (i=1; i<npes; i++)
33 splitters[i-1] = allpicks[i*npes];
34 splitters[npes-1] = MAXINT;
35
/* Compute the number of elements that belong to each bucket */ 36
37 scounts = (int *)malloc(npes*sizeof(int));
38 for (i=0; i<npes; i++)
39 scounts[i] = 0;
40
41 for (j=i=0; i<nlocal; i++) {
42 if (elmnts[i] < splitters[j])
43 scounts[j]++;
44 else
45 scounts[++j]++;
46 }
47
/* Determine the starting location of each bucket's elements in the elmnts array */ 48
49 sdispls = (int *)malloc(npes*sizeof(int));
50 sdispls[0] = 0;
51 for (i=1; i<npes; i++)
52 sdispls[i] = sdispls[i-1]+scounts[i-1];
53
/* Perform an all-to-all to inform the corresponding processes of the number of 54
elements */
/* they are going to receive. This information is stored in rcounts array */ 55
56 rcounts = (int *)malloc(npes*sizeof(int));
57 MPI_Alltoall(scounts, 1, MPI_INT, rcounts, 1, MPI_INT, comm);
58
/* Based on rcounts determine where in the local array the data from each 59
processor */
/* will be stored. This array will store the received elements as well as the 60
final */
/* sorted sequence */ 61
62 rdispls = (int *)malloc(npes*sizeof(int));
63 rdispls[0] = 0;
64 for (i=1; i<npes; i++)
65 rdispls[i] = rdispls[i-1]+rcounts[i-1];
66
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
67 *nsorted = rdispls[npes-1]+rcounts[i-1];
68 sorted_elmnts = (int *)malloc((*nsorted)*sizeof(int));
69
/* Each process sends and receives the corresponding elements, using the 70
MPI_Alltoallv */
/* operation. The arrays scounts and sdispls are used to specify the number of 71
elements */
/* to be sent and where these elements are stored, respectively. The arrays 72
rcounts */
/* and rdispls are used to specify the number of elements to be received, and 73
where these */
/* elements will be stored, respectively. */ 74
75 MPI_Alltoallv(elmnts, scounts, sdispls, MPI_INT, sorted_elmnts,
76 rcounts, rdispls, MPI_INT, comm);
77
/* Perform the final local sort */ 78
79 qsort(sorted_elmnts, *nsorted, sizeof(int), IncOrder);
80
81 free(splitters); free(allpicks); free(scounts); free(sdispls);
82 free(rcounts); free(rdispls);
83
84 return sorted_elmnts;
85 }
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
6.7 Groups and Communicators
In many parallel algorithms, communication operations need to be restricted to certain subsets of processes. MPI provides several
mechanisms for partitioning the group of processes that belong to a communicator into subgroups each corresponding to a different
 that is defined as follows: MPI_Comm_split communicator. A general method for partitioning a graph of processes is to use
int MPI_Comm_split(MPI_Comm comm, int color, int key,
 MPI_Comm *newcomm)
. The function takes comm This function is a collective operation, and thus needs to be called by all the processes in the communicator
 into comm  as input parameters in addition to the communicator, and partitions the group of processes in the communicator key  and color
 parameter. Within each color disjoint subgroups. Each subgroup contains all processes that have supplied the same value for the
 parameter, with ties broken according to their rank in the key subgroup, the processes are ranked in the order defined by the value of the
 shows an Figure 6.7  parameter. newcomm ). A new communicator for each subgroup is returned in the comm old communicator (i.e.,
 using the values of MPI_Comm_split  function. If each process called MPI_Comm_split example of splitting a communicator using the
, then three communicators will be created, containing processes {0, 1, 2}, {3, 4, 5, 6}, and Figure 6.7  as shown in key  and color parameters
{7}, respectively.
 to split a group of processes in a communicator into MPI_Comm_split Figure 6.7. Using
subgroups.
e  In many parallel algorithms, processes are arranged in a virtual grid, and in different steps of th Splitting Cartesian Topologies
algorithm, communication needs to be restricted to a different subset of the grid. MPI provides a convenient way to partition a Cartesian
topology to form lower-dimensional grids.
 function that allows us to partition a Cartesian topology into sub-topologies that form lower-dimensional MPI_Cart_sub MPI provides the
grids. For example, we can partition a two-dimensional topology into groups, each consisting of the processes along the row or column of
 is the following: MPI_Cart_sub the topology. The calling sequence of
int MPI_Cart_sub(MPI_Comm comm_cart, int *keep_dims,
 MPI_Comm *comm_subcart)
 is true (non-zero value in C) keep_dims[i]  is used to specify how the Cartesian topology is partitioned. In particular, if keep_dims The array
th dimension is retained in the new sub-topology. For example, consider a three-dimensional topology of size 2 x 4 x 7. Ifi then the
 is {true, false, true}, then the original topology is split into four two-dimensional sub-topologies of size 2 x 7, as illustrated in keep_dims
 is {false, false, true}, then the original topology is split into eight one-dimensional topologies of size seven, keep_dims . If Figure 6.8(a)
. Note that the number of sub-topologies created is equal to the product of the number of processes along the Figure 6.8(b) illustrated in
, and the returned communicator comm_cart dimensions that are not being retained. The original topology is specified by the communicator
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 stores information about the created sub-topology. Only a single communicator is returned to each process, and for comm_subcart
processes that do not belong to the same sub-topology, the group specified by the returned communicator is different.
Figure 6.8. Splitting a Cartesian topology of size 2 x 4 x 7 into (a) four subgroups of size 2 x 1 x
7, and (b) eight subgroups of size 1 x 1 x 7.
 x 2 d  x 1 d The processes belonging to a given sub-topology can be determined as follows. Consider a three-dimensional topology of size
 is set to {true, false, true}. The group of processes that belong to the same sub-topology as the process keep_dims , and assume that 3 d
, *), where a '*' in a coordinate denotes all the possible values for this coordinate. Note also that y ) is given by (*, z , y  , x with coordinates (
 sub-topologies are created. 2 d  values, a total of 2 d since the second coordinate can take
 can be obtained from its coordinate in the original topology MPI_Cart_sub Also, the coordinate of a process in a sub-topology created by
by disregarding the coordinates that correspond to the dimensions that were not retained. For example, the coordinate of a process in the
column-based sub-topology is equal to its row-coordinate in the two-dimensional topology. For instance, the process with coordinates (2,
3) has a coordinate of (2) in the sub-topology that corresponds to the third column of the grid.
6.7.1 Example: Two-Dimensional Matrix-Vector Multiplication
 using a row- and column-wise Ab  = x , we presented two programs for performing the matrix-vector multiplication Section 6.6.8 In
 is to use a two-dimensional distribution, A , an alternative way of distributing matrix Section 8.1.2 distribution of the matrix. As discussed in
giving rise to the two-dimensional parallel formulations of the matrix-vector multiplication algorithm.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 shows how these topologies and their partitioning are used to implement the two-dimensional matrix-vector multiplication. Program 6.8
 and vector A  point to the locally stored portions of matrix b  and a , the parameters n The dimension of the matrix is supplied in the parameter
 points to the local portion of the output matrix-vector product. Note that only the processes along the x , respectively, and the parameter b
. For simplicity, the x  initially, and that upon return, the same set of processes will store the result b first column of the process grid will store
.  is a multiple of n  is a perfect square and that p program assumes that the number of processes
Program 6.8 Two-Dimensional Matrix-Vector Multiplication
[View full width]
 1 MatrixVectorMultiply_2D(int n, double *a, double *b, double *x,
 2 MPI_Comm comm)
 3 {
/* Improve readability */  4 int ROW=0, COL=1;
 5 int i, j, nlocal;
/* Will store partial dot products */  6 double *px;
 7 int npes, dims[2], periods[2], keep_dims[2];
 8 int myrank, my2drank, mycoords[2];
 9 int other_rank, coords[2];
10 MPI_Status status;
11 MPI_Comm comm_2d, comm_row, comm_col;
12
/* Get information about the communicator */ 13
14 MPI_Comm_size(comm, &npes);
15 MPI_Comm_rank(comm, &myrank);
16
/* Compute the size of the square grid */ 17
18 dims[ROW] = dims[COL] = sqrt(npes);
19
20 nlocal = n/dims[ROW];
21
/* Allocate memory for the array that will hold the partial dot-products */ 22
23 px = malloc(nlocal*sizeof(double));
24
/* Set up the Cartesian topology and get the rank & coordinates of the process in 25
this topology */
/* Set the periods for wrap-around connections */ 26 periods[ROW] = periods[COL] = 1;
27
28 MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &comm_2d);
29
/* Get my rank in the new topology */ 30 MPI_Comm_rank(comm_2d, &my2drank);
/* Get my coordinates */ 31 MPI_Cart_coords(comm_2d, my2drank, 2, mycoords);
32
/* Create the row-based sub-topology */ 33
34 keep_dims[ROW] = 0;
35 keep_dims[COL] = 1;
36 MPI_Cart_sub(comm_2d, keep_dims, &comm_row);
37
/* Create the column-based sub-topology */ 38
39 keep_dims[ROW] = 1;
40 keep_dims[COL] = 0;
41 MPI_Cart_sub(comm_2d, keep_dims, &comm_col);
42
/* Redistribute the b vector. */ 43
Step 1. The processors along the 0th column send their data to the diagonal /* 44
processors */
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
/* I'm in the first column */ 45 if (mycoords[COL] == 0 && mycoords[ROW] != 0) {
46 coords[ROW] = mycoords[ROW];
47 coords[COL] = mycoords[ROW];
48 MPI_Cart_rank(comm_2d, coords, &other_rank);
49 MPI_Send(b, nlocal, MPI_DOUBLE, other_rank, 1, comm_2d);
50 }
51 if (mycoords[ROW] == mycoords[COL] && mycoords[ROW] != 0) {
52 coords[ROW] = mycoords[ROW];
53 coords[COL] = 0;
54 MPI_Cart_rank(comm_2d, coords, &other_rank);
55 MPI_Recv(b, nlocal, MPI_DOUBLE, other_rank, 1, comm_2d,
56 &status);
57 }
58
Step 2. The diagonal processors perform a column-wise broadcast */ /* 59
60 coords[0] = mycoords[COL];
61 MPI_Cart_rank(comm_col, coords, &other_rank);
62 MPI_Bcast(b, nlocal, MPI_DOUBLE, other_rank, comm_col);
63
/* Get into the main computational loop */ 64
65 for (i=0; i<nlocal; i++) {
66 px[i] = 0.0;
67 for (j=0; j<nlocal; j++)
68 px[i] += a[i*nlocal+j]*b[j];
69 }
70
/* Perform the sum-reduction along the rows to add up the partial dot-products */ 71
72 coords[0] = 0;
73 MPI_Cart_rank(comm_row, coords, &other_rank);
74 MPI_Reduce(px, x, nlocal, MPI_DOUBLE, MPI_SUM, other_rank,
75 comm_row);
76
/* Free up communicator */ 77 MPI_Comm_free(&comm_2d);
/* Free up communicator */ 78 MPI_Comm_free(&comm_row);
/* Free up communicator */ 79 MPI_Comm_free(&comm_col);
80
81 free(px);
82 }
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
6.8 Bibliographic Remarks
]. At the time of writing of this book, there Mes94 The best source for information about MPI is the actual reference of the library itself [
have been two major releases of the MPI standard. The first release, version 1.0, was released in 1994 and its most recent revision,
], Mes97 version 1.2, has been implemented by the majority of hardware vendors. The second release of the MPI standard, version 2.0 [
contains numerous significant enhancements over version 1.x, such as one-sided communication, dynamic process creation, and
extended collective operations. However, despite the fact that the standard was voted in 1997, there are no widely available MPI-2
implementations that support the entire set of features specified in that standard. In addition to the above reference manuals, a number
]. GLS99 , GSNL98 , Pac98 of books have been written that focus on parallel programming using MPI [
In addition to MPI implementations provided by various hardware vendors, there are a number of publicly available MPI implementations
] GL96b , GLDS96 that were developed by various government research laboratories and universities. Among them, the MPICH [
) distributed by Argonne National Laboratories and the LAM-MPI (available at http://www-unix.mcs.anl.gov/mpi/mpich (available at
) distributed by Indiana University are widely used and are portable to a number of different architectures. In fact, http://www.lam-mpi.org
these implementations of MPI have been used as the starting point for a number of specialized MPI implementations that are suitable for
off-the-shelf high-speed interconnection networks such as those based on gigabit Ethernet and Myrinet networks.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
 Describe a message-transfer protocol for buffered sends and receives in which the buffering is performed only 6.1
by the sending process. What kind of additional hardware support is needed to make these types of protocols
practical?
 One of the advantages of non-blocking communication operations is that they allow the transmission of the 6.2
data to be done concurrently with computations. Discuss the type of restructuring that needs to be performed on a
program to allow for the maximal overlap of computation with communication. Is the sending process in a better
position to benefit from this overlap than the receiving process?
MPI_Send  the MPI standard allows for two different implementations of the Section 6.3.4  As discussed in 6.3
s operation – one using buffered-sends and the other using blocked-sends. Discuss some of the potential reason
s why MPI allows these two different implementations. In particular, consider the cases of different message-size
. and/or different architectural characteristics
. Show Figure 6.5  Consider the various mappings of 16 processors on a 4 x 4 two-dimensional grid shown in 6.4
 processors will be mapped using each one of these four schemes. how
 Consider Cannon's matrix-matrix multiplication algorithm. Our discussion of Cannon's algorithm has been 6.5
 are square matrices, mapped onto a square grid of processes. However, B  and A limited to cases in which
, and the process grid are not square. In particular, let B , A Cannon's algorithm can be extended for cases in which
 x n  is of size B  and A  obtained by multiplying C . The matrix m  x k  be of size B  and matrix k  x n  be of size A matrix
 columns. Develop an MPI r  rows and q  be the number of processes in the grid arranged in r  x q . Also, let m
 process grid using Cannon's algorithm. r  x q program for multiplying two such matrices on a
) needs to be changed so that it will Program 6.4  Show how the row-wise matrix-vector multiplication program ( 6.6
work correctly in cases in which the dimension of the matrix does not have to be a multiple of the number of
processes.
). An alternate implementation Program 6.5  Consider the column-wise implementation of matrix-vector product ( 6.7
 to perform the required reduction operation and then have each process copy the MPI_Allreduce will be to use
. What will be the cost of this implementation? Another fx  from the vector x locally stored elements of vector
 single-node reduction operations using a different process as the root. What p implementation can be to perform
will be the cost of this implementation?
. Describe why a Section 6.6.9  Consider Dijkstra's single-source shortest-path algorithm described in 6.8
column-wise distribution is preferable to a row-wise distribution of the weighted adjacency matrix.
) needs to be changed so Program 6.8  Show how the two-dimensional matrix-vector multiplication program ( 6.9
 process grid. r  x q  on a m  x n that it will work correctly for a matrix of size
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 7. Programming Shared Address Space
Platforms
Explicit parallel programming requires specification of parallel tasks along with their interactions. These interactions may be in the form of
synchronization between concurrent tasks or communication of intermediate results. In shared address space architectures,
communication is implicitly specified since some (or all) of the memory is accessible to all the processors. Consequently, programming
paradigms for shared address space machines focus on constructs for expressing concurrency and synchronization along with
techniques for minimizing associated overheads. In this chapter, we discuss shared-address-space programming paradigms along with
their performance issues and related extensions to directive-based paradigms.
Shared address space programming paradigms can vary on mechanisms for data sharing, concurrency models, and support for
synchronization. Process based models assume that all data associated with a process is private, by default, unless otherwise specified
). While this is important for ensuring protection in multiuser systems, it is not shmat  and shmget (using UNIX system calls such as
necessary when multiple concurrent aggregates are cooperating to solve the same problem. The overheads associated with enforcing
protection domains make processes less suitable for parallel programming. In contrast, lightweight processes and threads assume that
all memory is global. By relaxing the protection domain, lightweight processes and threads support much faster manipulation. As a result,
this is the preferred model for parallel programming and forms the focus of this chapter. Directive based programming models extend the
threaded model by facilitating creation and synchronization of threads. In this chapter, we study various aspects of programming using
threads and parallel directives.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.1 Thread Basics
 is a single stream of control in the flow of a program. We initiate threads with a simple example: thread A
Example 7.1 What are threads?
. n  x n Consider the following code segment that computes the product of two dense matrices of size
1 for (row = 0; row < n; row++)
2 for (column = 0; column < n; column++)
3 c[row][column] =
4 dot_product(get_row(a, row),
5 get_col(b, col));
 iterations, each of which can be executed independently. Such an 2 n  loop in this code fragment has for The
 threads, 2 n independent sequence of instructions is referred to as a thread. In the example presented above, there are
one for each iteration of the for-loop. Since each of these threads can be executed independently of the others, they
can be scheduled concurrently on multiple processors. We can transform the above code segment as follows:
1 for (row = 0; row < n; row++)
2 for (column = 0; column < n; column++)
3 c[row][column] =
4 create_thread(dot_product(get_row(a, row),
5 get_col(b, col)));
, to provide a mechanism for specifying a C function as a thread. The underlying create_thread Here, we use a function,
system can then schedule these threads on multiple processors.
 on multiple processors, each processor must have Example 7.1  To execute the code fragment in Logical Memory Model of a Thread
). All memory in the logical Chapter 2 . This is accomplished via a shared address space (described in c , and b , a access to matrices
. However, since threads are invoked as Figure 7.1(a) machine model of a thread is globally accessible to every thread as illustrated in
function calls, the stack corresponding to the function call is generally treated as being local to the thread. This is due to the liveness
 schedule of their execution can be safely assumed), a priori considerations of the stack. Since threads are scheduled at runtime (and no
it is not possible to determine which stacks are live. Therefore, it is considered poor programming practice to treat stacks (thread-local
 hold thread-local M , where memory modules Figure 7.1(b) variables) as global data. This implies a logical machine model illustrated in
(stack allocated) data.
Figure 7.1. The logical machine model of a thread-based programming paradigm.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
While this logical machine model gives the view of an equally accessible address space, physical realizations of this model deviate from
this assumption. In distributed shared address space machines such as the Origin 2000, the cost to access a physically local memory
may be an order of magnitude less than that of accessing remote memory. Even in architectures where the memory is truly equally
accessible to all processors (such as shared bus architectures with global shared memory), the presence of caches with processors
skews memory access time. Issues of locality of memory reference become important for extracting performance from such
architectures.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.2 Why Threads?
Threaded programming models offer significant advantages over message-passing programming models along with some
disadvantages as well. Before we discuss threading APIs, let us briefly look at some of these.
.  Threaded applications can be developed on serial machines and run on parallel machines without any changes Software Portability
This ability to migrate programs between diverse architectural platforms is a very significant advantage of threaded APIs. It has
implications not just for software utilization but also for application development since supercomputer time is often scarce and expensive.
d  One of the major overheads in programs (both serial and parallel) is the access latency for memory access, I/O, an Latency Hiding
communication. By allowing multiple threads to execute on the same processor, threaded APIs enable this latency to be hidden (as seen
). In effect, while one thread is waiting for a communication operation, other threads can utilize the CPU, thus masking Chapter 2 in
associated overhead.
a  While writing shared address space parallel programs, a programmer must express concurrency in Scheduling and Load Balancing
way that minimizes overheads of remote interaction and idling. While in many structured applications the task of allocating equal work to
processors is easily accomplished, in unstructured and dynamic applications (such as game playing and discrete optimization) this task
is more difficult. Threaded APIs allow the programmer to specify a large number of concurrent tasks and support system-level dynamic
mapping of tasks to processors with a view to minimizing idling overheads. By providing this support at the system level, threaded APIs
rid the programmer of the burden of explicit scheduling and load balancing.
e  Due to the aforementioned advantages, threaded programs are significantly easier to writ Ease of Programming, Widespread Use
than corresponding programs using message passing APIs. Achieving identical levels of performance for the two programs may require
additional effort, however. With widespread acceptance of the POSIX thread API, development tools for POSIX threads are more widely
available and stable. These issues are important from the program development and software engineering aspects.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.3 The POSIX Thread API
A number of vendors provide vendor-specific thread APIs. The IEEE specifies a standard 1003.1c-1995, POSIX API. Also referred to as
Pthreads, POSIX has emerged as the standard threads API, supported by most vendors. We will use the Pthreads API for introducing
multithreading concepts. The concepts themselves are largely independent of the API and can be used for programming with other
thread APIs (NT threads, Solaris threads, Java threads, etc.) as well. All the illustrative programs presented in this chapter can be
executed on workstations as well as parallel computers that support Pthreads.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.4 Thread Basics: Creation and Termination
. p Let us start our discussion with a simple threaded program for computing the value of
p Example 7.2 Threaded program for computing
The method we use here is based on generating random numbers in a unit length square and counting the number of
/4, and the p ) is equal to 2  r p points that fall within the largest circle inscribed in the square. Since the area of the circle (
/4. p area of the square is 1 x 1, the fraction of random points that fall in the circle should approach
 assigns a fixed number of points to each thread. Each thread p A simple threaded strategy for generating the value of
generates these random points and keeps track of the number of points that land in the circle locally. After all threads
 (by calculating the fraction over all threads and p finish execution, their counts are combined to compute the value of
multiplying by 4).
To implement this threaded program, we need a function for creating threads and waiting for all threads to finish execution
. The prototype pthread_create (so we can accrue count). Threads can be created in the Pthreads API using the function
of this function is:
1 #include <pthread.h>
2 int
3 pthread_create (
4 pthread_t *thread_handle,
5 const pthread_attr_t *attribute,
6 void * (*thread_function)(void *),
7 void *arg);
 (and thread_function  function creates a single thread that corresponds to the invocation of the function pthread_create The
). On successful creation of a thread, a unique identifier is associated with thread_function any other functions called by
. The thread has the attributes described by the thread_handle the thread and assigned to the location pointed to by
attribute  argument. When this argument is NULL, a thread with default attributes is created. We will discuss the attribute
. This thread_function  field specifies a pointer to the argument to function arg . The Section 7.6 parameter in detail in
 example, it compute_pi argument is typically used to pass the workspace and other thread-specific data to a thread. In the
 variable is written before the the thread_handle is used to pass an integer id that is used as a seed for randomization. The
 returns; and the new thread is ready for execution as soon as it is created. If the thread is pthread_create function
scheduled on the same processor, the new thread may, in fact, preempt its creator. This is important to note because all
thread initialization procedures must be completed before creating the thread. Otherwise, errors may result based on
thread scheduling. This is a very common class of errors caused by race conditions for data access that shows itself in
 returns 0; else it returns pthread_create some execution instances, but not in others. On successful creation of a thread,
an error code. The reader is referred to the Pthreads specification for a detailed description of the error-codes.
, and the desired num threads , we first read in the desired number of threads, p In our program for computing the value of
. These points are divided equally among the threads. The program uses an sample_points number of sample points,
, for assigning an integer id to each thread (this id is used as a seed for randomizing the random number hits array,
generator). The same array is used to keep track of the number of hits (points inside the circle) encountered by each
, using the compute_pi  threads, each invoking the function num_threads thread upon return. The program creates
 function. pthread_create
 threads have generated assigned number of random points and computed their hit compute_pi Once the respective
. The main program must wait for the threads to run to completion. p ratios, the results must be combined to determine
 which suspends execution of the calling thread until the specified thread pthread_join This is done using the function
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 function is as follows: pthread_join terminates. The prototype of the
1 int
2 pthread_join (
3 pthread_t thread,
4 void **ptr);
. On a successful call to thread A call to this function waits for the termination of the thread whose id is given by
. On successful completion, ptr  is returned in the location pointed to by pthread_exit , the value passed to pthread_join
 returns 0, else it returns an error-code. pthread_join
 is computed by multiplying the combined hit ratio by 4.0. The complete p Once all threads have joined, the value of
program is as follows:
1 #include <pthread.h>
2 #include <stdlib.h>
3
4 #define MAX_THREADS 512
5 void *compute_pi (void *);
6
7 int total_hits, total_misses, hits[MAX_THREADS],
8 sample_points, sample_points_per_thread, num_threads;
9
10 main() {
11 int i;
12 pthread_t p_threads[MAX_THREADS];
13 pthread_attr_t attr;
14 double computed_pi;
15 double time_start, time_end;
16 struct timeval tv;
17 struct timezone tz;
18
19 pthread_attr_init (&attr);
20 pthread_attr_setscope (&attr,PTHREAD_SCOPE_SYSTEM);
21 printf("Enter number of sample points: ");
22 scanf("%d", &sample_points);
23 printf("Enter number of threads: ");
24 scanf("%d", &num_threads);
25
26 gettimeofday(&tv, &tz);
27 time_start = (double)tv.tv_sec +
28 (double)tv.tv_usec / 1000000.0;
29
30 total_hits = 0;
31 sample_points_per_thread = sample_points / num_threads;
32 for (i=0; i< num_threads; i++) {
33 hits[i] = i;
34 pthread_create(&p_threads[i], &attr, compute_pi,
35 (void *) &hits[i]);
36 }
37 for (i=0; i< num_threads; i++) {
38 pthread_join(p_threads[i], NULL);
39 total_hits += hits[i];
40 }
41 computed_pi = 4.0*(double) total_hits /
42 ((double)(sample_points));
43 gettimeofday(&tv, &tz);
44 time_end = (double)tv.tv_sec +
45 (double)tv.tv_usec / 1000000.0;
46
47 printf("Computed PI = %lf\n", computed_pi);
48 printf(" %lf\n", time_end - time_start);
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
49 }
50
51 void *compute_pi (void *s) {
52 int seed, i, *hit_pointer;
53 double rand_no_x, rand_no_y;
54 int local_hits;
55
56 hit_pointer = (int *) s;
57 seed = *hit_pointer;
58 local_hits = 0;
59 for (i = 0; i < sample_points_per_thread; i++) {
60 rand_no_x =(double)(rand_r(&seed))/(double)((2<<14)-1);
61 rand_no_y =(double)(rand_r(&seed))/(double)((2<<14)-1);
62 if (((rand_no_x - 0.5) * (rand_no_x - 0.5) +
63 (rand_no_y - 0.5) * (rand_no_y - 0.5)) < 0.25)
64 local_hits ++;
65 seed *= i;
66 }
67 *hit_pointer = local_hits;
68 pthread_exit(0);
69 }
 (instead of superior rand_r  The reader must note, in the above example, the use of the function Programming Notes
) are drand48  and rand ). The reason for this is that many functions (including drand48 random number generators such as
. Reentrant functions are those that can be safely called when another instance has been suspended in the reentrant not
middle of its invocation. It is easy to see why all thread functions must be reentrant because a thread can be preempted in
the middle of its execution. If another thread starts executing the same function at this point, a non-reentrant function
might not work as desired.
f  We execute this program on a four-processor SGI Origin 2000. The logarithm of the number o Performance Notes
 (the curve labeled "local"). We can see that at 32 threads, the Figure 7.2 threads and execution time are illustrated in
runtime of the program is roughly 3.91 times less than the corresponding time for one thread. On a four-processor
machine, this corresponds to a parallel efficiency of 0.98.
 program as a function of number of threads. compute_pi Figure 7.2. Execution time of the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Consider the following false sharing  illustrate an important performance overhead called Figure 7.2 The other curves in
, and assigning it to the array entry outside the local_hits change to the program: instead of incrementing a local variable,
 array. This can be done by changing line 64 to hits loop, we now directly increment the corresponding entry in the
, and deleting line 67. It is easy to verify that the program is semantically identical to the one before. *(hit_pointer) ++;
However, on executing this modified program the observed performance is illustrated in the curve labeled "spaced_1" in
. This represents a significant slowdown instead of a speedup! Figure 7.2
. In this false sharing The drastic impact of this seemingly innocuous change is explained by a phenomenon called
example, two adjoining data items (which likely reside on the same cache line) are being continually written to by threads
, we know that a write to a shared Chapter 2 that might be scheduled on different processors. From our discussion in
cache line results in an invalidate and a subsequent read must fetch the cache line from the most recent write location.
 array generate a large number of invalidates hits With this in mind, we can see that the cache lines corresponding to the
and reads because of repeated increment operations. This situation, in which two threads 'falsely' share data because it
happens to be on the same cache line, is called false sharing.
 to a hits It is in fact possible to use this simple example to estimate the cache line size of the system. We change
two-dimensional array and use only the first column of the array to store counts. By changing the size of the second
 array to lie on different cache lines (since arrays in C are hits dimension, we can force entries in the first column of the
 by curves labeled "spaced_16" and Figure 7.2 stored row-major). The results of this experiment are illustrated in
 array is 16 and 32 integers, respectively. It is evident from the hits "spaced_32", in which the second dimension of the
figure that as the entries are spaced apart, the performance improves. This is consistent with our understanding that
spacing the entries out pushes them into different cache lines, thereby reducing the false sharing overhead.
Having understood how to create and join threads, let us now explore mechanisms in Pthreads for synchronizing threads.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.5 Synchronization Primitives in Pthreads
While communication is implicit in shared-address-space programming, much of the effort associated with writing correct threaded
programs is spent on synchronizing concurrent threads with respect to their data accesses or scheduling.
7.5.1 Mutual Exclusion for Shared Variables
 calls, we can create concurrent tasks. These tasks work together to manipulate data and pthread_join  and pthread_create Using
accomplish a given task. When multiple threads attempt to manipulate the same data item, the results can often be incoherent if proper
 is my_cost care is not taken to synchronize them. Consider the following code fragment being executed by multiple threads. The variable
 is a global variable shared by all threads. best_cost thread-local and
1 /* each thread tries to update variable best_cost as follows */
2 if (my_cost < best_cost)
3 best_cost = my_cost;
To understand the problem with shared data access, let us examine one execution instance of the above code fragment. Assume that
 are 50 and 75 at threads t1 and t2, respectively. If my_cost  is 100, and the values of best_cost there are two threads, the initial value of
 part of the statement. Depending then  statement concurrently, then both threads enter the if both threads execute the condition inside the
 at the end could be either 50 or 75. There are two problems here: the first is the best_cost on which thread executes first, the value of
 is inconsistent in the sense that no best_cost non-deterministic nature of the result; second, and more importantly, the value 75 of
serialization of the two threads can possibly yield this result. This is an undesirable situation, sometimes also referred to as a race
condition (so called because the result of the computation depends on the race between competing threads).
The aforementioned situation occurred because the test-and-update operation illustrated above is an atomic operation; i.e., the operation
should not be broken into sub-operations. Furthermore, the code corresponds to a critical segment; i.e., a segment that must be
executed by only one thread at any time. Many statements that seem atomic in higher level languages such as C may in fact be
 may comprise several assembler instructions and therefore must be global_count += 5 non-atomic; for example, a statement of the form
handled carefully.
 (mutual exclusion locks). mutex-locks Threaded APIs provide support for implementing critical sections and atomic operations using
Mutex-locks have two states: locked and unlocked. At any point of time, only one thread can lock a mutex lock. A lock is an atomic
operation generally associated with a piece of code that manipulates shared data. To access the shared data, a thread must first try to
acquire a mutex-lock. If the mutex-lock is already locked, the process trying to acquire the lock is blocked. This is because a locked
mutex-lock implies that there is another thread currently in the critical section and that no other thread must be allowed in. When a
thread leaves a critical section, it must unlock the mutex-lock so that other threads can enter the critical section. All mutex-locks must be
initialized to the unlocked state at the beginning of the program.
 can be used to attempt a pthread_mutex_lock The Pthreads API provides a number of functions for handling mutex-locks. The function
lock on a mutex-lock. The prototype of the function is:
1 int
2 pthread_mutex_lock (
3 pthread_mutex_t *mutex_lock);
.) pthread_mutex_t  is predefined to be mutex_lock . (The data type of a mutex_lock A call to this function attempts a lock on the mutex-lock
If the mutex-lock is already locked, the calling thread blocks; otherwise the mutex-lock is locked and the calling thread returns. A
successful return from the function returns a value 0. Other values indicate error conditions such as deadlocks.
On leaving a critical section, a thread must unlock the mutex-lock associated with the section. If it does not do so, no other thread will be
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is used to unlock a mutex-lock. pthread_mutex_unlock able to enter this section, typically resulting in a deadlock. The Pthreads function
The prototype of this function is:
1 int
2 pthread_mutex_unlock (
3 pthread_mutex_t *mutex_lock);
On calling this function, in the case of a normal mutex-lock, the lock is relinquished and one of the blocked threads is scheduled to enter
the critical section. The specific thread is determined by the scheduling policy. There are other types of locks (other than normal locks),
. If a programmer attempts pthread_mutex_unlock  along with the associated semantics of the function Section 7.6 which are discussed in
 on a previously unlocked mutex or one that is locked by another thread, the effect is undefined. pthread_mutex_unlock a
We need one more function before we can start using mutex-locks, namely, a function to initialize a mutex-lock to its unlocked state. The
. The prototype of this function is as follows: pthread_mutex_init Pthreads function for this is
1 int
2 pthread_mutex_init (
3 pthread_mutex_t *mutex_lock,
4 const pthread_mutexattr_t *lock_attr);
. If this lock_attr  to an unlocked state. The attributes of the mutex-lock are specified by mutex_lock This function initializes the mutex-lock
argument is set to NULL, the default mutex-lock attributes are used (normal mutex-lock). Attributes objects for threads are discussed in
. Section 7.6 greater detail in
Example 7.3 Computing the minimum entry in a list of integers
Armed with basic mutex-lock functions, let us write a simple threaded program to compute the minimum of a list of
integers. The list is partitioned equally among the threads. The size of each thread's partition is stored in the variable
. The threaded list_ptr  and the pointer to the start of each thread's partial list is passed to it as the pointer partial_list_size
program for accomplishing this is as follows:
1 #include <pthread.h>
2 void *find_min(void *list_ptr);
3 pthread_mutex_t minimum_value_lock;
4 int minimum_value, partial_list_size;
5
6 main() {
7 /* declare and initialize data structures and list */
8 minimum_value = MIN_INT;
9 pthread_init();
10 pthread_mutex_init(&minimum_value_lock, NULL);
11
12 /* initialize lists, list_ptr, and partial_list_size */
13 /* create and join threads here */
14 }
15
16 void *find_min(void *list_ptr) {
17 int *partial_list_pointer, my_min, i;
18 my_min = MIN_INT;
19 partial_list_pointer = (int *) list_ptr;
20 for (i = 0; i < partial_list_size; i++)
21 if (partial_list_pointer[i] < my_min)
22 my_min = partial_list_pointer[i];
23 /* lock the mutex associated with minimum_value and
24 update the variable as required */
25 pthread_mutex_lock(&minimum_value_lock);
26 if (my_min < minimum_value)
27 minimum_value = my_min;
28 /* and unlock the mutex */
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
29 pthread_mutex_unlock(&minimum_value_lock);
30 pthread_exit(0);
31 }
 is protected by the mutex-lock minimum_value  In this example, the test-update operation for Programming Notes
. minimum_value  to gain exclusive access to the variable pthread_mutex_lock . Threads execute minimum value lock
Once this access is gained, the value is updated as required, and the lock subsequently released. Since at any point of
time, only one thread can hold a lock, only one thread can test-update the variable.
Example 7.4 Producer-consumer work queues
A common use of mutex-locks is in establishing a producer-consumer relationship between threads. The producer
creates tasks and inserts them into a work-queue. The consumer threads pick up tasks from the task queue and
execute them. Let us consider a simple instance of this paradigm in which the task queue can hold only one task (in a
general case, the task queue may be longer but is typically of bounded size). Producer-consumer relations are
ubiquitous. See Exercise 7.4 for an example application in multimedia processing. A simple (and incorrect) threaded
program would associate a producer thread with creating a task and placing it in a shared data structure and the
consumer threads with picking up tasks from this shared data structure and executing them. However, this simple
version does not account for the following possibilities:
The producer thread must not overwrite the shared buffer when the previous task has not been picked up
by a consumer thread.
The consumer threads must not pick up tasks until there is something present in the shared data structure.
Individual consumer threads should pick up tasks one at a time.
. If this variable is 0, consumer threads must wait, but task_available To implement this, we can use a variable called
 is equal to 1, the task_available . If task_queue the producer thread can insert tasks into the shared data structure
producer thread must wait to insert the task into the shared data structure but one of the consumer threads can pick up
 should be protected by mutex-locks to ensure task_available the task available. All of these operations on the variable
that only one thread is executing test-update on it. The threaded version of this program is as follows:
1 pthread_mutex_t task_queue_lock;
2 int task_available;
3
4 /* other shared data structures here */
5
6 main() {
7 /* declarations and initializations */
8 task_available = 0;
9 pthread_init();
10 pthread_mutex_init(&task_queue_lock, NULL);
11 /* create and join producer and consumer threads */
12 }
13
14 void *producer(void *producer_thread_data) {
15 int inserted;
16 struct task my_task;
17 while (!done()) {
18 inserted = 0;
19 create_task(&my_task);
20 while (inserted == 0) {
21 pthread_mutex_lock(&task_queue_lock);
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
22 if (task_available == 0) {
23 insert_into_queue(my_task);
24 task_available = 1;
25 inserted = 1;
26 }
27 pthread_mutex_unlock(&task_queue_lock);
28 }
29 }
30 }
31
32 void *consumer(void *consumer_thread_data) {
33 int extracted;
34 struct task my_task;
35 /* local data structure declarations */
36 while (!done()) {
37 extracted = 0;
38 while (extracted == 0) {
39 pthread_mutex_lock(&task_queue_lock);
40 if (task_available == 1) {
41 extract_from_queue(&my_task);
42 task_available = 0;
43 extracted = 1;
44 }
45 pthread_mutex_unlock(&task_queue_lock);
46 }
47 process_task(my_task);
48 }
49 }
s  In this example, the producer thread creates a task and waits for space on the queue. This i Programming Notes
 being 0. The test and update of this variable as well as insertion and extraction task_available indicated by the variable
. Once space is available on the task queue, task_queue_lock from the shared queue are protected by a mutex called
the recently created task is inserted into the task queue and the availability of the task is signaled by setting
 to 1. Within the producer thread, the fact that the recently created task has been inserted into the queue task_available
 being set to 1, which allows the producer to produce the next task. Irrespective of inserted is signaled by the variable
whether a recently created task is successfully inserted into the queue or not, the lock is relinquished. This allows
consumer threads to pick up work from the queue in case there is work on the queue to begin with. If the lock is not
relinquished, threads would deadlock since a consumer would not be able to get the lock to pick up the task and the
producer would not be able to insert its task into the task queue. The consumer thread waits for a task to become
available and executes it when available. As was the case with the producer thread, the consumer relinquishes the
 loop to allow the producer to insert work into the queue if there was none. while lock in each iteration of the
Overheads of Locking
Locks represent serialization points since critical sections must be executed by threads one after the other. Encapsulating large
segments of the program within locks can, therefore, lead to significant performance degradation. It is important to minimize the size of
 functions are left outside the critical region, but process_task  and create_task critical sections. For instance, in the above example, the
 functions are left inside the critical region. The former is left out in the interest of making the extract_from_queue  and insert_into_queue
 functions are left inside because if the lock is extract_from_queue  and insert_into_queue critical section as small as possible. The
 but not inserting or extracting the task, other threads may gain access to the shared data task_available relinquished after updating
structure while the insertion or extraction is in progress, resulting in errors. It is therefore important to handle critical sections and shared
data structures with extreme care.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Alleviating Locking Overheads
. This function pthread_mutex_trylock It is often possible to reduce the idling overhead associated with locks using an alternate function,
. If the lock is successful, the function returns a zero. If it is already locked by another thread, instead of mutex_lock attempts a lock on
. This allows the thread to do other work and to poll the mutex for a lock. EBUSY blocking the thread execution, it returns a value
 on typical systems since it does not have to deal pthread_mutex_lock  is typically much faster than pthread_mutex_trylock Furthermore,
 is: pthread_mutex_trylock with queues associated with locks for multiple threads waiting on the lock. The prototype of
1 int
2 pthread_mutex_trylock (
3 pthread_mutex_t *mutex_lock);
We illustrate the use of this function using the following example:
Example 7.5 Finding k matches in a list
 matches to a query item in a given list. The list is partitioned equally among the k We consider the example of finding
 entries of the list. n/p  threads is responsible for searching p  entries, each of the n threads. Assuming that the list has
 function is as follows: pthread_mutex_lock The program segment for computing this using the
1 void *find_entries(void *start_pointer) {
2
3 /* This is the thread function */
4
5 struct database_record *next_record;
6 int count;
7 current_pointer = start_pointer;
8 do {
9 next_record = find_next_entry(current_pointer);
10 count = output_record(next_record);
11 } while (count < requested_number_of_records);
12 }
13
14 int output_record(struct database_record *record_ptr) {
15 int count;
16 pthread_mutex_lock(&output_count_lock);
17 output_count ++;
18 count = output_count;
19 pthread_mutex_unlock(&output_count_lock);
20
21 if (count <= requested_number_of_records)
22 print_record(record_ptr);
23 return (count);
24 }
This program segment finds an entry in its part of the database, updates the global count and then finds the next entry.
, then the total time for satisfying 2t  and the time to find an entry is 1 t If the time for a lock-update count-unlock cycle is
 are 2t  and 1 t  is the maximum number of entries found by any thread. If max n  , where max n ) x 2t  + 1 t the query is (
comparable, then locking leads to considerable overhead.
. Each thread now finds the next pthread_mutex_trylock This locking overhead can be alleviated by using the function
entry and tries to acquire the lock and update count. If another thread already has the lock, the record is inserted into a
local list and the thread proceeds to find other matches. When it finally gets the lock, it inserts all entries found locally
thus far into the list (provided the number does not exceed the desired number of entries). The corresponding
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 function is as follows: output_record
1 int output_record(struct database_record *record_ptr) {
2 int count;
3 int lock_status;
4 lock_status = pthread_mutex_trylock(&output_count_lock);
5 if (lock_status == EBUSY) {
6 insert_into_local_list(record_ptr);
7 return(0);
8 }
9 else {
10 count = output_count;
11 output_count += number_on_local_list + 1;
12 pthread_mutex_unlock(&output_count_lock);
13 print_records(record_ptr, local_list,
14 requested_number_of_records - count);
15 return(count + number_on_local_list + 1);
16 }
17 }
t  Examining this function closely, we notice that if the lock for updating the global count is no Programming Notes
available, the function inserts the current record into a local list and returns. If the lock is available, it increments the
global count by the number of records on the local list, and then by one (for the current record). It then unlocks the
. print_records associated lock and proceeds to print as many records as are required using the function
s  The time for execution of this version is less than the time for the first one on two counts: first, a Performance Notes
 is typically much smaller than that for a pthread_mutex_trylock mentioned, the time for executing a
. Second, since multiple records may be inserted on each lock, the number of locking operations is pthread_mutex_lock
also reduced. The number of records actually searched (across all threads) may be slightly larger than the number of
records actually desired (since there may be entries in the local lists that may never be printed). However, since this
time would otherwise have been spent idling for the lock anyway, this overhead does not cause a slowdown.
. The general use of the pthread_mutex_lock  instead of pthread_mutex_trylock The above example illustrates the use of the function
function is in reducing idling overheads associated with mutex-locks. If the computation is such that the critical section can be delayed
is the function of choice. Another determining factor, as pthread mutex_trylock_ and other computations can be performed in the interim,
 is a much cheaper function than pthread_mutex_trylock has been mentioned, is the fact that for most implementations
 inside a pthread_mutex_trylock  is required, a pthread_mutex_lock . In fact, for highly optimized codes, even when a pthread_mutex_lock
. pthread_mutex_lock loop may often be desirable, since if the lock is acquired within the first few calls, it would be cheaper than a
7.5.2 Condition Variables for Synchronization
As we noted in the previous section, indiscriminate use of locks can result in idling overhead from blocked threads. While the function
 alleviates this overhead, it introduces the overhead of polling for availability of locks. For example, if the pthread_mutex_trylock
, the producer and consumer pthread_mutex_lock  instead of pthread_mutex_trylock producer-consumer example is rewritten using
threads would have to periodically poll for availability of lock (and subsequently availability of buffer space or tasks on queue). A natural
solution to this problem is to suspend the execution of the producer until space becomes available (an interrupt driven mechanism as
opposed to a polled mechanism). The availability of space is signaled by the consumer thread that consumes the task. The functionality
. condition variable to accomplish this is provided by a
A condition variable is a data object used for synchronizing threads. This variable allows a thread to block itself until specified data
 must become 1 before the consumer task_available reaches a predefined state. In the producer-consumer case, the shared variable
 is referred to as a predicate. A condition variable is associated with task_available == 1 threads can be signaled. The boolean condition
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
this predicate. When the predicate becomes true, the condition variable is used to signal one or more threads waiting on the condition. A
single condition variable may be associated with more than one predicate. However, this is strongly discouraged since it makes the
program difficult to debug.
A condition variable always has a mutex associated with it. A thread locks this mutex and tests the predicate defined on the shared
); if the predicate is not true, the thread waits on the condition variable associated with the predicate task_available variable (in this case
. The prototype of this function is: pthread_cond_wait using the function
1 int pthread_cond_wait(pthread_cond_t *cond,
2 pthread_mutex_t *mutex);
A call to this function blocks the execution of the thread until it receives a signal from another thread or is interrupted by an OS signal. In
. This is important because otherwise no other mutex  function releases the lock on pthread_cond_wait addition to blocking the thread, the
 and the predicate would never be satisfied. When the thread is released task_available thread will be able to work on the shared variable
 before resuming execution. It is convenient to think of each condition variable as being mutex on a signal, it waits to reacquire the lock on
associated with a queue. Threads performing a condition wait on the variable relinquish their lock and enter the queue. When the
), one of these threads in the queue is unblocked, and when the mutex becomes pthread_cond_signal condition is signaled (using
available, it is handed to this thread (and the thread becomes runnable).
 has been mutex In the context of our producer-consumer example, the producer thread produces the task and, since the lock on
. Since the mutex  to 1 after locking task_available relinquished (by waiting consumers), it can insert its task on the queue and set
predicate has now been satisfied, the producer must wake up one of the consumer threads by signaling it. This is done using the function
, whose prototype is as follows: pthread_cond_signal
1 int pthread_cond_signal(pthread_cond_t *cond);
. The producer then relinquishes its lock cond The function unblocks at least one thread that is currently waiting on the condition variable
, allowing one of the blocked consumer threads to consume the task. pthread_mutex_unlock  by explicitly calling mutex on
Before we rewrite our producer-consumer example using condition variables, we need to introduce two more function calls for initializing
 respectively. The prototypes of these calls are as pthread_cond_destroy  and pthread_cond_init and destroying condition variables,
follows:
1 int pthread_cond_init(pthread_cond_t *cond,
2 const pthread_condattr_t *attr);
3 int pthread_cond_destroy(pthread_cond_t *cond);
. attr ) whose attributes are defined in the attribute object cond  initializes a condition variable (pointed to by pthread_cond_init The function
Setting this pointer to NULL assigns default attributes for condition variables. If at some point in a program a condition variable is no
. These functions for manipulating condition variables pthread_cond_destroy longer required, it can be discarded using the function
enable us to rewrite our producer-consumer segment as follows:
Example 7.6 Producer-consumer using condition variables
Condition variables can be used to block execution of the producer thread when the work queue is full and the
 and cond_queue_empty consumer thread when the work queue is empty. We use two condition variables
 is cond_queue_empty  for specifying empty and full queues respectively. The predicate associated with cond_queue_full
. task_available == 1  is asserted when cond_queue_full , and task_available == 0
. It task_available  associated with the shared variable task_queue_cond_lock The producer queue locks the mutex
 is 0 (i.e., queue is empty). If this is the case, the producer inserts the task into the work task_available checks to see if
. It cond_queue_full queue and signals any waiting consumer threads to wake up by signaling the condition variable
 is 1 (i.e., queue is full), the producer performs a task_available subsequently proceeds to create additional tasks. If
 (i.e., it waits for the queue to become empty). The reason cond_queue_empty condition wait on the condition variable
 becomes clear at this point. If the lock is not released, no task_queue_cond_lock for implicitly releasing the lock on
consumer will be able to consume the task and the queue would never be empty. At this point, the producer thread is
blocked. Since the lock is available to the consumer, the thread can consume the task and signal the condition variable
 when the task has been taken off the work queue. cond_queue_empty
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is 1. If not, task_available  to check if the shared variable task_queue_cond_lock The consumer thread locks the mutex
. (Note that this signal is generated from the producer when a task is cond_queue_full it performs a condition wait on
inserted into the work queue.) If there is a task available, the consumer takes it off the work queue and signals the
producer. In this way, the producer and consumer threads operate by signaling each other. It is easy to see that this
mode of operation is similar to an interrupt-based operation as opposed to a polling-based operation of
. The program segment for accomplishing this producer-consumer behavior is as follows: pthread_mutex_trylock
1 pthread_cond_t cond_queue_empty, cond_queue_full;
2 pthread_mutex_t task_queue_cond_lock;
3 int task_available;
4
5 /* other data structures here */
6
7 main() {
8 /* declarations and initializations */
9 task_available = 0;
10 pthread_init();
11 pthread_cond_init(&cond_queue_empty, NULL);
12 pthread_cond_init(&cond_queue_full, NULL);
13 pthread_mutex_init(&task_queue_cond_lock, NULL);
14 /* create and join producer and consumer threads */
15 }
16
17 void *producer(void *producer_thread_data) {
18 int inserted;
19 while (!done()) {
20 create_task();
21 pthread_mutex_lock(&task_queue_cond_lock);
22 while (task_available == 1)
23 pthread_cond_wait(&cond_queue_empty,
24 &task_queue_cond_lock);
25 insert_into_queue();
26 task_available = 1;
27 pthread_cond_signal(&cond_queue_full);
28 pthread_mutex_unlock(&task_queue_cond_lock);
29 }
30 }
31
32 void *consumer(void *consumer_thread_data) {
33 while (!done()) {
34 pthread_mutex_lock(&task_queue_cond_lock);
35 while (task_available == 0)
36 pthread_cond_wait(&cond_queue_full,
37 &task_queue_cond_lock);
38 my_task = extract_from_queue();
39 task_available = 0;
40 pthread_cond_signal(&cond_queue_empty);
41 pthread_mutex_unlock(&task_queue_cond_lock);
42 process_task(my_task);
43 }
44 }
a  An important point to note about this program segment is that the predicate associated with Programming Notes
 is asserted, the value of cond_queue_full condition variable is checked in a loop. One might expect that when
 must be 1. However, it is a good practice to check for the condition in a loop because the thread might task_available
be woken up due to other reasons (such as an OS signal). In other cases, when the condition variable is signaled
using a condition broadcast (signaling all waiting threads instead of just one), one of the threads that got the lock
earlier might invalidate the condition. In the example of multiple producers and multiple consumers, a task available on
the work queue might be consumed by one of the other consumers.
t  When a thread performs a condition wait, it takes itself off the runnable list – consequently, i Performance Notes
t does not use any CPU cycles until it is woken up. This is in contrast to a mutex lock which consumes CPU cycles as i
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
polls for the lock.
In the above example, each task could be consumed by only one consumer thread. Therefore, we choose to signal one blocked thread
at a time. In some other computations, it may be beneficial to wake all threads that are waiting on the condition variable as opposed to a
. pthread_cond_broadcast single thread. This can be done using the function
1 int pthread_cond_broadcast(pthread_cond_t *cond);
An example of this is in the producer-consumer scenario with large work queues and multiple tasks being inserted into the work queue
pthread_cond_broadcast on each insertion cycle. This is left as an exercise for the reader (Exercise 7.2). Another example of the use of
. Section 7.8.2 is in the implementation of barriers illustrated in
, a thread can perform a wait on a pthread_cond_timedwait It is often useful to build time-outs into condition waits. Using the function
condition variable until a specified time expires. At this point, the thread wakes up by itself if it does not receive a signal or a broadcast.
The prototype for this function is:
1 int pthread_cond_timedwait(pthread_cond_t *cond,
2 pthread_mutex_t *mutex,
3 const struct timespec *abstime);
 specified expires before a signal or broadcast is received, the function returns an error message. It also abstime If the absolute time
 when it becomes available. mutex reacquires the lock on
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.6 Controlling Thread and Synchronization Attributes
In our discussion thus far, we have noted that entities such as threads and synchronization variables can have several attributes
associated with them. For example, different threads may be scheduled differently (round-robin, prioritized, etc.), they may have different
stack sizes, and so on. Similarly, a synchronization variable such as a mutex-lock may be of different types. The Pthreads API allows a
. attributes objects programmer to change the default attributes of entities using
An attributes object is a data-structure that describes entity (thread, mutex, condition variable) properties. When creating a thread or a
synchronization variable, we can specify the attributes object that determines the properties of the entity. Once created, the thread or
synchronization variable's properties are largely fixed (Pthreads allows the user to change the priority of the thread). Subsequent
changes to attributes objects do not change the properties of entities created using the attributes object prior to the change. There are
several advantages of using attributes objects. First, it separates the issues of program semantics and implementation. Thread properties
are specified by the user. How these are implemented at the system level is transparent to the user. This allows for greater portability
across operating systems. Second, using attributes objects improves modularity and readability of the programs. Third, it allows the user
to modify the program easily. For instance, if the user wanted to change the scheduling from round robin to time-sliced for all threads,
they would only need to change the specific attribute in the attributes object.
To create an attributes object with the desired properties, we must first create an object with default properties and then modify the
object as required. We look at Pthreads functions for accomplishing this for threads and synchronization variables.
7.6.1 Attributes Objects for Threads
 lets us create an attributes object for threads. The prototype of this function is pthread_attr_init The function
1 int
2 pthread_attr_init (
3 pthread_attr_t *attr);
 to the default values. Upon successful completion, the function returns a 0, otherwise it attr This function initializes the attributes object
. The prototype of this function is: pthread_attr_destroy returns an error code. The attributes object may be destroyed using the function
1 int
2 pthread_attr_destroy (
3 pthread_attr_t *attr);
. Individual properties associated with the attributes object can be attr The call returns a 0 on successful removal of the attributes object
, pthread_attr_setstacksize , pthread_attr_setguardsize_np , pthread_attr_setdetachstate changed using the following functions:
. These functions can be used to set the pthread_attr_setschedparam , and pthread_attr_setschedpolicy , pthread_attr_setinheritsched
detach state in a thread attributes object, the stack guard size, the stack size, whether scheduling policy is inherited from the creating
thread, the scheduling policy (in case it is not inherited), and scheduling parameters, respectively. We refer the reader to the Pthreads
manuals for a detailed description of these functions. For most parallel programs, default thread properties are generally adequate.
7.6.2 Attributes Objects for Mutexes
The Pthreads API supports three different kinds of locks. All of these locks use the same functions for locking and unlocking; however,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. This is the normal mutex the type of lock is determined by the lock attribute. The mutex lock used in examples thus far is called a
default type of lock. Only a single thread is allowed to lock a normal mutex once at any point in time. If a thread with a lock attempts to
lock it again, the second locking call results in a deadlock.
Consider the following example of a thread searching for an element in a binary tree. To ensure that other threads are not changing the
. The search function is as follows: tree_lock tree during the search process, the thread locks the tree with a single mutex
1 search_tree(void *tree_ptr)
2 {
3 struct node *node_pointer;
4 node_pointer = (struct node *) tree_ptr;
5 pthread_mutex_lock(&tree_lock);
6 if (is_search_node(node_pointer) == 1) {
7 /* solution is found here */
8 print_node(node_pointer);
9 pthread_mutex_unlock(&tree_lock);
10 return(1);
11 }
12 else {
13 if (tree_ptr -> left != NULL)
14 search_tree((void *) tree_ptr -> left);
15 if (tree_ptr -> right != NULL)
16 search_tree((void *) tree_ptr -> right);
17 }
18 printf("Search unsuccessful\n");
19 pthread_mutex_unlock(&tree_lock);
20 }
 ends in a deadlock since a thread attempts to lock a search_tree  is a normal mutex, the first recursive call to the function tree_lock If
. A recursive mutex allows a recursive mutex mutex that it holds a lock on. For addressing such situations, the Pthreads API supports a
single thread to lock a mutex multiple times. Each time a thread locks the mutex, a lock counter is incremented. Each unlock decrements
the counter. For any other thread to be able to successfully lock a recursive mutex, the lock counter must be zero (i.e., each lock by
another thread must have a corresponding unlock). A recursive mutex is useful when a thread function needs to call itself recursively.
 is also supported. The operation of an errorcheck mutex In addition to normal and recursive mutexes, a third kind of mutex called an
errorcheck mutex is similar to a normal mutex in that a thread can lock a mutex only once. However, unlike a normal mutex, when a
thread attempts a lock on a mutex it has already locked, instead of deadlocking it returns an error. Therefore, an errorcheck mutex is
more useful for debugging purposes.
The type of mutex can be specified using a mutex attribute object. To create and initialize a mutex attribute object to default values,
. The prototype of the function is: pthread_mutexattr_init Pthreads provides the function
1 int
2 pthread_mutexattr_init (
3 pthread_mutexattr_t *attr);
. The default type of mutex is a normal mutex. Pthreads provides the function attr This creates and initializes a mutex attributes object
 for setting the type of mutex specified by the mutex attributes object. The prototype for this function is: pthread_mutexattr_settype_np
1 int
2 pthread_mutexattr_settype_np (
3 pthread_mutexattr_t *attr,
4 int type);
,  specifies the type of the mutex and can take one of the following values corresponding to the three mutex types – normal type Here,
: recursive, or errorcheck
PTHREAD_MUTEX_NORMAL_ NP
PTHREAD_MUTEX_RECURSIVE_NP
PTHREAD_MUTEX_ERRORCHECK_NP
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 as its only argument. attr  that takes the mutex attributes object pthread_attr_destroy A mutex-attributes object can be destroyed using the
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.7 Thread Cancellation
 moves, each being evaluated by an k Consider a simple program to evaluate a set of positions in a chess game. Assume that there are
independent thread. If at any point of time, a position is established to be of a certain quality, the other positions that are known to be of
worse quality must stop being evaluated. In other words, the threads evaluating the corresponding board positions must be canceled.
. The prototype of this function is: pthread_cancel Posix threads provide this cancellation feature in the function
1 int
2 pthread_cancel (
3 pthread_t thread);
 is the handle to the thread to be canceled. A thread may cancel itself or cancel other threads. When a call to this function is thread Here,
made, a cancellation is sent to the specified thread. It is not guaranteed that the specified thread will receive or act on the cancellation.
Threads can protect themselves against cancellation. When a cancellation is actually performed, cleanup functions are invoked for
reclaiming the thread data structures. After this the thread is canceled. This process is similar to termination of a thread using the
pthread_cancel  call. This is performed independently of the thread that made the original request for cancellation. The pthread_exit
function returns after a cancellation has been sent. The cancellation may itself be performed later. The function returns a 0 on successful
completion. This does not imply that the requested thread has been canceled; it implies that the specified thread is a valid thread for
cancellation.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.8 Composite Synchronization Constructs
While the Pthreads API provides a basic set of synchronization constructs, often, there is a need for higher level constructs. These higher
level constructs can be built using basic synchronization constructs. In this section, we look at some of these constructs along with their
performance aspects and applications.
7.8.1 Read-Write Locks
In many applications, a data structure is read frequently but written infrequently. For such scenarios, it is useful to note that multiple reads
can proceed without any coherence problems. However, writes must be serialized. This points to an alternate structure called a read-write
lock. A thread reading a shared data item acquires a read lock on the variable. A read lock is granted when there are other threads that
may already have read locks. If there is a write lock on the data (or if there are queued write locks), the thread performs a condition wait.
Similarly, if there are multiple threads requesting a write lock, they must perform a condition wait. Using this principle, we design functions
. mylib_rwlock_unlock , and unlocking mylib_rwlock_wlock , write locks mylib_rwlock_rlock for read locks
. This structure maintains a count of the number of mylib_rwlock_t The read-write locks illustrated are based on a data structure called
 that is signaled when readers_proceed readers, the writer (a 0/1 integer specifying whether a writer is present), a condition variable
 of pending_writers  that is signaled when one of the writers can proceed, a count writer_proceed readers can proceed, a condition variable
 is used to initialize mylib_rwlock_init  associated with the shared data structure. The function read_write_lock pending writers, and a mutex
various components of this data structure.
 attempts a read lock on the data structure. It checks to see if there is a write lock or pending writers. If so, it mylib rwlock rlock The function
, otherwise it increments the count of readers and proceeds to grant a readers proceed performs a condition wait on the condition variable
 attempts a write lock on the data structure. It checks to see if there are readers or writers; if so, mylib_rwlock_wlock read lock. The function
. If there are no readers or writer_proceed it increments the count of pending writers and performs a condition wait on the condition variable
writer, it grants a write lock and proceeds.
 unlocks a read or write lock. It checks to see if there is a write lock, and if so, it unlocks the data mylib_rwlock_unlock The function
. If there are no readers left and readers  field to 0. If there are readers, it decrements the number of readers writer structure by setting the
). If there are no pending writers but there are writer_proceed there are pending writers, it signals one of the writers to proceed (by signaling
pending readers, it signals all the reader threads to proceed. The code for initializing and locking/unlocking is as follows:
1 typedef struct {
2 int readers;
3 int writer;
4 pthread_cond_t readers_proceed;
5 pthread_cond_t writer_proceed;
6 int pending_writers;
7 pthread_mutex_t read_write_lock;
8 } mylib_rwlock_t;
9
10
11 void mylib_rwlock_init (mylib_rwlock_t *l) {
12 l -> readers = l -> writer = l -> pending_writers = 0;
13 pthread_mutex_init(&(l -> read_write_lock), NULL);
14 pthread_cond_init(&(l -> readers_proceed), NULL);
15 pthread_cond_init(&(l -> writer_proceed), NULL);
16 }
17
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
18 void mylib_rwlock_rlock(mylib_rwlock_t *l) {
19 /* if there is a write lock or pending writers, perform condition
20 wait.. else increment count of readers and grant read lock */
21
22 pthread_mutex_lock(&(l -> read_write_lock));
23 while ((l -> pending_writers > 0) || (l -> writer > 0))
24 pthread_cond_wait(&(l -> readers_proceed),
25 &(l -> read_write_lock));
26 l -> readers ++;
27 pthread_mutex_unlock(&(l -> read_write_lock));
28 }
29
30
31 void mylib_rwlock_wlock(mylib_rwlock_t *l) {
32 /* if there are readers or writers, increment pending writers
33 count and wait. On being woken, decrement pending writers
34 count and increment writer count */
35
36 pthread_mutex_lock(&(l -> read_write_lock));
37 while ((l -> writer > 0) || (l -> readers > 0)) {
38 l -> pending_writers ++;
39 pthread_cond_wait(&(l -> writer_proceed),
40 &(l -> read_write_lock));
41 }
42 l -> pending_writers --;
43 l -> writer ++
44 pthread_mutex_unlock(&(l -> read_write_lock));
45 }
46
47
48 void mylib_rwlock_unlock(mylib_rwlock_t *l) {
49 /* if there is a write lock then unlock, else if there are
50 read locks, decrement count of read locks. If the count
51 is 0 and there is a pending writer, let it through, else
52 if there are pending readers, let them all go through */
53
54 pthread_mutex_lock(&(l -> read_write_lock));
55 if (l -> writer > 0)
56 l -> writer = 0;
57 else if (l -> readers > 0)
58 l -> readers --;
59 pthread_mutex_unlock(&(l -> read_write_lock));
60 if ((l -> readers == 0) && (l -> pending_writers > 0))
61 pthread_cond_signal(&(l -> writer_proceed));
62 else if (l -> readers > 0)
63 pthread_cond_broadcast(&(l -> readers_proceed));
64 }
We now illustrate the use of read-write locks with some examples.
Example 7.7 Using read-write locks for computing the minimum of a list of numbers
A simple use of read-write locks is in computing the minimum of a list of numbers. In our earlier implementation, we
associated a lock with the minimum value. Each thread locked this object and updated the minimum value, if necessary.
In general, the number of times the value is examined is greater than the number of times it is updated. Therefore, it is
beneficial to allow multiple reads using a read lock and write after a write lock only if needed. The corresponding
program segment is as follows:
1 void *find_min_rw(void *list_ptr) {
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
2 int *partial_list_pointer, my_min, i;
3 my_min = MIN_INT;
4 partial_list_pointer = (int *) list_ptr;
5 for (i = 0; i < partial_list_size; i++)
6 if (partial_list_pointer[i] < my_min)
7 my_min = partial_list_pointer[i];
8 /* lock the mutex associated with minimum_value and
9 update the variable as required */
10 mylib_rwlock_rlock(&read_write_lock);
11 if (my_min < minimum_value) {
12 mylib_rwlock_unlock(&read_write_lock);
13 mylib_rwlock_wlock(&read_write_lock);
14 minimum_value = my_min;
15 }
16 /* and unlock the mutex */
17 mylib_rwlock_unlock(&read_write_lock);
18 pthread_exit(0);
19 }
a  In this example, each thread computes the minimum element in its partial list. It then attempts Programming Notes
read lock on the lock associated with the global minimum value. If the global minimum value is greater than the locally
minimum value (thus requiring an update), the read lock is relinquished and a write lock is sought. Once the write lock
has been obtained, the global minimum can be updated. The performance gain obtained from read-write locks is
influenced by the number of threads and the number of updates (write locks) required. In the extreme case when the first
value of the global minimum is also the true minimum value, no write locks are subsequently sought. In this case, the
version using read-write locks performs better. In contrast, if each thread must update the global minimum, the read
locks are superfluous and add overhead to the program.
Example 7.8 Using read-write locks for implementing hash tables
A commonly used operation in applications ranging from database query to state space search is the search of a key in
a database. The database is organized as a hash table. In our example, we assume that collisions are handled by
chaining colliding entries into linked lists. Each list has a lock associated with it. This lock ensures that lists are not being
updated and searched at the same time. We consider two versions of this program: one using mutex locks and one
using read-write locks developed in this section.
The mutex lock version of the program hashes the key into the table, locks the mutex associated with the table index,
and proceeds to search/update within the linked list. The thread function for doing this is as follows:
1 manipulate_hash_table(int entry) {
2 int table_index, found;
3 struct list_entry *node, *new_node;
4
5 table_index = hash(entry);
6 pthread_mutex_lock(&hash_table[table_index].list_lock);
7 found = 0;
8 node = hash_table[table_index].next;
9 while ((node != NULL) && (!found)) {
10 if (node -> value == entry)
11 found = 1;
12 else
13 node = node -> next;
14 }
15 pthread_mutex_unlock(&hash_table[table_index].list_lock);
16 if (found)
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
17 return(1);
18 else
19 insert_into_hash_table(entry);
20 }
 before performing the actual hash_table[table_index].list_lock  must lock insert_into_hash_table Here, the function
insertion. When a large fraction of the queries are found in the hash table (i.e., they do not need to be inserted), these
searches are serialized. It is easy to see that multiple threads can be safely allowed to search the hash table and only
updates to the table must be serialized. This can be accomplished using read-write locks. We can rewrite the
 function as follows: manipulate_hash table
1 manipulate_hash_table(int entry)
2 {
3 int table_index, found;
4 struct list_entry *node, *new_node;
5
6 table_index = hash(entry);
7 mylib_rwlock_rlock(&hash_table[table_index].list_lock);
8 found = 0;
9 node = hash_table[table_index].next;
10 while ((node != NULL) && (!found)) {
11 if (node -> value == entry)
12 found = 1;
13 else
14 node = node -> next;
15 }
16 mylib_rwlock_rlock(&hash_table[table_index].list_lock);
17 if (found)
18 return(1);
19 else
20 insert_into_hash_table(entry);
21 }
 before hash_table[table_index].list_lock  must first get a write lock on insert_into_hash_table Here, the function
performing actual insertion.
mylib_rwlock_t  field has been defined to be of type list_lock  In this example, we assume that the Programming Notes
. Using mylib_rwlock_init and all read-write locks associated with the hash tables have been initialized using the function
instead of a mutex lock allows multiple threads to search respective entries concurrently. Thus, if the mylib rwlock_rlock_
number of successful searches outnumber insertions, this formulation is likely to yield better performance. Note that the
 function must be suitably modified to use write locks (instead of mutex locks as before). insert into_hash_table
It is important to identify situations where read-write locks offer advantages over normal locks. Since read-write locks offer no advantage
over normal mutexes for writes, they are beneficial only when there are a significant number of read operations. Furthermore, as the
critical section becomes larger, read-write locks offer more advantages. This is because the serialization overhead paid by normal
mutexes is higher. Finally, since read-write locks rely on condition variables, the underlying thread system must provide fast condition wait,
signal, and broadcast functions. It is possible to do a simple analysis to understand the relative merits of read-write locks (Exercise 7.7).
7.8.2 Barriers
. A barrier call is used to hold a thread barrier An important and often used construct in threaded (as well as other parallel) programs is a
until all other threads participating in the barrier have reached the barrier. Barriers can be implemented using a counter, a mutex and a
condition variable. (They can also be implemented simply using mutexes; however, such implementations suffer from the overhead of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
busy-wait.) A single integer is used to keep track of the number of threads that have reached the barrier. If the count is less than the total
number of threads, the threads execute a condition wait. The last thread entering (and setting the count to the number of threads) wakes
up all the threads using a condition broadcast. The code for accomplishing this is as follows:
1 typedef struct {
2 pthread_mutex_t count_lock;
3 pthread_cond_t ok_to_proceed;
4 int count;
5 } mylib_barrier_t;
6
7 void mylib_init_barrier(mylib_barrier_t *b) {
8 b -> count = 0;
9 pthread_mutex_init(&(b -> count_lock), NULL);
10 pthread_cond_init(&(b -> ok_to_proceed), NULL);
11 }
12
13 void mylib_barrier (mylib_barrier_t *b, int num_threads) {
14 pthread_mutex_lock(&(b -> count_lock));
15 b -> count ++;
16 if (b -> count == num_threads) {
17 b -> count = 0;
18 pthread_cond_broadcast(&(b -> ok_to_proceed));
19 }
20 else
21 while (pthread_cond_wait(&(b -> ok_to_proceed),
22 &(b -> count_lock)) != 0);
23 pthread_mutex_unlock(&(b -> count_lock));
24 }
In the above implementation of a barrier, threads enter the barrier and stay until the broadcast signal releases them. The threads are
 is passed among them one after the other. The trivial lower bound on execution time of count_lock released one by one since the mutex
 threads. This implementation of a barrier can be speeded up using multiple barrier variables. n ) for n  ( O this function is therefore
n /2 condition variable-mutex pairs for implementing a barrier for n Let us consider an alternate barrier implementation in which there are
threads. The barrier works as follows: at the first level, threads are paired up and each pair of threads shares a single condition
variable-mutex pair. A designated member of the pair waits for both threads to arrive at the pairwise barrier. Once this happens, all the
designated members are organized into pairs, and this process continues until there is only one thread. At this point, we know that all
/2 n threads have reached the barrier point. We must release all threads at this point. However, releasing them requires signaling all
condition variables. We use the same hierarchical strategy for doing this. The designated thread in a pair signals the respective condition
variables.
1 typedef struct barrier_node {
2 pthread_mutex_t count_lock;
3 pthread_cond_t ok_to_proceed_up;
4 pthread_cond_t ok_to_proceed_down;
5 int count;
6 } mylib_barrier_t_internal;
7
8 typedef struct barrier_node mylog_logbarrier_t[MAX_THREADS];
9 pthread_t p_threads[MAX_THREADS];
10 pthread_attr_t attr;
11
12 void mylib_init_barrier(mylog_logbarrier_t b) {
13 int i;
14 for (i = 0; i < MAX_THREADS; i++) {
15 b[i].count = 0;
16 pthread_mutex_init(&(b[i].count_lock), NULL);
17 pthread_cond_init(&(b[i].ok_to_proceed_up), NULL);
18 pthread_cond_init(&(b[i].ok_to_proceed_down), NULL);
19 }
20 }
21
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
22 void mylib_logbarrier (mylog_logbarrier_t b, int num_threads,
23 int thread_id) {
24 int i, base, index;
25 i=2;
26 base = 0;
27
28 do {
29 index = base + thread_id / i;
30 if (thread_id % i == 0) {
31 pthread_mutex_lock(&(b[index].count_lock));
32 b[index].count ++;
33 while (b[index].count < 2)
34 pthread_cond_wait(&(b[index].ok_to_proceed_up),
35 &(b[index].count_lock));
36 pthread_mutex_unlock(&(b[index].count_lock));
37 }
38 else {
39 pthread_mutex_lock(&(b[index].count_lock));
40 b[index].count ++;
41 if (b[index].count == 2)
42 pthread_cond_signal(&(b[index].ok_to_proceed_up));
43 while (pthread_cond_wait(&(b[index].ok_to_proceed_down),
44 &(b[index].count_lock)) != 0);
45 pthread_mutex_unlock(&(b[index].count_lock));
46 break;
47 }
48 base = base + num_threads/i;
49 i=i*2;
50 } while (i <= num_threads);
51 i=i/2;
52 for (; i > 1; i = i / 2) {
53 base = base - num_threads/i;
54 index = base + thread_id / i;
55 pthread_mutex_lock(&(b[index].count_lock));
56 b[index].count = 0;
57 pthread_cond_signal(&(b[index].ok_to_proceed_down));
58 pthread_mutex_unlock(&(b[index].count_lock));
59 }
60 }
In this implementation of a barrier, we visualize the barrier as a binary tree. Threads arrive at the leaf nodes of this tree. Consider an
instance of a barrier with eight threads. Threads 0 and 1 are paired up on a single leaf node. One of these threads is designated as the
representative of the pair at the next level in the tree. In the above example, thread 0 is considered the representative and it waits on the
 for thread 1 to catch up. All even numbered threads proceed to the next level in the tree. Now thread ok_to_proceed_up condition variable
0 is paired up with thread 2 and thread 4 with thread 6. Finally thread 0 and 4 are paired. At this point, thread 0 realizes that all threads
. When all threads are ok_to_proceed_down have reached the desired barrier point and releases threads by signaling the condition
released, the barrier is complete.
 thread barrier. Each node corresponds to two condition variables, one for n  - 1 nodes in the tree for an n It is easy to see that there are
releasing the thread up and one for releasing it down, one lock, and a count of number of threads reaching the node. The tree nodes are
/4 tree nodes at the next higher n /2 elements, the n /2 leaf nodes taking the first n  with the mylog_logbarrier_t linearly laid out in the array
/4 nodes and so on. n level taking the next
It is interesting to study the performance of this program. Since threads in the linear barrier are released one after the other, it is
, we plot the runtime of 1000 Figure 7.3 reasonable to expect runtime to be linear in the number of threads even on multiple processors. In
barriers in a sequence on a 32 processor SGI Origin 2000. The linear runtime of the sequential barrier is clearly reflected in the runtime.
The logarithmic barrier executing on a single processor does just as much work asymptotically as a sequential barrier (albeit with a higher
constant). However, on a parallel machine, in an ideal case when threads are assigned so that subtrees of the binary barrier tree are
). While this is difficult to achieve without being able to examine or assign p  + log p/ n ( O assigned to different processors, the time grows as
blocks of threads corresponding to subtrees to individual processors, the logarithmic barrier displays significantly better performance than
 term p/ n  becomes large for a given number of processors. This is because the n  as n the serial barrier. Its performance tends to be linear in
 term in the execution time. This is observed both from observations as well as from analytical intuition. p starts to dominate the log
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 7.3. Execution time of 1000 sequential and logarithmic barriers as a function of number
of threads on a 32 processor SGI Origin 2000.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.9 Tips for Designing Asynchronous Programs
When designing multithreaded applications, it is important to remember that one cannot assume any order of execution with respect to
other threads. Any such order must be explicitly established using the synchronization mechanisms discussed above: mutexes,
condition variables, and joins. In addition, the system may provide other means of synchronization. However, for portability reasons, we
discourage the use of these mechanisms.
In many thread libraries, threads are switched at semi-deterministic intervals. Such libraries are more forgiving of synchronization errors
 libraries. On the other hand, kernel threads (threads supported by the slightly asynchronous in programs. These libraries are called
kernel) and threads scheduled on multiple processors are less forgiving. The programmer must therefore not make any assumptions
regarding the level of asynchrony in the threads library.
Let us look at some common errors that arise from incorrect assumptions on relative execution times of threads:
Say, a thread T1 creates another thread T2. T2 requires some data from thread T1. This data is transferred using a global
memory location. However, thread T1 places the data in the location after creating thread T2. The implicit assumption here is
that T1 will not be switched until it blocks; or that T2 will get to the point at which it uses the data only after T1 has stored it
there. Such assumptions may lead to errors since it is possible that T1 gets switched as soon as it creates T2. In such a
situation, T1 will receive uninitialized data.
Assume, as before, that thread T1 creates T2 and that it needs to pass data to thread T2 which resides on its stack. It passes
this data by passing a pointer to the stack location to thread T2. Consider the scenario in which T1 runs to completion before
T2 gets scheduled. In this case, the stack frame is released and some other thread may overwrite the space pointed to
formerly by the stack frame. In this case, what thread T2 reads from the location may be invalid data. Similar problems may
exist with global variables.
We strongly discourage the use of scheduling techniques as means of synchronization. It is especially difficult to keep track of
scheduling decisions on parallel machines. Further, as the number of processors change, these issues may change
depending on the thread scheduling policy. It may happen that higher priority threads are actually waiting while lower priority
threads are running.
We recommend the following rules of thumb which help minimize the errors in threaded programs.
Set up all the requirements for a thread before actually creating the thread. This includes initializing the data, setting thread
attributes, thread priorities, mutex-attributes, etc. Once you create a thread, it is possible that the newly created thread
actually runs to completion before the creating thread gets scheduled again.
When there is a producer-consumer relation between two threads for certain data items, make sure the producer thread
places the data before it is consumed and that intermediate buffers are guaranteed to not overflow.
At the consumer end, make sure that the data lasts at least until all potential consumers have consumed the data. This is
particularly relevant for stack variables.
Where possible, define and use group synchronizations and data replication. This can improve program performance
significantly.
While these simple tips provide guidelines for writing error-free threaded programs, extreme caution must be taken to avoid race
conditions and parallel overheads associated with synchronization.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.10 OpenMP: a Standard for Directive Based Parallel Programming
In the first part of this chapter, we studied the use of threaded APIs for programming shared address space machines. While
standardization and support for these APIs has come a long way, their use is still predominantly restricted to system programmers as
opposed to application programmers. One of the reasons for this is that APIs such as Pthreads are considered to be low-level primitives.
Conventional wisdom indicates that a large class of applications can be efficiently supported by higher level constructs (or directives) which
rid the programmer of the mechanics of manipulating threads. Such directive-based languages have existed for a long time, but only
recently have standardization efforts succeeded in the form of OpenMP. OpenMP is an API that can be used with FORTRAN, C, and C++
for programming shared address space machines. OpenMP directives provide support for concurrency, synchronization, and data
handling while obviating the need for explicitly setting up mutexes, condition variables, data scope, and initialization. We use the OpenMP
C API in the rest of this chapter.
7.10.1 The OpenMP Programming Model
We initiate the OpenMP programming model with the aid of a simple program. OpenMP directives in C and C++ are based on the
 compiler directives. The directive itself consists of a directive name followed by clauses. #pragma
1 #pragma omp directive [clause list]
 directive. This directive is responsible for creating a group of threads. parallel OpenMP programs execute serially until they encounter the
The exact number of threads can be specified in the directive, set using an environment variable, or at runtime using OpenMP functions.
 of this group of threads and is assigned the thread id 0 within master  directive becomes the parallel The main thread that encounters the
 directive has the following prototype: parallel the group. The
1 #pragma omp parallel [clause list]
2 /* structured block */
3
 specified by the parallel directive. The clause list is used to specify structured block Each thread created by this directive executes the
conditional parallelization, number of threads, and data handling.
 determines whether the parallel construct results in creation of if (scalar expression)  The clause Conditional Parallelization:
 clause can be used with a parallel directive. if threads. Only one
 specifies the number of threads that are created by the expression) num_threads (integer  The clause Degree of Concurrency:
 directive. parallel
h  indicates that the set of variables specified is local to each thread – i.e., eac private (variable list)  The clause Data Handling:
 is similar to the private clause, except the firstprivate (variable list) thread has its own copy of each variable in the list. The clause
values of variables on entering the threads are initialized to corresponding values before the parallel directive. The clause shared
(variable list) indicates that all variables in the list are shared across all the threads, i.e., there is only one copy. Special care
must be taken while handling these variables by threads to ensure serializability.
It is easy to understand the concurrency model of OpenMP when viewed in the context of the corresponding Pthreads translation. In
, we show one possible translation of an OpenMP program to a Pthreads program. The interested reader may note that such a Figure 7.4
translation can easily be automated through a Yacc or CUP script.
Figure 7.4. A sample OpenMP program along with its Pthreads translation that might be performed by an OpenMP
compiler.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Example 7.9 Using the parallel directive
1 #pragma omp parallel if (is_parallel == 1) num_threads(8) \
2 private (a) shared (b) firstprivate(c)
3 {
4 /* structured block */
5 }
 equals one, eight threads are created. Each of these threads gets private is_parallel Here, if the value of the variable
 is initialized to c . Furthermore, the value of each copy of b , and shares a single value of variable c  and a copies of variables
 before the parallel directive. c the value of
 implies that, by default (shared) . The clause default (none)  or default (shared) The default state of a variable is specified by the clause
 implies that the state of each variable used in a thread must be default (none) default, a variable is shared by all the threads. The clause
explicitly specified. This is generally recommended, to guard against errors arising from unintentional concurrent access to shared data.
 clause specifies how multiple reduction  specifies how multiple local copies of a variable are initialized inside a thread, the firstprivate Just as
reduction local copies of a variable at different threads are combined into a single copy at the master when threads exit. The usage of the
. operator . This clause performs a reduction on the scalar variables specified in the list using the reduction (operator: variable list) clause is
. || , and +, *, -, &, |, ^, &&  can be one of operator The variables in the list are implicitly specified as being private to threads. The
Example 7.10 Using the reduction clause
1 #pragma omp parallel reduction(+: sum) num_threads(8)
2 {
3 /* compute local sums here */
4 }
5 /* sum here contains sum of all local instances of sums */
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. When the threads exit, the sum of all of these sum In this example, each of the eight threads gets a copy of the variable
local copies is stored in the single copy of the variable (at the master thread).
 after we discuss Section 7.10.4 . We will describe this clause in copyin In addition to these data handling clauses, there is one other clause,
data scope in greater detail.
 directive along with the clauses to write our first OpenMP program. We introduce two functions to facilitate parallel We can now use the
 function omp_get_thread_num()  function returns the number of threads in the parallel region and the omp_get_num_threads() this. The
returns the integer i.d. of each thread (recall that the master thread has an i.d. 0).
Example 7.11 Computing PI using OpenMP directives
, which presented a Pthreads program for the same problem. The Example 7.2 Our first OpenMP example follows from
, the total number of random points in two dimensions across all npoints parallel directive specifies that all variables except
 after all threads sum threads, are local. Furthermore, the directive specifies that there are eight threads, and the value of
 is used to determine the omp_get_num_threads complete execution is the sum of local values at each thread. The function
 loop generates the required number of random points (in two dimensions) for , a Example 7.2 total number of threads. As in
and determines how many of them are within the prescribed circle of unit diameter.
1 /* ******************************************************
2 An OpenMP version of a threaded program to compute PI.
3 ****************************************************** */
4
5 #pragma omp parallel default(private) shared (npoints) \
6 reduction(+: sum) num_threads(8)
7 {
8 num_threads = omp_get_num_threads();
9 sample_points_per_thread = npoints / num_threads;
10 sum = 0;
11 for (i = 0; i < sample_points_per_thread; i++) {
12 rand_no_x =(double)(rand_r(&seed))/(double)((2<<14)-1);
13 rand_no_y =(double)(rand_r(&seed))/(double)((2<<14)-1);
14 if (((rand_no_x - 0.5) * (rand_no_x - 0.5) +
15 (rand_no_y - 0.5) * (rand_no_y - 0.5)) < 0.25)
16 sum ++;
17 }
18 }
Note that this program is much easier to write in terms of specifying creation and termination of threads compared to the corresponding
POSIX threaded program.
7.10.2 Specifying Concurrent Tasks in OpenMP
s  directive can be used in conjunction with other directives to specify concurrency across iterations and tasks. OpenMP provide parallel The
.  – to specify concurrent iterations and tasks sections  and for two directives –
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 Directive for The
 directive is as follows: for  directive is used to split parallel iteration spaces across threads. The general form of a for The
1 #pragma omp for [clause list]
2 /* for loop */
3
. The first four ordered ,and nowait , schedule , reduction , lastprivate , firstprivate , private The clauses that can be used in this context are:
 clause deals with how lastprivate  directive. The parallel clauses deal with data handling and have identical semantics as in the case of the
sections  loop (or for  loop. When using a for multiple local copies of a variable are written back into a single copy at the end of the parallel
directive as we shall see) for farming work to threads, it is sometimes desired that the last iteration (as defined by serial execution) of the
 directive. lastprivate  loop update the value of a variable. This is accomplished using the for
p Example 7.12 Using the for directive for computing
 loop is independent, and can be executed concurrently. In such for  that each iteration of the Example 7.11 Recall from
 directive. The modified code segment is as follows: for situations, we can simplify the program using the
1 #pragma omp parallel default(private) shared (npoints) \
2 reduction(+: sum) num_threads(8)
3 {
4 sum=0;
5 #pragma omp for
6 for (i = 0; i < npoints; i++) {
7 rand_no_x =(double)(rand_r(&seed))/(double)((2<<14)-1);
8 rand_no_y =(double)(rand_r(&seed))/(double)((2<<14)-1);
9 if (((rand_no_x - 0.5) * (rand_no_x - 0.5) +
10 (rand_no_y - 0.5) * (rand_no_y - 0.5)) < 0.25)
11 sum ++;
12 }
13 }
 loop immediately following the directive must be executed in parallel, for  directive in this example specifies that the for The
 in this case, as opposed to npoints i.e., split across various threads. Notice that the loop index goes from 0 to
 directive is assumed to be private, by default. It is for . The loop index for the Example 7.11  in sample_points_per_thread
interesting to note that the only difference between this OpenMP segment and the corresponding serial code is the two
directives. This example illustrates how simple it is to convert many serial programs into OpenMP-based threaded
programs.
Assigning Iterations to Threads
 directive is schedule  directive deals with the assignment of iterations to threads. The general form of the for  clause of the schedule The
. runtime , and guided , dynamic , static . OpenMP supports four scheduling classes: schedule(scheduling_class[, parameter])
. Example 7.13 Scheduling classes in OpenMP – matrix multiplication
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
We explore various scheduling classes in the context of dense matrix multiplication. The code for multiplying two matrices
 is as follows: c  to yield matrix b  and a
1 for (i = 0; i < dim; i++) {
2 for (j = 0; j < dim; j++) {
3 c(i,j) = 0;
4 for (k = 0; k < dim; k++) {
5 c(i,j) += a(i, k) * b(k, j);
6 }
7 }
8 }
The code segment above specifies a three-dimensional iteration space providing us with an ideal example for studying
various scheduling classes in OpenMP.
. This technique splits the iteration space into equal chunk-size]) schedule(static[,  scheduling class is static  The general form of the Static
 is specified, the iteration space is split chunk-size  and assigns them to threads in a round-robin fashion. When no chunk-size chunks of size
into as many chunks as there are threads and one chunk is assigned to each thread.
Example 7.14 Static scheduling of loops in matrix multiplication
The following modification of the matrix-multiplication program causes the outermost iteration to be split statically across
. Figure 7.5(a) threads as illustrated in
1 #pragma omp parallel default(private) shared (a, b, c, dim) \
2 num_threads(4)
3 #pragma omp for schedule(static)
4 for (i = 0; i < dim; i++) {
5 for (j = 0; j < dim; j++) {
6 c(i,j) = 0;
7 for (k = 0; k < dim; k++) {
8 c(i,j) += a(i, k) * b(k, j);
9 }
10 }
11 }
Figure 7.5. Three different schedules using the static scheduling class of OpenMP.
, the size of each partition is 32 columns, since we have not specified the dim = 128 Since there are four threads in all, if
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Another Figure 7.5(b)  results in the partitioning of the iteration space illustrated in schedule(static, 16) chunk size. Using
 is parallelized Example 7.13  loop in the program in for  results when each Figure 7.5(c) example of the split illustrated in
). Section 7.10.6  and nested parallelism is enabled (see schedule(static) across threads with a
,  Often, because of a number of reasons, ranging from heterogeneous computing resources to non-uniform processor loads Dynamic
 scheduling class. The general dynamic equally partitioned workloads take widely varying execution times. For this reason, OpenMP has a
. However, these are chunk-size . The iteration space is partitioned into chunks given by schedule(dynamic[, chunk-size]) form of this class is
 is chunk-size assigned to threads as they become idle. This takes care of the temporal imbalances resulting from static scheduling. If no
specified, it defaults to a single iteration per chunk.
e  Consider the partitioning of an iteration space of 100 iterations with a chunk size of 5. This corresponds to 20 chunks. If there ar Guided
16 threads, in the best case, 12 threads get one chunk each and the remaining four threads get two chunks. Consequently, if there are as
edge many processors as threads, this assignment results in considerable idling. The solution to this problem (also referred to as an
 scheduling class. The general guided ) is to reduce the chunk size as we proceed through the computation. This is the principle of the effect
.In this class, the chunk size is reduced exponentially as each chunk is dispatched to a schedule(guided[, chunk-size]) form of this class is
 refers to the smallest chunk that should be dispatched. Therefore, when the number of iterations left is less than chunk-size thread. The
 defaults to one if none is specified. chunk-size , the entire set of iterations is dispatched at once. The value of chunk-size
s  Often it is desirable to delay scheduling decisions until runtime. For example, if one would like to see the impact of variou Runtime
OMP_SCHEDULE . In this case the environment variable runtime scheduling strategies to select the best one, the scheduling can be set to
determines the scheduling class and the chunk size.
 directive, the actual scheduling technique is not specified and is implementation omp for When no scheduling class is specified with the
 loop that follows. For example, it must not have a break statement, the for  directive places additional restrictions on the for dependent. The
 loop must be an integer assignment, the logical expression for loop control variable must be an integer, the initialization expression of the
, and the increment expression must have integer increments or decrements only. For more details on , >, or must be one of <,
these restrictions, we refer the reader to the OpenMP manuals.
 Directives for Synchronization Across Multiple
-directives within a parallel construct that do not execute an implicit barrier at the end of each for Often, it is desirable to have a sequence of
 directive to indicate that the threads can proceed to the next for , which can be used with a nowait  directive. OpenMP provides a clause – for
 loop execution. This is illustrated in the following example: for statement without waiting for all other threads to complete the
Example 7.15 Using the nowait clause
. If the past_list  and current_list  needs to be looked up in two lists – name Consider the following example in which variable
name exists in a list, it must be processed accordingly. The name might exist in both lists. In this case, there is no need to
wait for all threads to complete execution of the first loop before proceeding to the second loop. Consequently, we can use
 clause to save idling and synchronization overheads as follows: nowait the
1 #pragma omp parallel
2 {
3 #pragma omp for nowait
4 for (i = 0; i < nmax; i++)
5 if (isEqual(name, current_list[i])
6 processCurrentName(name);
7 #pragma omp for
8 for (i = 0; i < mmax; i++)
9 if (isEqual(name, past_list[i])
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
10 processPastName(name);
11 }
 Directive sections The
, taskA  directive is suited to partitioning iteration spaces across threads. Consider now a scenario in which there are three tasks ( for The
) that need to be executed. Assume that these tasks are independent of each other and therefore can be assigned to taskC , and taskB
 directive. The general form of the sections different threads. OpenMP supports such non-iterative parallel task assignment using the
 directive is as follows: sections
1 #pragma omp sections [clause list]
2 {
3 [#pragma omp section
4 /* structured block */
5 ]
6 [#pragma omp section
7 /* structured block */
8 ]
9 ...
10 }
 directive assigns the structured block corresponding to each section to one thread (indeed more than one section can be sections This
. The no wait , and reduction , lastprivate , firstprivate , private  may include the following clauses – clause list assigned to a single thread). The
 clause, in this case, specifies lastprivate  directive. The for syntax and semantics of these clauses are identical to those in the case of the
 clause specifies that there is no implicit nowait  directive updates the value of the variable. The sections that the last section (lexically) of the
 directive. sections synchronization among all threads at the end of the
 directive is as follows: sections , the corresponding taskC , and taskB , taskA For executing the three concurrent tasks
1 #pragma omp parallel
2 {
3 #pragma omp sections
4 {
5 #pragma omp section
6 {
7 taskA();
8 }
9 #pragma omp section
10 {
11 taskB();
12 }
13 #pragma omp section
14 {
15 taskC();
16 }
17 }
18 }
If there are three threads, each section (in this case, the associated task) is assigned to one thread. At the end of execution of the
 blocks. section  clause is used). Note that it is illegal to branch in and out of nowait assigned section, the threads synchronize (unless the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Merging Directives
 to farm out work to threads. If sections  and for  to create concurrent threads, and parallel In our discussion thus far, we use the directive
 directives would execute serially (all work is farmed to a single thread, the sections  and for  directive specified, the parallel there was no
 directive. OpenMP allows the programmer parallel  directives are generally preceded by the sections  and for master thread). Consequently,
, re-spectively. The clause list for the merged directive can be from the parallel sections  and parallel for  directives to parallel to merge the
 directives. for / sections  or parallel clause lists of either the
For example:
1 #pragma omp parallel default (private) shared (n)
2 {
3 #pragma omp for
4 for (i = 0 < i < n; i++) {
5 /* body of parallel for loop */
6 }
7 }
is identical to:
1 #pragma omp parallel for default (private) shared (n)
2 {
3 for (i = 0 < i < n; i++) {
4 /* body of parallel for loop */
5 }
6 }
7
and:
1 #pragma omp parallel
2 {
3 #pragma omp sections
4 {
5 #pragma omp section
6 {
7 taskA();
8 }
9 #pragma omp section
10 {
11 taskB();
12 }
13 /* other sections here */
14 }
15 }
is identical to:
1 #pragma omp parallel sections
2 {
3 #pragma omp section
4 {
5 taskA();
6 }
7 #pragma omp section
8 {
9 taskB();
10 }
11 /* other sections here */
12 }
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 Directives parallel Nesting
 loops across various threads, we would modify the program as follows: for Let us revisit Program 7.13. To split each of the
1 #pragma omp parallel for default(private) shared (a, b, c, dim) \
2 num_threads(2)
3 for (i = 0; i < dim; i++) {
4 #pragma omp parallel for default(private) shared (a, b, c, dim) \
5 num_threads(2)
6 for (j = 0; j < dim; j++) {
7 c(i,j) = 0;
8 #pragma omp parallel for default(private) \
9 shared (a, b, c, dim) num_threads(2)
10 for (k = 0; k < dim; k++) {
11 c(i,j) += a(i, k) * b(k, j);
12 }
13 }
14 }
parallel  directives inside a single for We start by making a few observations about how this segment is written. Instead of nesting three
 directives that bind to single , and sections , for  directives. This is because OpenMP does not allow parallel for directive, we have used three
 directive to be nested. Furthermore, the code as written only generates a logical team of threads on encountering a parallel the same
 directive. parallel  directive. The newly generated logical team is still executed by the same thread corresponding to the outer parallel nested
 environment variable. If the OMP_NESTED To generate a new set of threads, nested parallelism must be enabled using the
 region is serialized and executed by a single thread. If the parallel , then the inner FALSE  environment variable is set to OMP_NESTED
, nested parallelism is enabled. The default state of this environment variable is TRUE  environment variable is set to OMP_NESTED
. Section 7.10.6 , i.e., nested parallelism is disabled. OpenMP environment variables are discussed in greater detail in FALSE
There are a number of other restrictions associated with the use of synchronization constructs in nested parallelism. We refer the reader to
the OpenMP manual for a discussion of these restrictions.
7.10.3 Synchronization Constructs in OpenMP
, we described the need for coordinating the execution of multiple threads. This may be the result of a desired execution Section 7.5 In
order, the atomicity of a set of instructions, or the need for serial execution of code segments. The Pthreads API supports mutexes and
condition variables. Using these we implemented a range of higher level functionality in the form of read-write locks, barriers, monitors,
etc. The OpenMP standard provides this high-level functionality in an easy-to-use API. In this section, we will explore these directives and
their use.
 Directive barrier Synchronization Point: The
 directive, whose syntax is as follows: barrier A barrier is one of the most frequently used synchronization primitives. OpenMP provides a
1 #pragma omp barrier
parallel On encountering this directive, all threads in a team wait until others have caught up, and then release. When used with nested
barrier  directive. For executing barriers conditionally, it is important to note that a parallel  directive binds to the closest barrier directives, the
directive must be enclosed in a compound statement that is conditionally executed. This is because pragmas are compiler directives and
 regions. However, there is usually a higher parallel not a part of the language. Barriers can also be effected by ending and restarting
overhead associated with this. Consequently, it is not the method of choice for implementing barriers.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 Directives master  and single Single Thread Executions: The
Often, a computation within a parallel section needs to be performed by just one thread. A simple example of this is the computation of the
mean of a list of numbers. Each thread can compute a local sum of partial lists, add these local sums to a shared global sum, and have
one thread compute the mean by dividing this global sum by the number of entries in the list. The last step can be accomplished using a
 directive. single
 directive is as follows: single  directive specifies a structured block that is executed by a single (arbitrary) thread. The syntax of the single A
1 #pragma omp single [clause list]
2 structured block
. These clauses have the same semantics as before. On encountering the nowait , and firstprivate , private The clause list can take clauses
 clause has been specified at nowait  block, the first thread enters the block. All the other threads proceed to the end of the block. If the single
 block for the thread to finish executing the single the end of the block, then the other threads proceed; otherwise they wait at the end of the
block. This directive is useful for computing global data as well as performing I/O.
 directive in which only the master thread executes the structured block. The syntax of single  directive is a specialization of the master The
 directive is as follows: master the
1 #pragma omp master
2 structured block
 directive. master  directive, there is no implicit barrier associated with the single In contrast to the
 Directives atomic  and critical Critical Sections: The
e In our discussion of Pthreads, we had examined the use of locks to protect critical regions – regions that must be executed serially, on
 directive for implementing critical critical ), OpenMP provides a Section 7.10.5 ( thread at a time. In addition to explicit lock management
 directive is: critical regions. The syntax of a
1 #pragma omp critical [(name)]
2 structured block
 allows different threads to execute different name  can be used to identify a critical region. The use of name Here, the optional identifier
code while being protected from each other.
Example 7.16 Using the critical directive for producer-consumer threads
Consider a producer-consumer scenario in which a producer thread generates a task and inserts it into a task-queue. The
consumer thread extracts tasks from the queue and executes them one at a time. Since there is concurrent access to the
task-queue, these accesses must be serialized using critical blocks. Specifically, the tasks of inserting and extracting from
the task-queue must be serialized. This can be implemented as follows:
1 #pragma omp parallel sections
2 {
3 #pragma parallel section
4 {
5 /* producer thread */
6 task = produce_task();
7 #pragma omp critical ( task_queue)
8 {
9 insert_into_queue(task);
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
10 }
11 }
12 #pragma parallel section
13 {
14 /* consumer thread */
15 #pragma omp critical ( task_queue)
16 {
17 task = extract_from_queue(task);
18 }
19 consume_task(task);
20 }
21 }
 and insert_into_queue Note that queue full and queue empty conditions must be explicitly handled here in functions
. extract_from_queue
 directive ensures that at any point in the execution of the program, only one thread is within a critical section specified by a critical The
certain name. If a thread is already inside a critical section (with a name), all others must wait until it is done before entering the named
critical section. The name field is optional. If no name is specified, the critical section maps to a default name that is the same for all
unnamed critical sections. The names of critical sections are global across the program.
 function in Pthreads. The name field maps to mutex  directive is a direct application of the corresponding critical It is easy to see that the
 sections critical the name of the mutex on which the lock is performed. As is the case with Pthreads, it is important to remember that
represent serialization points in the program and therefore we must reduce the size of the critical sections as much as possible (in terms of
execution time) to get good performance.
 of instructions must represent a block  directive. The critical There are some obvious safeguards that must be noted while using the
structured block, i.e., no jumps are permitted into or out of the block. It is easy to see that the former would result in non-critical access and
the latter in an unreleased lock, which could cause the threads to wait indefinitely.
Often, a critical section consists simply of an update to a single memory location, for example, incrementing or adding to an integer.
 directive specifies that the memory atomic , for such atomic updates to memory locations. The atomic OpenMP provides another directive,
location update in the following instruction should be performed as an atomic operation. The update instruction can be one of the following
forms:
1 x binary_operation = expr
2 x++
3 ++x
4 x--
5 --x
 is one of {+, binary_operation  itself is an lvalue of scalar type, and x , x  is a scalar expression that does not include a reference to expr Here,
 directive only atomizes the load and store of the scalar variable. The atomic ,}. It is important to note that the , ||, , *, -, /, &,
evaluation of the expression is not atomic. Care must be taken to ensure that there are no race conditions hidden therein. This also
critical  directives can be replaced by atomic  directive cannot contain the updated variable itself. All atomic  term in the expr explains why the
directives provided they have the same name. However, the availability of atomic hardware instructions may optimize the performance of
 directives. critical the program, compared to translation to
 Directive ordered In-Order Execution: The
In many circumstances, it is necessary to execute a segment of a parallel loop in the order in which the serial version would execute it. For
. The list  of a list stored in array cumul_sum  loop in which, at some point, we compute the cumulative sum in array for example, consider a
. When cumul_sum[i] = cumul_sum[i-1] + list[i]  serially by executingi  loop over index for  can be computed using a cumul_sum array
 has been cumul_sum[i-1]  can be computed only after cumul_sum[i]  loop across threads, it is important to note that for executing this
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 block. ordered computed. Therefore, the statement would have to executed within an
 directive is as follows: ordered The syntax of the
1 #pragma omp ordered
2 structured block
 directive. parallel for  or for  loop, it must be within the scope of a for  directive refers to the in-order execution of a ordered Since the
 block. ordered  clause specified to indicate that the loop contains an ordered  directive must have the parallel for  or for Furthermore, the
Example 7.17 Computing the cumulative sum of a list using the ordered directive
 numbers of a list, we can add the current number to thei As we have just seen, to compute the cumulative sum of
 numbers of the list. This loop must, however, be executed in order. Furthermore, the cumulative sum i-1 cumulative sum of
ordered of the first element is simply the element itself. We can therefore write the following code segment using the
directive.
1 cumul_sum[0] = list[0];
2 #pragma omp parallel for private (i) \
3 shared (cumul_sum, list, n) ordered
4 for (i = 1; i < n; i++)
5 {
6 /* other processing on list[i] if needed */
7
8 #pragma omp ordered
9 {
10 cumul_sum[i] = cumul_sum[i-1] + list[i];
11 }
12 }
 directive represents an ordered serialization point in the program. Only a single thread can enter an ordered It is important to note that the
ordered block when all prior threads (as determined by loop indices) have exited. Therefore, if large portions of a loop are enclosed in
 directives, corresponding speedups suffer. In the above example, the parallel formulation is expected to be no faster than the ordered
 directive is for  directive. A single ordered  outside the list[i] serial formulation unless there is significant processing associated with
 block in it. ordered constrained to have only one
 Directive flush Memory Consistency: The
 directive provides a mechanism for making memory consistent across threads. While it would appear that such a directive is flush The
superfluous for shared address space machines, it is important to note that variables may often be assigned to registers and
 directive provides a memory fence by forcing a variable to be flush register-allocated variables may be inconsistent. In such cases, the
written to or read from the memory system. All write operations to shared variables must be committed to memory at a flush and all
references to shared variables after a fence must be satisfied from the memory. Since private variables are relevant only to a single
 directive applies only to shared variables. flush thread, the
 directive is as follows: flush The syntax of the
1 #pragma omp flush[(list)]
The optional list specifies the variables that need to be flushed. The default is that all shared variables are flushed.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, parallel , ordered , critical , at the entry and exit of barrier  is implied at a flush . Specifically, a flush Several OpenMP directives have an implicit
 clause is present. nowait  is not implied if a flush  blocks. A single , and sections , for  blocks and at the exit of parallel sections , and for parallel
 block. master  blocks and at entry or exit of a single , and sections , for It is also not implied at the entry of
7.10.4 Data Handling in OpenMP
One of the critical factors influencing program performance is the manipulation of data by threads. We have briefly discussed OpenMP
. We now examine these in greater detail, with a view lastprivate , and firstprivate , shared , private support for various data classes such as
to understanding how these classes should be used. We identify the following heuristics to guide the process:
If a thread initializes and uses a variable (such as loop indices) and no other thread accesses the data, then a local copy of the
. private variable should be made for the thread. Such data should be specified as
If a thread repeatedly reads a variable that has been initialized earlier in the program, it is beneficial to make a copy of the
variable and inherit the value at the time of thread creation. This way, when a thread is scheduled on the processor, the data can
reside at the same processor (in its cache if possible) and accesses will not result in interprocessor communication. Such data
. firstprivate should be specified as
If multiple threads manipulate a single piece of data, one must explore ways of breaking these manipulations into local
operations followed by a single global operation. For example, if multiple threads keep a count of a certain event, it is beneficial
to keep local counts and to subsequently accrue it using a single summation at the end of the parallel block. Such operations are
 clause. reduction supported by the
If multiple threads manipulate different parts of a large data structure, the programmer should explore ways of breaking it into
smaller data structures and making them private to the thread manipulating them.
After all the above techniques have been explored and exhausted, remaining data items may be shared among various threads
. shared using the clause
. threadprivate , OpenMP supports one additional data class called lastprivate , and firstprivate , shared , private In addition to
e  Often, it is useful to make a set of objects locally available to a thread in such a way that thes  Directives copyin  and threadprivate The
 variables, these private objects persist through parallel and serial blocks provided the number of threads remains the same. In contrast to
variables are useful for maintaining persistent objects across parallel regions, which would otherwise have to be copied into the master
threadprivate thread's data space and reinitialized at the next parallel block. This class of variables is supported in OpenMP using the
directive. The syntax of the directive is as follows:
1 #pragma omp threadprivate(variable_list)
 are local to each thread and are initialized once before they are accessed in a variable_list This directive implies that all variables in
parallel region. Furthermore, these variables persist across different parallel regions provided dynamic adjustment of the number of
threads is disabled and the number of threads is the same.
 variables across all threads in a threadprivate , OpenMP provides a mechanism for assigning the same value to firstprivate Similar to
. copyin(variable_list)  directives, is parallel parallel region. The syntax of the clause, which can be used with
7.10.5 OpenMP Library Functions
In addition to directives, OpenMP also supports a number of functions that allow a programmer to control the execution of threaded
programs. As we shall notice, these functions are similar to corresponding Pthreads functions; however, they are generally at a higher level
of abstraction, making them easier to use.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Controlling Number of Threads and Processors
The following OpenMP functions relate to the concurrency and number of processors used by a threaded program:
1 #include <omp.h>
2
3 void omp_set_num_threads (int num_threads);
4 int omp_get_num_threads ();
5 int omp_get_max_threads ();
6 int omp_get_thread_num ();
7 int omp_get_num_procs ();
8 int omp_in_parallel();
 directive parallel  sets the default number of threads that will be created on encountering the next omp_set_num_threads The function
 directive. This function must be called outsid the scope of a parallel region and parallel  clause is not used in the num_threads provided the
 or the Section 7.10.6  environment variable discussed in OMP_DYNAMIC dynamic adjustment of threads must be enabled (using either the
 library function). omp_set_dynamic
 function returns the number of threads participating in a team. It binds to the closest parallel directive and in omp_get_num_threads The
 function returns the maximum number of omp_get_max_threads the absence of a parallel directive, returns 1 (for master thread). The
 clause. The num_threads  directive encountered, which does not have a parallel threads that could possibly be created by a
omp  returns a unique thread i.d. for each thread in a team. This integer lies between 0 (for the master thread) and omp_get_thread_num
 function returns the number of processors that are available to execute the threaded omp_get_num_procs . The get_num_threads() -1
 returns a non-zero value if called from within the scope of a parallel region, and omp_in_parallel program at that point. Finally, the function
zero otherwise.
Controlling and Monitoring Thread Creation
The following OpenMP functions allow a programmer to set and monitor thread creation:
1 #include <omp.h>
2
3 void omp_set_dynamic (int dynamic_threads);
4 int omp_get_dynamic ();
5 void omp_set_nested (int nested);
6 int omp_get_nested ();
 function allows the programmer to dynamically alter the number of threads created on encountering a parallel omp_set_dynamic The
 evaluates to zero, dynamic adjustment is disabled, otherwise it is enabled. The function must be dynamic_threads region. If the value
called outside the scope of a parallel region. The corresponding state, i.e., whether dynamic adjustment is enabled or disabled, can be
, which returns a non-zero value if dynamic adjustment is enabled, and zero otherwise. omp_get_dynamic queried using the function
, is non-zero, and disables it otherwise. When nested nested  enables nested parallelism if the value of its argument, omp_set_nested The
parallelism is disabled, any nested parallel regions subsequently encountered are serialized. The state of nested parallelism can be
 function, which returns a non-zero value if nested parallelism is enabled, and zero otherwise. omp_get_nested queried using the
Mutual Exclusion
While OpenMP provides support for critical sections and atomic updates, there are situations where it is more convenient to use an explicit
lock. For such programs, OpenMP provides functions for initializing, locking, unlocking, and discarding locks. The lock data structure in
. The following functions are defined: omp_lock_t OpenMP is of type
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
1 #include <omp.h>
2
3 void omp_init_lock (omp_lock_t *lock);
4 void omp_destroy_lock (omp_lock_t *lock);
5 void omp_set_lock (omp_lock_t *lock);
6 void omp_unset_lock (omp_lock_t *lock);
7 int omp_test_lock (omp_lock_t *lock);
 function. When a lock is no longer needed, it must be omp_init_lock Before a lock can be used, it must be initialized. This is done using the
. It is illegal to initialize a previously initialized lock and destroy an uninitialized lock. Once a omp_destroy_lock discarded using the function
. On locking a previously omp_unset_lock  and omp_set_lock lock has been initialized, it can be locked and unlocked using the functions
. omp_set_lock unlocked lock, a thread gets exclusive access to the lock. All other threads must wait on this lock when they attempt an
Only a thread owning a lock can unlock it. The result of a thread attempting to unlock a lock owned by another thread is undefined. Both of
 can be used to attempt to set omp_test_lock these operations are illegal prior to initialization or after the destruction of a lock. The function
a lock. If the function returns a non-zero value, the lock has been successfully set, otherwise the lock is currently owned by another thread.
Similar to recursive mutexes in Pthreads, OpenMP also supports nestable locks that can be locked multiple times by the same thread. The
 and the corresponding functions for handling a nested lock are: omp_nest_lock_t lock object in this case is
1 #include <omp.h>
2
3 void omp_init_nest_lock (omp_nest_lock_t *lock);
4 void omp_destroy_nest_lock (omp_nest_lock_t *lock);
5 void omp_set_nest_lock (omp_nest_lock_t *lock);
6 void omp_unset_nest_lock (omp_nest_lock_t *lock);
7 int omp_test_nest_lock (omp_nest_lock_t *lock);
The semantics of these functions are similar to corresponding functions for simple locks. Notice that all of these functions have directly
corresponding mutex calls in Pthreads.
7.10.6 Environment Variables in OpenMP
OpenMP provides additional environment variables that help control execution of parallel programs. These environment variables include
the following.
 region. The parallel  This environment variable specifies the default number of threads created upon entering a OMP_NUM_THREADS
 directive. parallel  clause in the num_threads  function or the omp_set_num_threads number of threads can be changed using either the
 or if the function TRUE  is set to OMP_SET_DYNAMIC Note that the number of threads can be changed dynamically only if the variable
 prior to execution csh  has been called with a non-zero argument. For example, the following command, when typed into omp_set_dynamic
of the program, sets the default number of threads to 8.
1 setenv OMP_NUM_THREADS 8
omp_set_num , allows the number of threads to be controlled at runtime using the TRUE  This variable, when set to OMP_DYNAMIC
omp_set_dynamic  clause. Dynamic control of number of threads can be disabled by calling the num_threads  function or the threads
function with a zero argument.
 function omp_set_nested , enables nested parallelism, unless it is disabled by calling the TRUE  This variable, when set to OMP_NESTED
with a zero argument.
runtime  directives that use the for  This environment variable controls the assignment of iteration spaces associated with OMP_SCHEDULE
 along with optional chunk size. For example, the following guided , and dynamic , static scheduling class. The variable can take values
assignment:
1 setenv OMP_SCHEDULE "static,4"
 directives use static scheduling with a chunk size of 4. Other examples of assignments include: for specifies that by default, all
1 setenv OMP_SCHEDULE "dynamic"
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
2 setenv OMP_SCHEDULE "guided"
In each of these cases, a default chunk size of 1 is used.
7.10.7 Explicit Threads versus OpenMP Based Programming
OpenMP provides a layer on top of native threads to facilitate a variety of thread-related tasks. Using directives provided by OpenMP, a
programmer is rid of the tasks of initializing attributes objects, setting up arguments to threads, partitioning iteration spaces, etc. This
convenience is especially useful when the underlying problem has a static and/or regular task graph. The overheads associated with
automated generation of threaded code from directives have been shown to be minimal in the context of a variety of applications.
However, there are some drawbacks to using directives as well. An artifact of explicit threading is that data exchange is more apparent.
This helps in alleviating some of the overheads from data movement, false sharing, and contention. Explicit threading also provides a
richer API in the form of condition waits, locks of different types, and increased flexibility for building composite synchronization operations
. Finally, since explicit threading is used more widely than OpenMP, tools and support for Pthreads programs is Section 7.8 as illustrated in
easier to find.
A programmer must weigh all these considerations before deciding on an API for programming.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
7.11 Bibliographic Remarks
] LB95a , LB97 A number of excellent references exist for both explicit thread-based and OpenMP-based programming. Lewis and Berg [
] provide an excellent description of KSS95 provide a detailed guide to programming with Pthreads. Kleiman, Shah, and Smaalders [
thread systems as well as programming using threads. Several other books have also addressed programming and system software
]. ND96 , RRRR96 , Lew91 , Gal95 , But97 , NBF96 issues related to multithreaded programming [
Many other thread APIs and systems have also been developed and are commonly used in a variety of applications. These include Java
]. Sun95 , KSS95 ], and the Solaris threads API [ BW97 , Wil00 , CWP98 , PG98 ], Microsoft thread APIs [ Lea99 , Hyd99 , MK99 , Dra96 threads [
] from both the software as HLM84 Thread systems have a long and rich history of research dating back to the days of the HEP Denelcor [
], Active Threads HDM97 ], OxfordBSP [ LRZ95 , 95 + BJK well as the hardware viewpoints. More recently, software systems such as Cilk [
] have been developed. Hardware support for multithreading has been explored in the Tera computer 96 + HMT ], and Earth Manna [ Wei97[
], and Fra93 ], multiscalar architecture [ Tul96 , TEL95 ], simultaneous multithreading [ KS88 ], Horizon [ 91 + ADJ ], MIT Alewife [ RS90a system [
], among others. TY96 superthreaded architecture [
The performance aspects of threads have also been explored. Early work on the performance tradeoffs of multithreaded processors was
]. Consistency models for shared memory have been extensively studied. Other LB95b , CGL92 , Aga91 , SBCV90 , Aga89 reported in [
areas of active research include runtime systems, compiler support, object-based extensions, performance evaluation, and software
development tools. There have also been efforts aimed at supporting software shared memory across networks of workstations. All of
these are only tangentially related to the issue of programming using threads.
]. The OpenMP standard and an extensive set of 00 + CDK Due to its relative youth, relatively few texts exist for programming in OpenMP [
. A number of other articles (and special issues) have addressed issues relating to http://www.openmp.org resources is available at
]. Thr99 , LHZ98 , DM98 , CM98 , Bra97 OpenMP performance, compilation, and interoperability [
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
 Estimate the time taken for each of the following in Pthreads: 7.1
Thread creation.
Thread join.
Successful lock.
Successful unlock.
Successful trylock.
Unsuccessful trylock.
Condition wait.
Condition signal.
Condition broadcast.
In each case, carefully document the method used to compute the time for each of these function calls. Also
document the machine on which these observations were made.
 Implement a multi-access threaded queue with multiple threads inserting and multiple threads extracting from 7.2
the queue. Use mutex-locks to synchronize access to the queue. Document the time for 1000 insertions and 1000
extractions each by 64 insertion threads (producers) and 64 extraction threads (consumers).
 Repeat Problem 7.2 using condition variables (in addition to mutex locks). Document the time for the same test 7.3
case as above. Comment on the difference in the times.
a  A simple streaming media player consists of a thread monitoring a network port for arriving data, 7.4
g decompressor thread for decompressing packets and generating frames in a video sequence, and a renderin
– thread that displays frames at programmed intervals. The three threads must communicate via shared buffers
. an in-buffer between the network and decompressor, and an out-buffer between the decompressor and renderer
 to gather listen_to_port Implement this simple threaded framework. The network thread calls a dummy function
data from the network. For the sake of this program, this function generates a random string of bytes of desired
, which takes in data from the in-buffer and returns a decompress length. The decompressor thread calls function
frame of predetermined size. For this exercise, generate a frame with random bytes. Finally the render thread
picks frames from the out buffer and calls the display function. This function takes a frame as an argument, and for
this exercise, it does nothing. Implement this threaded framework using condition variables. Note that you can
easily change the three dummy functions to make a meaningful streaming media decompressor.
 Illustrate the use of recursive locks using a binary tree search algorithm. The program takes in a large list of 7.5
numbers. The list is divided across multiple threads. Each thread tries to insert its elements into the tree by using
a single lock associated with the tree. Show that the single lock becomes a bottleneck even for a moderate
number of threads.
 Improve the binary tree search program by associating a lock with each node in the tree (as opposed to a 7.6
single lock with the entire tree). A thread locks a node when it reads or writes it. Examine the performance
properties of this implementation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 Improve the binary tree search program further by using read-write locks. A thread read-locks a node before 7.7
reading. It write-locks a node only when it needs to write into the tree node. Implement the program and document
the range of program parameters where read-write locks actually yield performance improvements over regular
locks.
 Implement a threaded hash table in which collisions are resolved by chaining. Implement the hash table so that 7.8
 hash-table entries. Threads attempting to read/write an element k there is a single lock associated with a block of
in a block must first lock the corresponding block. Examine the performance of your implementation as a function
. k of
 Change the locks to read-write locks in the hash table and use write locks only when inserting an entry into the 7.9
. Compare the performance to that obtained k linked list. Examine the performance of this program as a function of
using regular locks.
 Write a threaded program for computing the Sieve of Eratosthenes. Think through the threading strategy 7.10
carefully before implementing it. It is important to realize, for instance, that you cannot eliminate multiples of 6 from
the sieve until you have eliminated multiples of 3 (at which point you would realize that you did not need to
eliminate multiples of 6 in the first place). A pipelined (assembly line) strategy with the current smallest element
forming the next station in the assembly line is one way to think about the problem.
 Write a threaded program for solving a 15-puzzle. The program takes an initial position and keeps an open 7.11
list of outstanding positions. This list is sorted on the "goodness" measure of the boards. A simple goodness
-displacement of every tile from where it y  -displacement and x measure is the Manhattan distance (i.e., the sum of
needs to be). This open list is a work queue implemented as a heap. Each thread extracts work (a board) from the
work queue, expands it to all possible successors, and inserts the successors into the work queue if it has not
already been encountered. Use a hash table (from Problem 7.9) to keep track of entries that have been previously
encountered. Plot the speedup of your program with the number of threads. You can compute the speedups for
some reference board that is the same for various thread counts.
). Now each thread picks a random k  Modify the above program so that you now have multiple open lists (say 7.12
open list and tries to pick a board from the random list and expands and inserts it back into another, randomly
selected list. Plot the speedup of your program with the number of threads. Compare your performance with the
previous case. Make sure you use your locks and trylocks carefully to minimize serialization overheads.
. Use the Example 7.14  Implement and test the OpenMP program for computing a matrix-matrix product in 7.13
OMP_NUM_THREADS environment variable to control the number of threads and plot the performance with
varying numbers of threads. Consider three cases in which (i) only the outermost loop is parallelized; (ii) the outer
two loops are parallelized; and (iii) all three loops are parallelized. What is the observed result from these three
cases?
 containing a programmable delay. All invocations of the dummy  Consider a simple loop that calls a function 7.14
guided , and dynamic , static function are independent of the others. Partition this loop across four threads using
scheduling. Use different parameters for static and guided scheduling. Document the result of this experiment as
 function becomes large. dummy the delay within the
 Consider a sparse matrix stored in the compressed row format (you may find a description of this format on 7.15
the web or any suitable text on sparse linear algebra). Write an OpenMP program for computing the product of this
) and http://math.nist.gov/MatrixMarket/ matrix with a vector. Download sample matrices from the Matrix Market (
test the performance of your implementation as a function of matrix size and number of threads.
 task and a producer  to create a single sections  Implement a producer-consumer framework in OpenMP using 7.16
 task. Ensure appropriate synchronization using locks. Test your program for a varying number of consumer single
producers and consumers.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 8. Dense Matrix Algorithms
Algorithms involving matrices and vectors are applied in several numerical and non-numerical contexts. This chapter discusses some key
 that have no or few known usable zero entries. We deal specifically with square matrices for full matrices  or dense algorithms for
pedagogical reasons, but the algorithms in this chapter, wherever applicable, can easily be adapted for rectangular matrices as well.
Due to their regular structure, parallel computations involving matrices and vectors readily lend themselves to data-decomposition
). Depending on the computation at hand, the decomposition may be induced by partitioning the input, the output, or the Section 3.2.2 (
 describes in detail the various schemes of partitioning matrices for parallel computation. The algorithms Section 3.4.1 intermediate data.
discussed in this chapter use one- and two-dimensional block, cyclic, and block-cyclic partitionings. For the sake of brevity, we will
henceforth refer to one- and two-dimensional partitionings as 1-D and 2-D partitionings, respectively.
Another characteristic of most of the algorithms described in this chapter is that they use one task per process. As a result of a
one-to-one mapping of tasks to processes, we do not usually refer to the tasks explicitly and decompose or partition the problem directly
into processes.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
8.1 Matrix-Vector Multiplication
Algorithm . y  x 1 result vector n  to yield the x  x 1 vector n  with an A  matrix n  x n This section addresses the problem of multiplying a dense
 multiplications and additions. Assuming that a 2 n  shows a serial algorithm for this problem. The sequential algorithm requires 8.1
multiplication and addition pair takes unit time, the sequential run time is
1  Equation 8.
At least three distinct parallel formulations of matrix-vector multiplication are possible, depending on whether rowwise 1-D, columnwise
1-D, or a 2-D partitioning is used.
 to yield x  x 1 vector n  with an A  matrix n  x n Algorithm 8.1 A serial algorithm for multiplying an
. y  x 1 product vector n an
) y , x , A  MAT_VECT ( procedure 1.
begin 2.
do  - 1 n to  := 0i for 3.
begin 4.
]:=0;i[ y 5.
do  - 1 n to  := 0 j for 6.
]; j[ x ] x j , i[ A ] +i[ y ] :=i[ y 7.
; endfor 8.
 MAT_VECT end 9.
8.1.1 Rowwise 1-D Partitioning
This section details the parallel algorithm for matrix-vector multiplication using rowwise block 1-D partitioning. The parallel algorithm for
 describes the Figure 8.1 columnwise block 1-D partitioning is similar (Problem 8.2) and has a similar expression for parallel run time.
distribution and movement of data for matrix-vector multiplication with block 1-D partitioning.
 x 1 vector using rowwise block 1-D n  matrix with an n  x n Figure 8.1. Multiplication of an
. n  = p partitioning. For the one-row-per-process case,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
One Row Per Process
 processes so that each process stores one complete row of the n  matrix is partitioned among n  x n First, consider the case in which the
 is distributed such that each process owns one of its elements. The initial distribution of the matrix and the vector x  x 1 vector n matrix. The
-1] and is responsible for n , i[ A , 1], ..., i[ A , 0], i[ A ] andi[ x  initially ownsi . Process P Figure 8.1(a) for rowwise block 1-D partitioning is shown in
); hence, every process needs the entire vector. Since each Algorithm 8.1  is multiplied with each row of the matrix ( x ]. Vectori[ y computing
Figure 8.1(b) , an all-to-all broadcast is required to distribute all the elements to all the processes. x process starts with only one element of
 computesi ), process P Figure 8.1(c)  is distributed among the processes ( x illustrates this communication step. After the vector
 is stored y  shows, the result vector Figure 8.1(d) ). As Algorithm 8.1  (lines 6 and 7 of
 was stored. x exactly the way the starting vector
 processes requires n  Starting with one vector element per process, the all-to-all broadcast of the vector elements among Parallel Run Time
). n ( Q  is also performed by each process in time x  with A ). The multiplication of a single row of Table 4.1 ) on any architecture ( n ( Q time
). The parallel algorithm is 2 n ( Q ), resulting in a process-time product of n ( Q  processes in time n Thus, the entire procedure is completed by
). 2 n ( Q cost-optimal because the complexity of the serial algorithm is
Using Fewer than n Processes
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, and the matrix is partitioned among the processes by using block 1-D n  < p  processes are used such that p Consider the case in which
 must be x . Since the vector p/ n  complete rows of the matrix and a portion of the vector of size p/ n partitioning. Each process initially stores
multiplied with each row of the matrix, every process needs the entire vector (that is, all the portions belonging to separate processes).
 processes and p . The all-to-all broadcast takes place among (c)  and Figure 8.1(b) This again requires an all-to-all broadcast as shown in
 elements p/ n  to produce x  rows with the vector p/ n . After this communication step, each process multiplies its p/ n involves messages of size
. x  is distributed in the same format as that of the starting vector y  shows that the result vector Figure 8.1(d) of the result vector.
)( p / n ( wt  + p  log st  processes takes time p  among p/ n , an all-to-all broadcast of messages of size Table 4.1  According to Parallel Run Time
p/ n  multiplying its p/ 2 n . After the communication, each process spends time n wt  + p  log st , this can be approximated by p  - 1). For large p
rows with the vector. Thus, the parallel run time of this procedure is
2  Equation 8.
). n ( O  = p . The algorithm is cost-optimal for np wt  + p  log p st  + 2 n The process-time product for this parallel formulation is
Section 5.4.2  We now derive the isoefficiency function for matrix-vector multiplication along the lines of the analysis in Scalability Analysis
 for the hypercube Equation 8.2 by considering the terms of the overhead function one at a time. Consider the parallel run time given by
 gives the following expression for the overhead function of matrix-vector multiplication on a W  - P pT  = o T architecture. The relation
hypercube with block 1-D partitioning:
3  Equation 8.
), Equation 5.14  ( o KT  = W  that the central relation that determines the isoefficiency function of a parallel algorithm is Chapter 5 Recall from
, o T  term of st  is the desired efficiency. Rewriting this relation for matrix-vector multiplication, first with only the E ) and E /(1 - E  = K where
4  Equation 8.
 term of the overhead function, wt  gives the isoefficiency term with respect to message startup time. Similarly, for the Equation 8.4
) as follows: wt  (that is, the isoefficiency function due to wt  , and K , p  in terms of W ), we derive an expression for Equation 8.1  ( 2 n  = W Since
5  Equation 8.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processes can be used to n Now consider the degree of concurrency of this parallel algorithm. Using 1-D partitioning, a maximum of
), which yields the following condition: n ( O  is p  x 1 vector. In other words, n  matrix with an n  x n multiply an
6  Equation 8.
8.5 . Among the three, Equations 8.6 , and 8.5 , 8.4 The overall asymptotic isoefficiency function can be determined by comparing Equations
 give the highest asymptotic rate at which the problem size must increase with the number of processes to maintain a fixed 8.6 and
) is the asymptotic isoefficiency function of the parallel matrix-vector multiplication algorithm with 1-D 2 p ( Q efficiency. This rate of
partitioning.
8.1.2 2-D Partitioning
This section discusses parallel matrix-vector multiplication for the case in which the matrix is distributed among the processes using a
 shows the distribution of the matrix and the distribution and movement of vectors among the processes. Figure 8.2 block 2-D partitioning.
Figure 8.2. Matrix-vector multiplication with block 2-D partitioning. For the
. n  x n  if the matrix size is 2 n  = p one-element-per-process case,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
One Element Per Process
 processes such that each process owns a single element. 2 n  matrix is partitioned among n  x n We start with the simple case in which an
 processes, each of which owns one element of the vector. Since the algorithm n  is distributed only in the last column of x  x 1 vector n The
 with the corresponding elements in each row of the matrix, the vector must be distributed such that x multiplies the elements of the vector
Figure th element of each row of the matrix. The communication steps for this are shown ini th element of the vector is available to thei the
. Before the multiplication, the elements of the matrix and the vector must be Figure 8.1  to Figure 8.2 . Notice the similarity of (b)  and 8.2(a)
. However, the vector communication steps differ between various partitioning strategies. Figure 8.1(c) in the same relative locations as in
), but for 2-D partitioning, the Figure 8.1 With 1-D partitioning, the elements of the vector cross only the horizontal partition-boundaries (
). Figure 8.2 vector elements cross both horizontal and vertical partition-boundaries (
 along the principal diagonal of the matrix. x  shows, the first communication step for the 2-D partitioning aligns the vector Figure 8.2(a) As
Often, the vector is stored along the diagonal instead of the last column, in which case this step is not required. The second step copies
 shows, this step Figure 8.2(b) the vector elements from each diagonal process to all the processes in the corresponding column. As
 simultaneous one-to-all broadcast operations, one in each column of processes. After these two communication steps, each n consists of
, the products computed for each row y . To obtain the result vector x process multiplies its matrix element with the corresponding element of
 shows this step, which requires an all-to-one reduction Figure 8.2(c) must be added, leaving the sums in the last column of processes.
) in each row with the last process of the row as the destination. The parallel matrix-vector multiplication is complete after the Section 4.1 (
reduction step.
g  Three basic communication operations are used in this algorithm: one-to-one communication to align the vector alon Parallel Run Time
 processes of each column, and all-to-one reduction in each n the main diagonal, one-to-all broadcast of each vector element among the
). Since each process performs a single multiplication in constant time, the overall parallel n (log Q row. Each of these operations takes time
); hence, the algorithm is not cost-optimal. n  log 2 n ( Q ). The cost (process-time product) is n ( Q run time of this algorithm is
 Processes 2 n Using Fewer than
A cost-optimal parallel implementation of matrix-vector multiplication with block 2-D partitioning of the matrix can be obtained if the
 processes. 2 n granularity of computation at each process is increased by using fewer than
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 block of the  processes in which each process owns an p Consider a logical two-dimensional mesh of
 also illustrates the initial Figure 8.2  elements in the last process-column only. matrix. The vector is distributed in portions of
data-mapping and the various communication steps for this case. The entire vector must be distributed on each row of processes before
the multiplication can be performed. First, the vector is aligned along the main diagonal. For this, each process in the rightmost column
 vector elements to the diagonal process in its row. Then a columnwise one-to-all broadcast of these sends its
 sets of products. At the end of this  multiplications and locally adds the p/ 2 n elements takes place. Each process then performs
 partial sums that must be accumulated along each row to obtain the result , each process has Figure 8.2(c) step, as shown in
 values in each row, with the rightmost process of vector. Hence, the last step of the algorithm is an all-to-one reduction of the
the row as the destination.
 from the rightmost process of a row to the diagonal process  The first step of sending a message of size Parallel Run Time
. We can perform the columnwise one-to-all broadcast in at most time ) takes time Figure 8.2(a) (
. Ignoring the time to perform additions, the Section 4.1.3  by using the procedure described in
final rowwise all-to-one reduction also takes the same amount of time. Assuming that a multiplication and addition pair takes unit time,
 time in computation. Thus, the parallel run time for this procedure is as follows: p/ 2 n each process spends approximately
7  Equation 8.
), we get the following expression Equation 5.1  ( W  - p pT  = o T , and applying the relation 8.7  and 8.1  By using Equations Scalability Analysis
for the overhead function of this parallel algorithm:
8  Equation 8.
 by considering the terms of the overhead function Section 5.4.2 We now perform an approximate isoefficiency analysis along the lines of
 yields Equation 5.14  term of the overhead function, st one at a time (see Problem 8.4 for a more precise isoefficiency analysis). For the
9  Equation 8.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 by wt  gives the isoefficiency term with respect to the message startup time. We can obtain the isoefficiency function due to Equation 8.9
, we get the following: Equation 5.14 . Using the isoefficiency relation of 2 n  with the problem size p  log balancing the term
0  Equation 8.1
 processes can be used), we arrive at 2 n  (that is, a maximum of 2 n Finally, considering that the degree of concurrency of 2-D partitioning is
the following relation:
1  Equation 8.1
, the one with the largest right-hand side expression determines the overall isoefficiency function of 8.11 , and 8.10 , 8.9 Among Equations
this parallel algorithm. To simplify the analysis, we ignore the impact of the constants and consider only the asymptotic rate of the growth
) clearly Equation 8.10  ( wt of problem size that is necessary to maintain constant efficiency. The asymptotic isoefficiency term due to
). Therefore, the overall asymptotic isoefficiency Equation 8.11 ) and due to concurrency ( Equation 8.9  ( st dominates the ones due to
). p 2  log p ( Q function is given by
), the p 2  log p ( Q ). With an isoefficiency function of Section 5.4.3 The isoefficiency function also determines the criterion for cost-optimality (
 is determined by the following relations: W maximum number of processes that can be used cost-optimally for a given problem size
2  Equation 8.1
Ignoring the lower-order terms,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, Equation 8.12  in p  for log n Substituting log
3  Equation 8.1
n  gives an asymptotic upper bound on the number of processes that can be used cost-optimally for an Equation 8.13 The right-hand side of
 matrix-vector multiplication with a 2-D partitioning of the matrix. n x
Comparison of 1-D and 2-D Partitionings
 shows that matrix-vector multiplication is faster with block 2-D partitioning of the matrix than with 8.7  and 8.2 A comparison of Equations
, then the 1-D partitioning cannot be n block 1-D partitioning for the same number of processes. If the number of processes is greater than
, the analysis in this section suggests that 2-D partitioning is n used. However, even if the number of processes is less than or equal to
preferable.
Among the two partitioning schemes, 2-D partitioning has a better (smaller) asymptotic isoefficiency function. Thus, matrix-vector
multiplication is more scalable with 2-D partitioning; that is, it can deliver the same efficiency on more processes with 2-D partitioning than
with 1-D partitioning.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
8.2 Matrix-Matrix Multiplication
. All B  x A  = C  to yield the product matrix B  and A  dense, square matrices n  x n This section discusses parallel algorithms for multiplying two
. If we assume Algorithm 8.2 parallel matrix multiplication algorithms in this chapter are based on the conventional serial algorithm shown in
. Matrix multiplication 3 n that an addition and multiplication pair (line 8) takes unit time, then the sequential run time of this algorithm is
algorithms with better asymptotic sequential complexities are available, for example Strassen's algorithm. However, for the sake of
simplicity, in this book we assume that the conventional algorithm is the best available serial algorithm. Problem 8.5 explores the
performance of parallel matrix multiplication regarding Strassen's method as the base algorithm.
 matrices. n  x n Algorithm 8.2 The conventional serial algorithm for multiplication of two
) C , B , A  MAT_MULT ( procedure 1.
begin 2.
do  - 1 n to  := 0i for 3.
do  - 1 n to  := 0 j for 4.
begin 5.
] := 0; j , i[ C 6.
do  - 1 n to  := 0 k for 7.
]; j , k[ B ] x k , i[ A ] + j , i[ C ] := j , i[ C 8.
; endfor 9.
 MAT_MULT end 10.
 matrices with a block size of n  x n Algorithm 8.3 The block matrix multiplication algorithm for
). q / n ) x ( q / n(
) C , B , A  BLOCK_MAT_MULT ( procedure 1.
begin 2.
do  - 1 q to  := 0i for 3.
do  - 1 q to  := 0 j for 4.
begin 5.
 to zero; j, i C 6. Initialize all elements of
do  - 1 q to  := 0 k for 7.
; j, k B  x k, i A  + j, i C  := j, i C 8.
; endfor 9.
 BLOCK_MAT_MULT end 10.
A concept that is useful in matrix multiplication as well as in a variety of other matrix algorithms is that of block matrix operations. We can
often express a matrix computation involving scalar algebraic operations on all its elements in terms of identical matrix algebraic operations
. For block matrix operations on blocks or submatrices of the original matrix. Such algebraic operations on the submatrices are called
) submatrix. q / n ) x ( q / n ) such that each block is an ( q  < j , i  (0 j, i A  array of blocks q  x q  can be regarded as a A  matrix n  x n example, an
, in which the multiplication and addition Algorithm 8.3  can then be rewritten as Algorithm 8.2 The matrix multiplication algorithm in
8.3  and Algorithm 8.2 operations on line 8 are matrix multiplication and matrix addition, respectively. Not only are the final results of
 additions and 3 n  performs Algorithm 8.2 identical, but so are the total numbers of scalar additions and multiplications performed by each.
 additions and 3 ) q / n ) matrices and requiring ( q / n ) x ( q / n  matrix multiplications, each involving ( 3 q  performs Algorithm 8.3 multiplications, and
 and  processes to implement the block version of matrix multiplication in parallel by choosing p multiplications. We can use
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 block at each process. j, i C computing a distinct
. Each of the following parallel matrix multiplication Algorithm 8.3 In the following sections, we describe a few ways of parallelizing
algorithms uses a block 2-D partitioning of the matrices.
8.2.1 A Simple Parallel Algorithm
 of size j, i B  and j, i A  blocks p  partitioned into B  and A  matrices n  x n Consider two
 to 0,0  logical mesh of processes. The processes are labeled from P each. These blocks are mapped onto a
j, i C  of the result matrix. Computing submatrix j, i C  and computes block j, i B  and j, i A  initially stores i,j . Process P
's A  To acquire all the required blocks, an all-to-all broadcast of matrix.  for j, k B  and k, i A requires all submatrices
j, i 's blocks is performed in each column. After P B blocks is performed in each row of processes, and an all-to-all broadcast of matrix
, it performs the submatrix multiplication  and acquires
. Algorithm 8.3 and addition step of lines 7 and 8 in
 concurrent  The algorithm requires two all-to-all broadcast steps (each consisting of Performance and Scalability Analysis
 processes. The messages consist of submatrices of broadcasts in all rows and columns of the process mesh) among groups of
n
. After the , the total communication time is Table 4.1  elements. From p/ 2
 multiplications of , which requires j, i C communication step, each process computes a submatrix
. Thus, . This takes a total of time  with Algorithm 8.3 submatrices (lines 7 and 8 of
the parallel run time is approximately
4  Equation 8.1
). 2 n ( O  = p , and the parallel algorithm is cost-optimal for The process-time product is
, respectively. Hence, the overall isoefficiency function due to the 2 / 3 p 3 ) wt  and 8( p  log p st  are wt  and st The isoefficiency functions due to
. Therefore, the 2 / 3 p 3 n  or 2 n p  processes; hence, 2 n ). This algorithm can use a maximum of 3/2 p ( Q communication overhead is
). 3/2 p ( Q isoefficiency function due to concurrency is also
A notable drawback of this algorithm is its excessive memory requirements. At the end of the communication phase, each process has
 memory. The Q ) memory, each process requires p/ 2 n ( Q . Since each block requires B  and A  blocks of both matrices
 times the memory requirement of the sequential , which is Q total memory requirement over all the processes is
algorithm.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
8.2.2 Cannon's Algorithm
. To study this algorithm, we again Section 8.2.1 Cannon's algorithm is a memory-efficient version of the simple algorithm presented in
, and initially assign submatrices  to 0,0  square blocks. We label the processes from P p  into B  and A partition matrices
, it is  submatrices  th row requires alli . Although every process in the j, i  to process P j, i B  and j, i A
th row such that, at any given time, each process is using a differenti  processes of the possible to schedule the computations of the
. These blocks can be systematically rotated among the processes after every submatrix multiplication so that every process gets a k, i A
 after each rotation. If an identical schedule is applied to the columns, then no process holds more than one block of each matrix k, i A fresh
). Cannon's algorithm is based on this idea. 2 n ( Q at any time, and the total memory requirement of the algorithm over all the processes is
 for 16 Figure 8.3 The scheduling for the multiplication of submatrices on separate processes in Cannon's algorithm is illustrated in
processes.
Figure 8.3. The communication steps in Cannon's algorithm on 16 processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 in such a way that each process multiplies its local submatrices. B  and A The first communication step of the algorithm aligns the blocks of
 steps.i  to the left (with wraparound) by j, i A  by shifting all submatrices A  shows, this alignment is achieved for matrix Figure 8.3(a) As
 steps. These are circular shift operations j  are shifted up (with wraparound) by j, i B , all submatrices Figure 8.3(b) Similarly, as shown in
 and  with submatrices j, i ) in each row and column of processes, which leave process P Section 4.6 (
 after the initial alignment, when each process is ready for the first B  and A  shows the blocks of Figure 8.3(c) .
 moves one step up B  moves one step left and each block of A submatrix multiplication. After a submatrix multiplication step, each block of
 such submatrix multiplications and single-step shifts pairs up . A sequence of Figure 8.3(d) (again with wraparound), as shown in
. B  and A . This completes the multiplication of matrices j, i  at P  for j, k B  and k, i A each
 and (b)) involves a rowwise and a columnwise circular shift. Figure 8.3(a)(  The initial alignment of the two matrices Performance Analysis
st . The two shift operations require a total of time 2( In any of these shifts, the maximum distance over which a block shifts is
. Thus, the p/ 2 n wt  + st  single-step shifts in the compute-and-shift phase of the algorithm takes time ). Each of the Table 4.1 ) ( p/ 2 n wt +
 on a p . For large enough total communication time (for both matrices) during this phase of the algorithm is
network with sufficient bandwidth, the communication time for the initial alignment can be disregarded in comparison with the time spent in
communication during the compute-and-shift phase.
 submatrices. Assuming that a multiplication and addition  multiplications of Each process performs
. Thus, the approximate overall parallel run time of this p/ 3 n pair takes unit time, the total time that each process spends in computation is
algorithm is
5  Equation 8.1
. As in the simple Section 8.2.1 The cost-optimality condition for Cannon's algorithm is identical to that for the simple algorithm presented in
). 3/2 p ( Q algorithm, the isoefficiency function of Cannon's algorithm is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
8.2.3 The DNS Algorithm
The matrix multiplication algorithms presented so far use block 2-D partitioning of the input and the output matrices and use a maximum of
n
) operations in the 3 n ( Q ) because there are n ( W  matrices. As a result, these algorithms have a parallel run time of n  x n  processes for 2
 processes and that 3 n serial algorithm. We now present a parallel algorithm based on partitioning intermediate data that can use up to
) processes. This algorithm is known as the DNS algorithm because it is n /log 3 n ( W ) by using n (log Q performs matrix multiplication in time
due to Dekel, Nassimi, and Sahni.
 processes are available for multiplying 3 n We first introduce the basic idea, without concern for inter-process communication. Assume that
 logical array. Since the matrix multiplication algorithm n  x n  x n  matrices. These processes are arranged in a three-dimensional n  x n two
 processes is assigned a single scalar multiplication. The processes are labeled according 3 n  scalar multiplications, each of the 3 n performs
). After each process n  < k , j , i  (0 k, j, i ] is assigned to process P j , k[ B ] x k , i[ A to their location in the array, and the multiplication
] can be carried j , i  [ C ]. The additions for all j , i  [ C  are added to obtain -1 n, j, i , ..., P ,1 j, i , P ,0 j, i performs a single multiplication, the contents of P
) to multiply the n (log Q  steps to add; that is, it takes time n  steps each. Thus, it takes one step to multiply and log n out simultaneously in log
 matrices by this algorithm. n  x n
 shows, the process Figure 8.4 We now describe a practical parallel implementation of matrix multiplication based on this idea. As
. Initially, as shown in k  processes each. Each plane corresponds to a different value of n  x n  planes of n arrangement can be visualized as
 = 0 at the base of the three-dimensional k  processes of the plane corresponding to 2 n , the matrices are distributed among the Figure 8.4(a)
]. j , i[ B ] and j , i[ A  initially owns ,0 j, i process array. Process P
A Figure 8.4. The communication steps in the DNS algorithm while multiplying 4 x 4 matrices
A  on 64 processes. The shaded processes in part (c) store elements of the first row of B and
. B and the shaded processes in part (d) store elements of the first column of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
B  and columns of A ]. Therefore, rows of j [*, B , *] and column i[ A  computes the dot product of row ,* j, i The vertical column of processes P
]. More precisely, process j [*, B , *] and column i[ A  has row ,* j, i need to be moved appropriately so that each vertical column of processes P
]. j , k[ B ] and k , i[ A  should have k, j, i P
. First, each column (c) – Figure 8.4(a)  among the processes is shown in A The communication pattern for distributing the elements of matrix
 as it initially did in the j  = k  th column occupies the same position in the plane corresponding to j  moves to a different plane such that the A of
A . Now all the columns of Figure 8.4(b)  is shown in j, j, i  to P ,0 j, i ] from P j , i[ A  after moving A  = 0. The distribution of k plane corresponding to
Figure  axis. The result of this step is shown in j  times in their respective planes by a parallel one-to-all broadcast along the n are replicated
. At this point, each vertical column of processes j, j, i ] from P j , i[ A  receive a copy of j -1, n, i , ..., P j ,1, i , P j ,0, i  processes P n , in which the 8.4(c)
]. k , i[ A  has k, j, i , *]. More precisely, process P i[ A  has row ,* j, i P
 in process subscripts are switched. In the first one-to-one j  andi , the communication steps are similar, but the roles of B For matrix
B . The distribution of i, j -1, n , ..., P i, j 1, , P i, j 0,  among Pi, j, i . Then it is broadcast from P i, j, i  to P ,0 j, i ] is moved from P j , i[ B communication step,
 has column ,* j, i . At this point, each vertical column of processes P Figure 8.4(d)  axis is shown ini after this one-to-all broadcast along the
]. k , i[ A ], in addition to j , k[ B  has k, j, i ]. Now process P j [*, B
] of the product matrix is obtained by an j , i[ C . Now each element k, j, i ] are multiplied at P j , k[ B ] and k , i[ A After these communication steps,
, ..., ,1 j, i  accumulates the results of the multiplication from processes P ,0 j, i  axis. During this step, process P k all-to-one reduction along the
[0, 0]. C  shows this step for Figure 8.4 . -1 n, j, i P
 to their respective planes, (2) B  and the rows of A The DNS algorithm has three main communication steps: (1) moving the columns of
 axis. All these k , and (3) all-to-one reduction along the B  axis fori  and along the A  axis for j performing one-to-all broadcast along the
 matrices n  x n ). Thus, the parallel run time for multiplying two n (log Q  processes and take time n operations are performed within groups of
). n (log Q  processes is 3 n using the DNS algorithm on
 Processes 3 n DNS Algorithm with Fewer than
) sequential 3 n ( Q ) exceeds the n  log 3 n ( Q  processes, since its process-time product of 3 n The DNS algorithm is not cost-optimal for
 processes. Another 3 n complexity of matrix multiplication. We now present a cost-optimal version of this algorithm that uses fewer than
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processes is described in Problem 8.6. 3 n variant of the DNS algorithm that uses fewer than
. To implement the DNS algorithm, the two matrices are partitioned n  < q  for some 3 q  is equal to p Assume that the number of processes
 two-dimensional square array of blocks. The implementation of q  x q ). Each matrix can thus be regarded as a q / n ) x ( q / n into blocks of size (
 processes. The only difference is that now we operate on blocks rather than on 3 n  processes is very similar to that on 3 q this algorithm on
. 3 n , the number of processes can vary between 1 and n q individual elements. Since 1
 for each matrix. 2 ) q / n ( wt  + st , and takes time B  and A  The first one-to-one communication step is performed for both Performance Analysis
 for each matrix. The q  log 2 ) q / n ( wt  + q  log st The second step of one-to-all broadcast is also performed for both matrices and takes time
) q / n ) x ( q / n . The multiplication of ( q  log 2 ) q / n ( wt  + q  log st )and takes time C final all-to-one reduction is performed only once (for matrix
. We can ignore the communication time for the first one-to-one communication step 3 ) q / n submatrices by each process takes time (
because it is much smaller than the communication time of one-to-all broadcasts and all-to-one reduction. We can also ignore the
computation time for addition in the final reduction phase because it is of a smaller order of magnitude than the computation time for
multiplying the submatrices. With these assumptions, we get the following approximate expression for the parallel run time of the DNS
algorithm:
, we get 1/3 p  = q Since
6  Equation 8.1
). The algorithm is 3 ) p (log p ( Q . The isoefficiency function is p  log 3 / 1 p 2 n wt  + p  log p st  + 3 n The total cost of this parallel algorithm is
). 3 ) n /(log 3 n ( O  = p ), or 3 ) p (log p ( W  = 3 n cost-optimal for
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
8.3 Solving a System of Linear Equations
This section discusses the problem of solving a system of linear equations of the form
, 0 b  x 1 vector [ n  is an b , j, i a ] = j , i[ A  matrix of coefficients such that n  x n  is a dense A . Here b  = Ax In matrix notation, this system is written as
].i  [ x  byi x ] and j , i[ A  by j, i a . We will make all subsequent references to T ] -1 n x , ..., 1 x , 0 x  is the desired solution vector [ x , and T ] -1 n b , ..., 1 b
 is usually solved in two stages. First, through a series of algebraic manipulations, the original system of b  = Ax A system of equations
equations is reduced to an upper-triangular system of the form
l  is a unit upper-triangular matrix – one in which all subdiagonal entries are zero and all principal diagona U , where y  = Ux We write this as
. In the second stage of n  <i ] = 1 for 0i , i[ U . Furthermore, j, i u ] = j , i[ U , otherwise j  >i ] = 0 if j , i[ U entries are equal to one. Formally,
[0] by a x  - 1] to n[ x solving a system of linear equations, the upper-triangular system is solved for the variables in reverse order from
). Section 8.3.3  ( back-substitution procedure known as
. In 8.3.2  and 8.3.1 We discuss parallel formulations of the classical Gaussian elimination method for upper-triangularization in Sections
, we describe a straightforward Gaussian elimination algorithm assuming that the coefficient matrix is nonsingular, and its Section 8.3.1
 discusses the case in which a numerically Section 8.3.2 rows and columns are permuted in a way that the algorithm is numerically stable.
stable solution of the system of equations requires permuting the columns of the matrix during the execution of the Gaussian elimination
algorithm.
A Although we discuss Gaussian elimination in the context of upper-triangularization, a similar procedure can be used to factorize matrix
. This factorization is commonly referred to U  x L  = A  so that U  and a unit upper-triangular matrix L as the product of a lower-triangular matrix
. Performing LU factorization (rather than upper-triangularization) is particularly useful if multiple systems of equations LU factorization as
 gives a procedure for column-oriented LU factorization. Algorithm 3.3  need to be solved. Ax with the same left-hand side
8.3.1 A Simple Gaussian Elimination Algorithm
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The serial Gaussian elimination algorithm has three nested loops. Several variations of the algorithm exist, depending on the order in
 shows one variation of Gaussian elimination, which we will adopt for parallel implementation in Algorithm 8.4 which the loops are arranged.
. We y  = Ux  to a unit upper-triangular system b  = Ax the remainder of this section. This program converts a system of linear equations
] computed on line 6 of j , k[ A . The element A  and overwrites the upper-triangular portion of A  shares storage with U assume that the matrix
 0 ] k , k[ A  assumes that Algorithm 8.4 ]. k , k  [ U ] equated to 1 on line 8 is k , k[ A ]. Similarly, the element j , k  [ U  is actually Algorithm 8.4
when it is used as a divisor on lines 6 and 7.
Algorithm 8.4 A serial Gaussian elimination algorithm that converts the system of linear
 occupies the U . The matrix y  = Ux  to a unit upper-triangular system b  = Ax equations
 0 when it is used as a ] k , k[ A . This algorithm assumes that A upper-triangular locations of
divisor on lines 6 and 7.
) y , b , A  GAUSSIAN_ELIMINATION ( procedure 1.
begin 2.
 /* Outer loop */ do  - 1 n to  := 0 k for 3.
begin 4.
do  - 1 n to  + 1 k  := j for 5.
]; /* Division step */ k , k[ A ]/ j , k[ A ] := j , k[ A 6.
]; k , k[ A ]/ k[ b ] := k[ y 7.
] := 1; k , k[ A 8.
do  - 1 n to  + 1 k  :=i for 9.
begin 10.
do  - 1 n to  + 1 k  := j for 11.
]; /* Elimination step */ j , k[ A ] x k , i[ A ] - j , i[ A ] := j , i[ A 12.
]; k[ y ] x k , i[ A ] -i[ b ] :=i[ b 13.
] := 0; k , i[ A 14.
; /* Line 9 */ endfor 15.
; /* Line 3 */ endfor 16.
 GAUSSIAN_ELIMINATION end 17.
 on lines 7 and 13 of the b . The operations on vector Algorithm 8.4  in A In this section, we will concentrate only on the operations on matrix
program are straightforward to implement. Hence, in the rest of the section, we will ignore these steps. If the steps on lines 7, 8, 13, and
 is L . After the termination of the procedure, U  x L  as a product A  leads to the LU factorization of Algorithm 8.4 14 are not performed, then
 occupies the locations above the principal diagonal. U , and A stored in the lower-triangular part of
 - 1 so that n  + 1 to k ] from equations k[ x  - 1, the Gaussian elimination procedure systematically eliminates variable n  varying from 0 to k For
th iteration of the outer loop (starting on line 3), an k , in the Algorithm 8.4 the matrix of coefficients becomes upper-triangular. As shown in
 - 1 (loop starting on line 9). The multiples of the n  + 1 to k th equation is subtracted from each of the equations k appropriate multiple of the
] k  [ x  - 1 eliminating n  + 1 to k th coefficient becomes zero in equations k ) are chosen such that the A th row of matrix k th equation (or the k
Figure th iteration of the outer loop is shown in k from these equations. A typical computation of the Gaussian elimination procedure in the
 - 1. Thus, at this stage, only the k  - 1 or columns 1 to k th iteration of the outer loop does not involve any computation on rows 1 to k . The 8.5
) is computationally active. Figure 8.5  (the shaded portion in A ) submatrix of k  - n ) x ( k  - n lower-right (
Figure 8.5. A typical computation in Gaussian elimination.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
/2) subtractions and multiplications (line 2 n /3) - ( 3 n /2 divisions (line 6) and approximately ( 2 n Gaussian elimination involves approximately
12). In this section, we assume that each scalar arithmetic operation takes unit time. With this assumption, the sequential run time of the
); that is, n /3 (for large 3 n procedure is approximately 2
7  Equation 8.1
Parallel Implementation with 1-D Partitioning
, in which the coefficient matrix is rowwise 1-D partitioned among the Algorithm 8.4 We now consider a parallel implementation of
processes. A parallel implementation of this algorithm with columnwise 1-D partitioning is very similar, and its details can be worked out
based on the implementation using rowwise 1-D partitioning (Problems 8.8 and 8.9).
 is partitioned along the rows A  coefficient matrix n  x n We first consider the case in which one row is assigned to each process, and the
 illustrates Figure 8.6 . n  < j ] for 0 j , i[ A  initially stores elementsi . In this mapping, process P -1 n  to P 0  processes labeled from P n among
 = 8. The figure also illustrates the computation and communication that take place in n this mapping of the matrix onto the processes for
 = 3. k the iteration of the outer loop when
 = 3 for an 8 x 8 k Figure 8.6. Gaussian elimination steps during the iteration corresponding to
matrix partitioned rowwise among eight processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
th iteration. k ] (line 6) at the beginning of the k , k[ A  - 1] are divided by n , k[ A  + 2], ..., k , k[ A  + 1], k , k[ A  show that Figure 8.5  and Algorithm 8.4
) belong to the same process. Figure 8.6(a) All matrix elements participating in this operation (shown by the shaded portion of the matrix in
So this step does not require any communication. In the second computation step of the algorithm (the elimination step of line 12), the
 shows, this Figure 8.6(b) th row are used by all other rows of the active part of the matrix. As k modified (after division) elements of the
] := j , i[ A  - 1. Finally, the computation n  + 1 to k th row to the processes storing rows k requires a one-to-all broadcast of the active part of the
. Figure 8.6(c) ] takes place in the remaining active portion of the matrix, which is shown shaded in j , k[ A ] x k , i[ A ] - j , i[ A
. Similarly, the computation k  - 1 divisions at process P k  - n th iteration requires k  in the Figure 8.6(a) The computation step corresponding to
. Assuming a n  <i  < k  , such thati th iteration at all processes P k  - 1 multiplications and subtractions in the k  - n  involves Figure 8.6(c) step of
 is k  - 1). Note that when P k  - n th iteration is 3( k single arithmetic operation takes unit time, the total time spent in computation in the
 are performing the elimination step, -1 n  + 1, ..., P k  - 1 processes are idle, and while processes P p performing the divisions, the remaining
 in this parallel (c)  and 8.6(a)  are idle. Thus, the total time spent during the computation steps shown in Figures k , ..., P 0 processes P
 - 1)/2. n ( n , which is equal to 3 implementation of Gaussian elimination is
). Hence, the total communication time over all Table 4.1  ( n  -1)) log k  - n ( wt  + st  takes time ( Figure 8.6(b) The communication step of
. The overall parallel run time n  - 1)/2) log n ( n ( wt  + n  log n st , which is equal to n  log iterations is
of this algorithm is
8  Equation 8.1
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Equation  in wt ) due to the term associated with n 3 log n ( Q , the cost, or the process-time product, is n Since the number of processes is
). Hence, this parallel implementation is Equation 8.17 . This cost is asymptotically higher than the sequential run time of this algorithm ( 8.18
not cost-optimal.
n  We now present a parallel implementation of Gaussian elimination that is cost-optimal on Pipelined Communication and Computation
processes.
 execute sequentially. At any Algorithm 8.4  iterations of the outer loop of n In the parallel Gaussian elimination algorithm just presented, the
 + 1)th iteration starts only after all the computation and communication for the k given time, all processes work on the same iteration. The (
th iteration is complete. The performance of the algorithm can be improved substantially if the processes work asynchronously; that is, no k
 version of pipelined  or asynchronous process waits for the others to finish an iteration before starting the next one. We call this the
 illustrates the pipelined Algorithm 8.4 for a 5 x 5 matrix partitioned along the rows onto a logical linear Figure 8.7 Gaussian elimination.
array of five processes.
Figure 8.7. Pipelined Gaussian elimination on a 5 x 5 matrix partitioned with one row per
process.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). Figure 8.6(b)  ( -1 n , ..., P +1 k th row of the matrix to processes P k  broadcasts part of the k , process P Algorithm 8.4 th iteration of k During the
. Then process k th row from process P k  is the first process to receive the +1 k Assuming that the processes form a logical linear array, and P
 need not wait to perform the elimination +1 k , process P +2 k th row to P k . However, after forwarding the +2 k  must forward this data to P +1 k P
 can start its computation as soon as it has +2 k th row. Similarly, P k  have received the -1 n step (line 12) until all the processes up to P
 can perform the division +1 k th iteration, P k , and so on. Meanwhile, after completing the computation for the +3 k th row to P k forwarded the
. +2 k  + 1)th row by sending it to P k step (line 6), and start the broadcast of the (
 iterations n In pipelined Gaussian elimination, each process independently performs the following sequence of actions repeatedly until all
are complete. For the sake of simplicity, we assume that steps (1) and (2) take the same amount of time (this assumption does not affect
the analysis):
If a process has any data destined for other processes, it sends those data to the appropriate process. . 1
If the process can perform some computation using the data it has, it does so. . 2
Otherwise, the process waits to receive data to be used for one of the above actions. . 3
 shows the 16 steps in the pipelined parallel execution of Gaussian elimination for a 5 x 5 matrix partitioned along the rows Figure 8.7
. The modified row 0 is then 0  shows, the first step is to perform the division on row 0 at process P Figure 8.7(a) among five processes. As
). Figure 8.7(d)  is free to perform the elimination step using row 0 ( 1 ). Now P Figure 8.7(c)  ( 2 ), which forwards it to P Figure 8.7(b)  ( 1 sent to P
, having finished its computation for 1  performs the elimination step using row 0. In the same step, P 2 ), P Figure 8.7(e) In the next step (
iteration 0, starts the division step of iteration 1. At any given time, different stages of the same iteration can be active on different
 are engaged in 4  and P 3  performs the elimination step of iteration 1 while processes P 2 , process P Figure 8.7(h) processes. For instance, in
communication for the same iteration. Furthermore, more than one iteration may be active simultaneously on different processes. For
 is performing the elimination step of 3  is performing the division step of iteration 2 while process P 2 , process P Figure 8.7(i) instance, in
iteration 1.
We now show that, unlike the synchronous algorithm in which all processes work on the same iteration at a time, the pipelined or the
 shows, the initiation of consecutive iterations of the outer loop Figure 8.7 asynchronous version of Gaussian elimination is cost-optimal. As
 such iterations are initiated. The last iteration modifies only the n  is separated by a constant number of steps. A total of Algorithm 8.4 of
bottom-right corner element of the coefficient matrix; hence, it completes in a constant time after its initiation. Thus, the total number of
) elements are communicated between n ( O ) (Problem 8.7). In any step, either n ( Q steps in the entire pipelined procedure is
) n ( O ) elements of a row, or an elimination step is performed on n ( O directly-connected processes, or a division step is performed on
) complexity each, n ) steps of O( n ( Q ) time. Hence, the entire procedure consists of n ( O elements of a row. Each of these operations take
), which is of the same order as the sequential complexity of 3 n ( O  processes are used, the cost is n ). Since 2 n ( O and its parallel run time is
Gaussian elimination. Hence, the pipelined version of parallel Gaussian elimination with 1-D partitioning of the coefficient matrix is
cost-optimal.
e  The preceding pipelined implementation of parallel Gaussian elimination can b Block 1-D Partitioning with Fewer than n Processes
) such that each process is n  < p  processes ( p  matrix partitIoned among n  x n . Consider an p  > n easily adapted for the case in which
 illustrates the communication steps in a typical iteration of Gaussian elimination with Figure 8.8  contiguous rows of the matrix. p/ n assigned
th row be sent to the processes k th iteration of the algorithm requires that the active part of the k such a mapping. As the figure shows, the
 - 1. n  + 2, ..., k  + 1, k storing rows
 = 3 for k Figure 8.8. The communication in the Gaussian elimination iteration corresponding to
an 8 x 8 matrix distributed among four processes using block 1-D partitioning.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
p/ n  - 1) k  - n  shows that, with block 1-D partitioning, a process with all rows belonging to the active part of the matrix performs ( Figure 8.9(a)
) - 1 iterations, no process has all p/ n th iteration. Note that in the last ( k multiplications and subtractions during the elimination step of the
active rows, but we ignore this anomaly. If the pipelined version of the algorithm is used, then the number of arithmetic operations on a
 - 1) by a k  - n ) is much higher than the number of words communicated ( p/ n  - 1) k  - n th iteration (2( k maximally-loaded process in the
, computation dominates communication in each p  with respect to n process in the same iteration. Thus, for sufficiently large values of
iteration. Assuming that each scalar multiplication and subtraction pair takes unit time, the total parallel run time of this algorithm (ignoring
. p/ 3 n , which is approximately equal to communication overhead) is
Figure 8.9. Computation load on different processes in block and cyclic 1-D partitioning of an 8
 = 3. k x 8 matrix on four processes during the Gaussian elimination iteration corresponding to
, even if the communication costs are ignored. Thus, the cost of the parallel algorithm is 3 n The process-time product of this algorithm is
) by a factor of 3/2. This inefficiency of Gaussian elimination with block 1-D partitioning is Equation 8.17 higher than the sequential run time (
 shows for an 8 x 8 matrix and four processes, during the Figure 8.9(a) due to process idling resulting from an uneven load distribution. As
), one process is completely idle, one is partially loaded, and only two Algorithm 8.4  = 3 (in the outer loop of k iteration corresponding to
processes are fully active. By the time half of the iterations of the outer loop are over, only half the processes are active. The remaining
idle processes make the parallel algorithm costlier than the sequential algorithm.
. With Figure 8.9(b) This problem can be alleviated if the matrix is partitioned among the processes using cyclic 1-D mapping as shown in
the cyclic 1-D partitioning, the difference between the computational loads of a maximally loaded process and the least loaded process in
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 iterations, the cumulative overhead due to process n ) arithmetic operations). Since there are n ( O any iteration is of at most one row (that is,
) with a block mapping (Problem 8.12). 3 n ( Q ) with a cyclic mapping, compared to p 2 n ( O idling is only
Parallel Implementation with 2-D Partitioning
 mesh of processes such n  x n  is mapped onto an A  matrix n  x n  in which the Algorithm 8.4 We now describe a parallel implementation of
 = 3 are k ]. The communication and computation steps in the iteration of the outer loop corresponding to j , i[ A  initially stores j, i that process P
] is required by k , k[ A th iteration of the outer loop, k  show that in the 8.10  and 8.5  and Figures Algorithm 8.4  = 8. n  for Figure 8.10 illustrated in
 - 1], respectively. After the division on line 6, the modified n , k[ A  + 2], ..., k , k[ A  + 1], k , k[ A  to divide -1 n, k , ..., P +2 k, k , P +1 k, k processes P
th row are used to perform the elimination step by all the other rows in the active part of the matrix. The modified (after k elements of the
th k th row are used by all other rows of the active part of the matrix. Similarly, the elements of the k the division on line 6) elements of the
 shows, the communication in Figure 8.10 column are used by all other columns of the active part of the matrix for the elimination step. As
, k[ A , and a one-to-all broadcast of n  <i k ) for Figure 8.10(a)  th row (i ] along the k , i[ A th iteration requires a one-to-all broadcast of k the
. Just like the 1-D partitioning case, a non-cost-optimal parallel formulation results if n  < j  < k ) for Figure 8.10(c)  th column ( j ] along the j
these broadcasts are performed synchronously on all processes (Problem 8.11).
 = 3 for an 8 x k Figure 8.10. Various steps in the Gaussian elimination iteration corresponding to
8 matrix on 64 processes arranged in a logical two-dimensional mesh.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
t  Based on our experience with Gaussian elimination using 1-D partitioning of the coefficien Pipelined Communication and Computation
matrix, we develop a pipelined version of the algorithm using 2-D partitioning.
 to +1 k, k  to P k, k ] is sent to the right from P k , k[ A ), Algorithm 8.4 th iteration of the outer loop (lines 3–16 of k  shows, in the Figure 8.10 As
. It k, k ] from P k , k[ A ] as soon as it receives k , k[ A  + 1]/ k , k[ A  performs the division +1 k, k . Process P -1 n, k , and so on, until it reaches P +2 k, k P
 before performing its local computation. Similarly, any subsequent process -1 n, k ] to reach all the way up to P k , k[ A does not have to wait for
] is ready to be communicated j , k[ A ]. After performing the division, k , k[ A th row can perform its division as soon as it receives k  of the j, k P
 th column j ] moves down, each process it passes is free to use it for computation. Processes in the j , k[ A  th column. As j downward in the
] as j , k[ A ] x k , i[ A ] - j , i[ A ] := j , i[ A  performs the elimination step j, i ] reaches the last process of the column. Thus, P j , k[ A need not wait until
] are available. Since some processes perform the computation for a given iteration earlier than other processes, j , k[ A ] and k , i[ A soon as
they start working on subsequent iterations sooner.
, the Figure 8.11(a) . In Figure 8.11 The communication and computation can be pipelined in several ways. We present one such scheme in
[0, 1] := A  computes 0,1 [0, 0], P A . Upon receiving 0,1 [0, 0] to P A  sends 0,0 , when P 0,0  = 0 starts at process P k iteration of the outer loop for
). At the Figure 8.11(c)  ( 1,1 [0, 1] down to P A  and also sends the updated 0,2 [0, 0] to P A  forwards 0,1 ). Now P Figure 8.11(b) [0, 0] ( A [0, 1]/ A
[1, 0] x A [1, 1] - A [1, 1] := A  performs the elimination step 1,1 [1, 0], P A [0, 1] and A . Having received 1,1 [1, 0] to P A  sends 1,0 same time, P
). After this computation step, Figure 8.11(d) [0, 0] ( A [0, 2]/ A [0, 2] := A  performs the division step 0,2 [0, 0], P A [0, 1], and having received A
). Figure 8.11(e) ) is ready to initiate communication ( 2,0 , and P 1,1 , P 0,2 another set of processes (that is, processes P
Figure 8.11. Pipelined Gaussian elimination for a 5 x 5 matrix with 25 processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
All processes performing communication or computation during a particular iteration lie along a diagonal in the bottom-left to top-right
 performing computation 2,1 , and P 1,2 , P 0,3  and P Figure 8.11(e)  performing communication in 2,0 , and P 1,1 , P 0,2 direction (for example, P
). As the parallel algorithm progresses, this diagonal moves toward the bottom-right corner of the logical 2-D mesh. Thus, Figure 8.11(f) in
the computation and communication for each iteration moves through the mesh from top-left to bottom-right as a "front." After the front
Figure corresponding to a certain iteration passes through a process, the process is free to perform subsequent iterations. For instance, in
 = 1, k . This initiates a front for 1,2 [1, 1] to P A  = 1 by sending k , it initiates the iteration for 1,1  = 0 has passed P k , after the front for 8.11(g)
). Thus, multiple fronts that correspond Figure 8.11(m)  ( 2,2  = 2 starts at P k  = 0. Similarly, a third front for k which closely follows the front for
to different iterations are active simultaneously.
Every step of an iteration, such as division, elimination, or transmitting a value to a neighboring process, is a constant-time operation.
Figure Therefore, a front moves a single step closer to the bottom-right corner of the matrix in constant time (equivalent to two steps of
 iterations of the n  fronts for the n . The algorithm initiates 0,0  after its initiation at P 1 - 1,n - n ) to reach P n ( Q  = 0 takes time k ). The front for 8.11
) n ( Q outer loop. Each front lags behind the previous one by a single step. Thus, the last front passes the bottom-right corner of the matrix
). The procedure is n ( Q  and the last one finishing is 0,0 steps after the first one. The total time elapsed between the first front starting at P
 process are 2 n ). Since n ( Q complete after the last front passes the bottom-right corner of the matrix; hence, the total parallel run time is
), which is the same as the sequential run time of the algorithm. 3 n ( Q used, the cost of the pipelined version of Gaussian elimination is
Hence, the pipelined version of Gaussian elimination with 2-D partitioning is cost-optimal.
 and the matrix is mapped 2 n  < p  processes are used so that p  Consider the case in which Processes 2 n 2-D Partitioning with Fewer than
 illustrates that a typical parallel Gaussian iteration involves a Figure 8.12  mesh by using block 2-D partitioning. onto a
 = n  illustrates the load distribution in block 2-D mapping for Figure 8.13(a)  values. rowwise and a columnwise communication of
 = 16. p 8 and
k Figure 8.12. The communication steps in the Gaussian elimination iteration corresponding to
= 3 for an 8 x 8 matrix on 16 processes of a two-dimensional mesh.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 8.13. Computational load on different processes in block and cyclic 2-D mappings of an
 = 3. k 8 x 8 matrix onto 16 processes during the Gaussian elimination iteration corresponding to
 multiplications and p/ 2 n  show that a process containing a completely active part of the matrix performs 8.13(a)  and 8.12 Figures
 words along its row and its column (ignoring the fact that in the last subtractions, and communicates
iterations, the active part of the matrix becomes smaller than the size of a block, and no process contains a completely active part of the
) is an order of magnitude p/ 2 n matrix). If the pipelined version of the algorithm is used, the number of arithmetic operations per process (2
 with 2 n ) in each iteration. Thus, for sufficiently large values of higher than the number of words communicated per process (
, the communication in each iteration is dominated by computation. Ignoring the communication cost and assuming that each p respect to
. The p/ 3 n , which is equal to 2 n ) x p/ 2 n scalar arithmetic operation takes unit time, the total parallel run time of this algorithm is (2
). As a result, there is an upper bound of 1/3 Equation 8.17 , which is three times the cost of the serial algorithm ( 3 n process-time product is 2
on the efficiency of the parallel algorithm.
As in the case of a block 1-D mapping, the inefficiency of Gaussian elimination with a block 2-D partitioning of the matrix is due to process
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 shows the active part of an 8 x 8 matrix of coefficients in the iteration of the Figure 8.13(a) idling resulting from an uneven load distribution.
 = 3 when the matrix is block 2-D partitioned among 16 processes. As shown in the figure, seven out of 16 processes are k outer loop for
fully idle, five are partially loaded, and only four are fully active. By the time half of the iterations of the outer loop have been completed,
only one-fourth of the processes are active. The remaining idle processes make the parallel algorithm much costlier than the sequential
algorithm.
. With the cyclic 2-D Figure 8.13(b) This problem can be alleviated if the matrix is partitioned in a 2-D cyclic fashion as shown in
partitioning, the maximum difference in computational load between any two processes in any iteration is that of one row and one column
 elements are active in the p/ 2  - 1) n  matrix elements are active in the bottom-right process, and ( p/ 2 n , Figure 8.13(b) update. For example, in
( Q ) in any iteration, which contributes ( Q top-left process. The difference in workload between any two processes is at most
Q  iterations, the cumulative overhead due to process idling is only n ) to the overhead function. Since there are
) with block mapping (Problem 8.12). In practical parallel implementations of Gaussian elimination 3 n ( Q with cyclic mapping in contrast to
and LU factorization, a block-cyclic mapping is used to reduce the overhead due to message startup time associated with a pure cyclic
mapping and to obtain better serial CPU utilization by performing block-matrix operations (Problem 8.15).
p ) on p/ 3 n ( Q  matrix takes time n  x n From the discussion in this section, we conclude that pipelined parallel Gaussian elimination for an
)) for an n ( O )) than 1-D partitioning ( 2 n ( O processes with both 1-D and 2-D partitioning schemes. 2-D partitioning can use more processes (
 coefficient matrix. Hence, an implementation with 2-D partitioning is more scalable. n  x n
8.3.2 Gaussian Elimination with Partial Pivoting
] of the matrix of coefficients is close or equal to zero. k , k[ A  fails if any diagonal entry Algorithm 8.4 The Gaussian elimination algorithm in
 is used. At the beginning partial pivoting To avoid this problem and to ensure the numerical stability of the algorithm, a technique called
] is the largest in magnitudei , k[ A  column) such that pivot  (called thei th iteration, this method selects a column k of the outer loop in the
 th columns before starting the iteration. These columns cani th and the k . It then exchanges the n  < j k ] such that j , k[ A among all
either be exchanged explicitly by physically moving them into each other's locations, or they can be exchanged implicitly by simply
. If partial pivoting is performed with an implicit A  x 1 permutation vector to keep track of the new indices of the columns of n maintaining an
 are not exactly triangular matrices, but columnwise permutations of triangular U  and L exchange of column indices, then the factors
matrices.
 (after exchanging Algorithm 8.4 ] used as the divisor on line 6 of k , k[ A Assuming that columns are exchanged explicitly, the value of
 results in a unit Algorithm 8.4 th iteration. Partial pivoting in k ] that it divides in the j , k[ A ) is greater than or equal to any i  and k columns
upper-triangular matrix in which all elements above the principal diagonal have an absolute value of less than one.
1-D Partitioning
. Before performing the divide Section 8.3.1 Performing partial pivoting is straightforward with rowwise partitioning as discussed in
th row makes a comparison pass over the active portion of this row, and selects the k th iteration, the process storing the k operation in the
element with the largest absolute value as the divisor. This element determines the pivot column, and all processes must know the index of
th k this column. This information can be passed on to the rest of the processes along with the modified (after the division) elements of the
th iteration, as in case of Gaussian elimination without k  - 1) in the k  - n ( Q row. The combined pivot-search and division step takes time
 if the coefficient matrix is partitioned along the Algorithm 8.4 pivoting. Thus, partial pivoting has no significant effect on the performance of
rows.
Now consider a columnwise 1-D partitioning of the coefficient matrix. In the absence of pivoting, parallel implementations of Gaussian
elimination with rowwise and columnwise 1-D partitioning are almost identical (Problem 8.9). However, the two are significantly different if
partial pivoting is performed.
n  x n The first difference is that, unlike rowwise partitioning, the pivot search is distributed in columnwise partitioning. If the matrix size is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
th k , then the pivot search in columnwise partitioning involves two steps. During pivot search for the p and the number of processes is
th row that it stores. The next step is to find the k  (or fewer) elements of the p/ n iteration, first each process determines the maximum of the
) + p/ n ( Q  (or fewer) values, and to distribute the maximum among all processes. Each pivot search takes time p maximum of the resulting
) it takes to perform a pivot search with rowwise n ( Q , this is less than the time p  with respect to n ). For sufficiently large values of p (log Q
partitioning. This seems to suggest that a columnwise partitioning is better for partial pivoting that a rowwise partitioning. However, the
following factors favor rowwise partitioning.
 shows how communication and computation "fronts" move from top to bottom in the pipelined version of Gaussian elimination Figure 8.7
with rowwise 1-D partitioning. Similarly, the communication and computation fronts move from left to right in case of columnwise 1-D
 + 1)th iteration (that is, it is not fully updated) until the k  + 1)th row is not ready for pivot search for the ( k partitioning. This means that the (
th k  + 1)th iteration cannot start until the entire k th iteration reaches the rightmost process. As a result, the ( k front corresponding to the
iteration is complete. This effectively eliminates pipelining, and we are therefore forced to use the synchronous version with poor efficiency.
While performing partial pivoting, columns of the coefficient matrix may or may not be explicitly exchanged. In either case, the
 is adversely affected with columnwise 1-D partitioning. Recall that cyclic or block-cyclic mappings result in a Algorithm 8.4 performance of
better load balance in Gaussian elimination than a block mapping. A cyclic mapping ensures that the active portion of the matrix is almost
uniformly distributed among the processes at every stage of Gaussian elimination. If pivot columns are not exchanged explicitly, then this
condition may cease to hold. After a pivot column is used, it no longer stays in the active portion of the matrix. As a result of pivoting
without explicit exchange, columns are arbitrarily removed from the different processes' active portions of the matrix. This randomness
may disturb the uniform distribution of the active portion. On the other hand, if columns belonging to different processes are exchanged
explicitly, then this exchange requires communication between the processes. A rowwise 1-D partitioning neither requires communication
for exchanging columns, nor does it lose the load-balance if columns are not exchanged explicitly.
2-D Partitioning
In the case of 2-D partitioning of the coefficient matrix, partial pivoting seriously restricts pipelining, although it does not completely
eliminate it. Recall that in the pipelined version of Gaussian elimination with 2-D partitioning, fronts corresponding to various iterations
th k  + 1)th iteration can commence as soon as the front corresponding to the k move from top-left to bottom-right. The pivot search for the (
iteration has moved past the diagonal of the active matrix joining its top-right and bottom-left corners.
Thus, partial pivoting may lead to considerable performance degradation in parallel Gaussian elimination with 2-D partitioning. If numerical
considerations allow, it may be possible to reduce the performance loss due to partial pivoting. We can restrict the search for the pivot in
th iteration if k  th column is selected as the pivot in thei  columns). In this case, the k  - n  columns (instead of all q th iteration to a band of k the
 th row. This restricted partial pivoting not only reduces thei  elements of the active part of the q ] is the largest element in a band ofi , k[ A
, an iteration can start as q communication cost, but also permits limited pipelining. By restricting the number of columns for pivot search to
 + 1 columns. q soon as the previous iteration has updated the first
Another way to get around the loss of pipelining due to partial pivoting in Gaussian elimination with 2-D partitioning is to use fast
p  coefficient matrix on n  x n . With 2-D partitioning of the Section 4.7.1 algorithms for one-to-all broadcast, such as those described in
) in communication in each iteration of the pipelined version of Gaussian elimination. ( Q processes, a process spends time
Section , a non-pipelined version that performs explicit one-to-all broadcasts using the algorithm of st Disregarding the message startup time
) communicating in each iteration. This communication time is higher than that of the pipelined p ) log (( Q  spends time 4.1
) in each iteration (disregarding the startup ( Q  take time Section 4.7.1 version. The one-to-all broadcast algorithms described in
time). This time is asymptotically equal to the per-iteration communication time of the pipelined algorithm. Hence, using a smart algorithm
to perform one-to-all broadcast, even non-pipelined parallel Gaussian elimination can attain performance comparable to that of the
 split a message into smaller parts and route Section 4.7.1 pipelined algorithm. However, the one-to-all broadcast algorithms described in
 should be large n them separately. For these algorithms to be effective, the sizes of the messages should be large enough; that is,
. p compared to
Although pipelining and pivoting do not go together in Gaussian elimination with 2-D partitioning, the discussion of 2-D partitioning in this
 in Problem 8.16), which does Algorithm 8.6 section is still useful. With some modification, it applies to the Cholesky factorization algorithm (
 if positive definite  is A  matrix n  x n not require pivoting. Cholesky factorization applies only to symmetric, positive definite matrices. A real
x
. The communication pattern in Cholesky factorization is quite similar to that of Gaussian x  x 1 nonzero, real vector n  > 0 for any Ax T
elimination (Problem 8.16), except that, due to symmetric lower and upper-triangular halves in the matrix, Cholesky factorization uses only
one triangular half of the matrix.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
8.3.3 Solving a Triangular System: Back-Substitution
 has been reduced to an A We now briefly discuss the second stage of solving a system of linear equations. After the full matrix
. A sequential x  with ones along the principal diagonal, we perform back-substitution to determine the vector U upper-triangular matrix
. Algorithm 8.5  is shown in y  = Ux back-substitution algorithm for solving an upper-triangular system of equations
 computes the values of a variable and Algorithm 8.5 Starting with the last equation, each iteration of the main loop (lines 3–8) of
/2 multiplications and 2 n substitutes the variable's value back into the remaining equations. The program performs approximately
subtractions. Note that the number of arithmetic operations in back-substitution is less than that in Gaussian elimination by a factor of
). Hence, if back-substitution is used in conjunction with Gaussian elimination, it is best to use the matrix partitioning scheme that is the n ( Q
most efficient for parallel Gaussian elimination.
 is an upper-triangular matrix with all U Algorithm 8.5 A serial algorithm for back-substitution.
entries of the principal diagonal equal to one, and all subdiagonal entries equal to zero.
) y , x , U  BACK_SUBSTITUTION ( procedure 1.
begin 2.
 /* Main loop */ do  0 downto  - 1 n  := k for 3.
begin 4.
]; k[ y ] := k[ x 5.
do  0 downto  - 1 k  :=i for 6.
]; k , i[ U ] x k[ x ] -i[ y ] :=i[ y 7.
; endfor 8.
 BACK_SUBSTITUTION end 9.
 be distributed uniformly among all the y  processes. Let the vector p  onto U  matrix n  x n Consider a rowwise block 1-D mapping of the
processes. The value of the variable solved in a typical iteration of the main loop (line 3) must be sent to all the processes with equations
involving that variable. This communication can be pipelined (Problem 8.22). If so, the time to perform the computations of an iteration
dominates the time that a process spends in communication in an iteration. In every iteration of a pipelined implementation, a process
receives (or generates) the value of a variable and sends that value to another process. Using the value of the variable solved in the
 multiplications and subtractions (lines 6 and 7). Hence, each step of a pipelined p/ n current iteration, a process also performs up to
) n ( Q ) for computation. The algorithm terminates in p/ n ( Q implementation requires a constant amount of time for communication and time
). p/ 2 n ( Q steps (Problem 8.22), and the parallel run time of the entire algorithm is
 logical mesh of processes, and the elements of the vector are If the matrix is partitioned by using 2-D partitioning on a
 processes containing the vector perform any computation. distributed along one of the columns of the process mesh, then only the
 for the y  to the process containing the corresponding elements of U Using pipelining to communicate the appropriate elements of
 (Problem 8.22). Thus, the cost of parallel Q substitution step (line 7), the algorithm can be executed in time
). 2 n ( Q . The algorithm is not cost-optimal because its sequential cost is only Q back-substitution with 2-D mapping is
However, the entire process of solving the linear system, including upper-triangularization using Gaussian elimination, is still cost-optimal
). 3 n ( Q  because the sequential complexity of the entire process is for
8.3.4 Numerical Considerations in Solving Systems of Linear Equations
 as the product of a A  can be solved by using a factorization algorithm to express b  = Ax A system of linear equations of the form
, and is solved in two b  = LU x . The system of equations is then rewritten as U , and a unit upper-triangular matrix L lower-triangular matrix
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. x  is solved for y  = Ux . Second, the upper-triangular system y  is solved for b  = Ly steps. First, the lower-triangular system
. However, it also solves the lower-triangular U  and L  into A  effectively factorizes Algorithm 8.4 The Gaussian elimination algorithm given in
 Gaussian elimination row-oriented  gives what is called a Algorithm 8.4  on the fly by means of steps on lines 7 and 13. b  = Ly system
, is Section 8.3.2 algorithm. In this algorithm, multiples of rows are subtracted from other rows. If partial pivoting, as described in
 has all its elements less than or equal to one in magnitude. U incorporated into this algorithm, then the resulting upper-triangular matrix
, b  = Ax , whether implicit or explicit, may have elements with larger numerical values. While solving the system L The lower-triangular matrix
 due to the finite y  contains large elements, then rounding errors can occur while solving for L  is solved first. If b  = Ly the triangular system
. y  = Ux  are propagated through the solution of y precision of floating-point numbers in the computer. These errors in
 by reversing the roles of Algorithm 8.4  form that can be obtained from column-oriented An alternate form of Gaussian elimination is the
rows and columns. In the column-oriented algorithm, multiples of columns are subtracted from other columns, pivot search is also
performed along the columns, and numerical stability is guaranteed by row interchanges, if needed. All elements of the lower-triangular
 generated by the column-oriented algorithm have a magnitude less than or equal to one. This minimizes numerical error while L matrix
 gives a Algorithm 3.3 , and results in a significantly smaller error in the overall solution than the row-oriented algorithm. b  = Ly solving
procedure for column-oriented LU factorization.
From a practical point of view, the column-oriented Gaussian elimination algorithm is more useful than the row-oriented algorithm. We
have chosen to present the row-oriented algorithm in detail in this chapter because it is more intuitive. It is easy to see that the system of
linear equations resulting from the subtraction of a multiple of an equation from other equations is equivalent to the original system. The
 presented in this section applies to the column-oriented algorithm with the Algorithm 8.4 entire discussion on the row-oriented algorithm of
roles of rows and columns reversed. For example, columnwise 1-D partitioning is more suitable than rowwise 1-D partitioning for the
column-oriented algorithm with partial pivoting.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
8.4 Bibliographic Remarks
]. Hence, all the Ede89 Matrix transposition with 1-D partitioning is essentially an all-to-all personalized communication problem [
 for all-to-all personalized communication apply directly to matrix transposition. The recursive transposition Chapter 4 references in
]. Its adaptations for hypercubes have been described by Ekl72 algorithm, popularly known as RTA, was first reported by Eklundh [
] for one-port MdV87 ], and McBryan and Van de Velde [ Joh87 ], Johnsson [ FF86 ], Fox and Furmanski [ BT97 Bertsekas and Tsitsiklis [
] also discusses parallel RTA for hypercubes that permit simultaneous communication Joh87 communication on each process. Johnsson [
], Johnsson and Ho HR91 on all channels. Further improvements on the hypercube RTA have been suggested by Ho and Raghunath [
]. SW87 ], and Stout and Wagar [ Joh90 ], Johnsson [ JH88[
A number of sources of parallel dense linear algebra algorithms, including those for matrix-vector multiplication and matrix multiplication,
]. Since dense matrix multiplication is highly computationally intensive, OS85 , Mod88 , Joh87 , GL96a , GPS90 , CAHH91 are available [
there has been a great deal of interest in developing parallel formulations of this algorithm and in testing its performance on various
, Joh87 , HJE91 , Hip89 , GL96a , GK91 , FOH87 , 88 + FJL , dV89 , DNS81 , CS88 , Cha79 , Can69 , CAHH91 , Ber89 , Akl89 parallel architectures [
], Dekel, Nassimi, and Can69 ]. Some of the early parallel formulations of matrix multiplication were developed by Cannon [ Tic88 , PV80
], and by Ber89 ]. Variants and improvements of these algorithms have been presented by Berntsen [ FOH87 . [ et al ], and Fox DNS81 Sahni [
] presents an algorithm that has strictly smaller communication Ber89 ]. In particular, Berntsen [ HJE91 Ho, Johnsson, and Edelman [
] present another HJE91 overhead than Cannon's algorithm, but has a smaller degree of concurrency. Ho, Johnsson, and Edelman [
variant of Cannon's algorithm for a hypercube that permits communication on all channels simultaneously. This algorithm, while reducing
] present a detailed scalability analysis of several GK91 communication, also reduces the degree of concurrency. Gupta and Kumar [
-process p  matrices on a n  x n matrix multiplication algorithms. They present an analysis to determine the best algorithm to multiply two
 and the hardware-related constants. They also show that the improvements suggested by Berntsen p , n hypercube for different ranges of
and Ho et al. do not improve the overall scalability of matrix multiplication on a hypercube.
Parallel algorithms for LU factorization and solving dense systems of linear equations have been discussed by several researchers
, Ort88 , OR88 Mol86 , Mod88 , Lei92 , LD90 , Joh87 , GR88 , GPS90 , GH86 , Gei85 , 88 + FJL , DHvdV93 , Dav86 , Cha87 , CG87 , BT97 , Ber84[
] specifically concentrate on parallel dense Hea85 ], and Heath [ GH86 , GH85 ]. Geist and Heath [ Vav89 , Saa86 , Rob90 , PR85 , OS86
, LC89 , LC88 , HR88 , EHHR88 Cholesky factorization. Parallel algorithms for solving triangular systems have also been studied in detail [
] present a comprehensive survey of parallel matrix computations DHvdV93 ]. Demmel, Heath, and van der Vorst [ Rom87 , RO88
considering numerical implications in detail.
A portable software implementation of all matrix and vector operations discussed in this chapter, and many more, is available as PBLAS
] uses PBLAS to implement a variety of linear algebra 97 + B ]. The ScaLAPACK library [ 95 + C (parallel basic linear algebra subroutines) [
routines of practical importance, including procedures for various methods of matrix factorizations and solving systems of linear
equations.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
. Which method would Section 4.5.3  Consider the two algorithms for all-to-all personalized communication in 8.1
) bisection width for transposing a 1024 x 1024 matrix with the p ( Q you use on a 64-node parallel computer with
s? Why? µ  = 1 wt s and µ  = 100 st 1-D partitioning if
 Describe a parallel formulation of matrix-vector multiplication in which the matrix is 1-D block-partitioned along 8.2
the columns and the vector is equally partitioned among all the processes. Show that the parallel run time is the
same as in case of rowwise 1-D block partitioning.
 The basic communication operation used in the case of columnwise 1-D partitioning is all-to-all reduction, as Hint:
opposed to all-to-all broadcast in the case of rowwise 1-D partitioning. Problem 4.8 describes all-to-all reduction.
, then  describes and analyzes matrix-vector multiplication with 2-D partitioning. If Section 8.1.2 8.3
. Is the suggest ways of improving the parallel run time to
? Section 8.1.2 improved method more scalable than the one used in
 processes is p  x 1 vector using n  2-D partitioned matrix with an n  x n  The overhead function for multiplying an 8.4
 yields a Equation 5.14 ). Substituting this expression in Equation 8.8  (
. Using this equation, determine the precise isoefficiency function for the parallel algorithm n quadratic equation in
. Does this comparison alter the conclusion that the term associated 8.10  and 8.9 and compare it with Equations
 is responsible for the overall isoefficiency function of this parallel algorithm? wt with
] for matrix multiplication is an algorithm based on the divide-and-conquer CLR90 , AHU74  Strassen's method [ 8.5
). 2.81 n ( Q  matrices using Strassen's algorithm is n  x n technique. The sequential complexity of multiplying two
p  matrices using n  x n ) for multiplying two Section 8.2.1 Consider the simple matrix multiplication algorithm (
 submatrices are multiplied using Strassen's algorithm at processes. Assume that the
each process. Derive an expression for the parallel run time of this algorithm. Is the parallel algorithm
cost-optimal?
 describes a parallel formulation of the Section 8.2.3 ) ] DNS81 processes [ 3 n 8.6 (DNS algorithm with fewer than
q 2 n  = p  processes. Another variation of this algorithm works with 3 n DNS algorithm that uses fewer than
 logical q  x q  x q . Here the process arrangement is regarded as a n q processes, where 1
) mesh of processes. q / n ) x ( q / n three-dimensional array of "superprocesses," in which each superprocess is an (
, except that the role of each Section 8.2.3 This variant can be viewed as identical to the block variant described in
) logical mesh of processes. This means that each block multiplication q / n ) x ( q / n process is now assumed by an (
 processes rather than by a single process. Any of the 2 ) q / n ) submatrices is performed in parallel by ( q / n ) x ( q / n of (
 can be used to perform this multiplication. 8.2.2  or 8.2.1 algorithms described in Sections
. wt , and st , p , n Derive an expression for the parallel run time for this variant of the DNS algorithm in terms of
. Discuss the relative merits and drawbacks of the two variations of Equation 8.16 Compare the expression with
 processes. 3 n the DNS algorithm for fewer than
 shows that the pipelined version of Gaussian elimination requires 16 steps for a 5 x 5 matrix Figure 8.7 8.7
n partitioned rowwise on five processes. Show that, in general, the algorithm illustrated in this figure completes in 4(
 matrix partitioned rowwise with one row assigned to each process. n  x n - 1) steps for an
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 without Algorithm 8.4  Describe in detail a parallel implementation of the Gaussian elimination algorithm of 8.8
 processes. Consider both pipelined and p  coefficient matrix is partitioned columnwise among n  x n pivoting if the
. n  < p  and n  = p non-pipelined implementations. Also consider the cases
 shows horizontal and vertical Section 8.3.1  The parallel implementation of Gaussian elimination described in Hint:
). A rowwise partitioning requires Figure 8.12 communication on a logical two-dimensional mesh of processes (
only the vertical part of this communication. Similarly, columnwise partitioning performs only the horizontal part of
this communication.
 Derive expressions for the parallel run times of all the implementations in Problem 8.8. Is the run time of any 8.9
of these parallel implementations significantly different from the corresponding implementation with rowwise 1-D
partitioning?
 Rework Problem 8.9 with partial pivoting. In which implementations are the parallel run times significantly 8.10
different for rowwise and columnwise partitioning?
 logical mesh of processes is not n  x n  matrix 2-D partitioned on an n  x n  Show that Gaussian elimination on an 8.11
 one-to-all broadcasts are performed synchronously. n cost-optimal if the 2
) for a 3 n ( Q  Show that the cumulative idle time over all the processes in the Gaussian elimination algorithm is 8.12
 matrix is partitioned along one or both dimensions. Show that this idle time is n  x n block mapping, whether the
 for cyclic 2-D mapping. Q ) for cyclic 1-D mapping and p 2 n ( Q reduced to
 Prove that the isoefficiency function of the asynchronous version of the Gaussian elimination with 2-D 8.13
) if pivoting is not performed. 3/2 p ( Q mapping is
 Derive precise expressions for the parallel run time of Gaussian elimination with and without partial pivoting if 8.14
 processes of a logical square two-dimensional mesh in the p  matrix of coefficients is partitioned among n  x n the
following formats:
Rowwise block 1-D partitioning. . a
Rowwise cyclic 1-D partitioning. . b
Columnwise block 1-D partitioning. . c
Columnwise cyclic 1-D partitioning. . d
. Section 8.2  in terms of block matrix operations as discussed at the beginning of Algorithm 8.4  Rewrite 8.15
 array of submatrices, where each q  x q  matrix partitioned into a n  x n Consider Gaussian elimination of an
 mesh of . This array of blocks is mapped onto a logical q / n  x q / n submatrix is of size of
processes in a cyclic manner, resulting in a 2-D block cyclic mapping of the original matrix onto the mesh.
. Derive , which in turn is divisible by q  is divisible by n  and that Assume that
expressions for the parallel run time for both synchronous and pipelined versions of Gaussian elimination.
, ] is replaced by submatrix operation k , k[ A ]/ j , k[ A ] := j , k[ A  The division step Hint:
th diagonal submatrix. k  is the lower triangular part of the k, k A where
 describes a row-oriented version of the Cholesky factorization Algorithm 8.6 8.16 (Cholesky factorization)
. Cholesky factorization does U T U  = A algorithm for factorizing a symmetric positive definite matrix into the form
not require pivoting. Describe a pipelined parallel formulation of this algorithm that uses 2-D partitioning of the
. Figure 8.11 matrix on a square mesh of processes. Draw a picture similar to
Algorithm 8.6 A row-oriented Cholesky factorization algorithm.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
) A  CHOLESKY ( procedure 1.
begin 2.
do  - 1 n to  := 0 k for 3.
begin 4.
5.
do  - 1 n to  + 1 k  := j for 6.
]; k , k[ A ]/ j , k[ A ] := j , k[ A 7.
do  - 1 n to  + 1 k  :=i for 8.
do  - 1 n to i  := j for 9.
]; j , k[ A ] xi , k[ A ] - j , i[ A ] := j , i[ A 10.
; /*Line3*/ endfor 11.
 CHOLESKY end 12.
) is defined as the speedup obtained when the problem size Section 5.7  Scaled speedup ( 8.17 (Scaled speedup)
 is chosen as a base problem size for a single W is increased linearly with the number of processes; that is, if
process, then
9  Equation 8.1
, plot the standard and scaled speedup Section 8.2.1 For the simple matrix multiplication algorithm described in
 = 10 st  = 1, 4, 16, 64, and 256. Assume that p curves for the base problem of multiplying 16 x 16 matrices. Use
. Equation 8.14  = 1 in wt and
 Plot a third speedup curve for Problem 8.17, in which the problem size is scaled up according to the 8.18
. wt  and st ). Use the same values of 3/2 p ( Q isoefficiency function, which is
 The scaled speedup under this method of scaling is Hint:
 Plot the efficiency curves for the simple matrix multiplication algorithm corresponding to the standard 8.19
speedup curve (Problem 8.17), the scaled speedup curve (Problem 8.17), and the speedup curve when the
problem size is increased according to the isoefficiency function (Problem 8.18).
 A drawback of increasing the number of processes without increasing the total workload is that the speedup 8.20
does not increase linearly with the number of processes, and the efficiency drops monotonically. Based on your
experience with Problems 8.17 and 8.19, discuss whether using scaled speedup instead of standard speedup
solves the problem in general. What can you say about the isoefficiency function of a parallel algorithm whose
scaled speedup curve matches the speedup curve determined by increasing the problem size according to the
isoefficiency function?
 = 1 in the expression of parallel execution time wt  = 10 and st  Assume that 8.21 (Time-constrained scaling)
 = 1, 4, 16, 64, 256, 1024, p . For Section 8.2.1 ) of the matrix-multiplication algorithm discussed in Equation 8.14 (
and 4096, what is the largest problem that can be solved if the total run time is not to exceed 512 time units? In
general, is it possible to solve an arbitrarily large problem in a fixed amount of time, provided that an unlimited
number of processes is available? Give a brief explanation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 Describe a pipelined algorithm for performing back-substitution to solve a triangular system of equations of 8.22
 mesh of processes. n  x n  is 2-D partitioned onto an U  unit upper-triangular matrix n  x n , where the y  = Ux the form
2 n Give an expression for the parallel run time of the algorithm. Modify the algorithm to work on fewer than
processes, and derive an expression for the parallel execution time of the modified algorithm.
 to obtain the B  and A  matrices n  x n  for multiplying two Algorithm 8.7  Consider the parallel algorithm given in 8.23
 for a memory read or write operation on a matrix element and localt . Assume that it takes time C product matrix
-processor 2 n  to add and multiply two numbers. Determine the parallel run time for this algorithm on an ct time
CREW PRAM. Is this parallel algorithm cost-optimal?
 Assuming that concurrent read accesses to a memory location are serialized on an EREW PRAM, derive the 8.24
-processor EREW PRAM. Is this algorithm 2 n  on an Algorithm 8.7 parallel run time of the algorithm given in
cost-optimal on an EREW PRAM?
 on a B  and A  matrices n  x n Algorithm 8.7 An algorithm for multiplying two
. B  x A  = C CREW PRAM, yielding matrix
) n , C , B , A  MAT_MULT_CREW_PRAM ( procedure 1.
begin 2.
; n  x n  processes into a logical mesh of 2 n 3. Organize the
do j, i  each process P for 4.
begin 5.
] := 0; j , i[ C 6.
do  - 1 n to  := 0 k for 7.
]; j , k[ B ] x k , i[ A ] + j , i[ C ] := j , i[ C 8.
; endfor 9.
 MAT_MULT_CREW_PRAM end 10.
 processors. Assume that each processor has 2 n  Consider a shared-address-space parallel computer with 8.25
. Furthermore, processor j, i ] are stored in the local memory of processor P j , i  [ B ] and j , i[ A some local memory, and
 to perform a read or write operation wt  + localt ] in its local memory. Assume that it takes time j , i  [ C  computes j, i P
 on local memory. Derive an expression for the parallel run time of the localt on nonlocal memory and time
 on this parallel computer. Algorithm 8.7 algorithm in
 can be modified so that the parallel run time on an EREW PRAM is less than that in Problem Algorithm 8.7 8.26
. What is the parallel run time of Algorithm 8.8 on an EREW Algorithm 8.8 8.24. The modified program is shown in
PRAM and a shared-address-space parallel computer with memory access times as described in Problems 8.24
and 8.25? Is the algorithm cost-optimal on these architectures?
2 n  on a shared-address-space parallel computer with fewer than Algorithm 8.8  Consider an implementation of 8.27
) processors and with memory access times as described in Problem 8.25. What is the parallel runtime? p (say,
 on an B  and A  matrices n  x n Algorithm 8.8 An algorithm for multiplying two
. B  x A  = C EREW PRAM, yielding matrix
) n , C , B , A  MAT_MULT_EREW_PRAM ( procedure 1.
begin 2.
; n  x n  processes into a logical mesh of 2 n 3. Organize the
do j, i  each process P for 4.
begin 5.
] := 0; j , i[ C 6.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
do  - 1 n to  := 0 k for 7.
] + j , i[ C ] := j , i[ C 8.
]; j , n ) mod k  + j  +i [( B ] x n ) mod k  + j  +i , ( i[ A 
; endfor 9.
 MAT_MULT_EREW_PRAM end 10.
 on a Section 8.2.1  Consider the implementation of the parallel matrix multiplication algorithm presented in 8.28
shared-address-space computer with memory access times as given in Problem 8.25. In this algorithm, each
processor first receives all the data it needs into its local memory, and then performs the computation. Derive the
parallel run time of this algorithm. Compare the performance of this algorithm with that in Problem 8.27.
r  Use the results of Problems 8.23–8.28 to comment on the viability of the PRAM model as a platform fo 8.29
r parallel algorithm design. Also comment on the relevance of the message-passing model fo
. shared-address-space computers
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 9. Sorting
Sorting is one of the most common operations performed by a computer. Because sorted data are easier to manipulate than
randomly-ordered data, many algorithms require sorted data. Sorting is of additional importance to parallel computing because of its
close relation to the task of routing data among processes, which is an essential part of many parallel algorithms. Many parallel sorting
algorithms have been investigated for a variety of parallel computer architectures. This chapter presents several parallel sorting
algorithms for PRAM, mesh, hypercube, and general shared-address-space and message-passing architectures.
Sorting is defined as the task of arranging an unordered collection of elements into monotonically increasing (or decreasing) order.
 into a monotonically increasing S  elements in arbitrary order; sorting transforms n  > be a sequence of an , ..., 2 a , 1 a  = < S Specifically, let
. S ' is a permutation of S , and n j i  for 1  such that sequence
. In internal sorting, the number of elements to be sorted is small enough to fit external  or internal Sorting algorithms are categorized as
into the process's main memory. In contrast, external sorting algorithms use auxiliary storage (such as tapes and hard disks) for sorting
because the number of elements to be sorted is too large to fit into memory. This chapter concentrates on internal sorting algorithms
only.
. A comparison-based algorithm sorts an noncomparison-based  and comparison-based Sorting algorithms can be categorized as
unordered sequence of elements by repeatedly comparing pairs of elements and, if they are out of order, exchanging them. This
. The lower bound on the sequential complexity of any compare-exchange fundamental operation of comparison-based sorting is called
 is the number of elements to be sorted. Noncomparison-based n ), where n  log n ( Q sorting algorithms that is comparison-based is
algorithms sort by using certain known properties of the elements (such as their binary representation or their distribution). The
). We concentrate on comparison-based sorting algorithms in this chapter, although we n ( Q lower-bound complexity of these algorithms is
. Section 9.6 briefly discuss some noncomparison-based sorting algorithms in
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
9.1 Issues in Sorting on Parallel Computers
Parallelizing a sequential sorting algorithm involves distributing the elements to be sorted onto the available processes. This process
raises a number of issues that we must address in order to make the presentation of parallel sorting algorithms clearer.
9.1.1 Where the Input and Output Sequences are Stored
In sequential sorting algorithms, the input and the sorted sequences are stored in the process's memory. However, in parallel sorting there
are two places where these sequences can reside. They may be stored on only one of the processes, or they may be distributed among
the processes. The latter approach is particularly useful if sorting is an intermediate step in another algorithm. In this chapter, we assume
that the input and sorted sequences are distributed among the processes.
Consider the precise distribution of the sorted output sequence among the processes. A general method of distribution is to enumerate the
processes and use this enumeration to specify a global ordering for the sorted sequence. In other words, the sequence will be sorted with
 will be smaller thani P  in the enumeration, all the elements stored in j P  comes beforei P respect to this process enumeration. For instance, if
 . We can enumerate the processes in many ways. For certain parallel algorithms and interconnection networks, some j P those stored in
enumerations lead to more efficient parallel formulations than others.
9.1.2 How Comparisons are Performed
A sequential sorting algorithm can easily perform a compare-exchange on two elements because they are stored locally in the process's
memory. In parallel sorting algorithms, this step is not so easy. If the elements reside on the same process, the comparison can be done
easily. But if the elements reside on different processes, the situation becomes more complicated.
One Element Per Process
Consider the case in which each process holds only one element of the sequence to be sorted. At some point in the execution of the
j P  will hold the smaller andi P . After the comparison, j a  andi a ) may need to compare their elements, j P , i P algorithm, a pair of processes (
}. We can perform comparison by having both processes send their elements to each other. Each process compares the j a , i a the larger of {
, i a  will keep the larger of { j P  will keep the smaller andi P received element with its own and retains the appropriate element. In our example,
 illustrates, each compare-exchange operation Figure 9.1 . As compare-exchange }. As in the sequential case, we refer to this operation as j a
requires one comparison step and one communication step.
 send their elements to j P  andi P Figure 9.1. A parallel compare-exchange operation. Processes
}. j a  ,i a  keeps max{ j P }, and j a , i a  keeps min{i P each other. Process
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 are neighbors, and the communication channels are bidirectional, then the communication cost of a j P  andi P If we assume that processes
 are message-startup time and per-word transfer time, respectively. In commercially wt  and st ), where wt  + st compare-exchange step is (
. Note that in today's st , so the communication time is dominated by wt  is significantly larger than st available message-passing computers,
parallel computers it takes more time to send an element from one process to another than it takes to compare the elements.
Consequently, any parallel sorting formulation that uses as many processes as elements to be sorted will deliver very poor performance
because the overall parallel run time will be dominated by interprocess communication.
More than One Element Per Process
 be p A general-purpose parallel sorting algorithm must be able to sort a large sequence with a relatively small number of processes. Let
p/ n  be the number of elements to be sorted. Each process is assigned a block of n , and let -1 p P , ..., 1 P , 0 P the number of processes
, -1 p P , ... 1 P , 0 P  be the blocks assigned to processes -1 p A , ... 1 A , 0 A elements, and all the processes cooperate to sort the sequence. Let
. When the sorting algorithm finishes, j A  is less than or equal to every element ini A  if every element of j A i A respectively. We say that
. , and j i  for  such that  holds a seti P each process
 elements so that one of them p/ n  may have to redistribute their blocks of j P  andi P As in the one-element-per-process case, two processes
. If j P  andi P  be the blocks stored in processes j A  andi A  elements. Let p/ n  elements and the other will get the larger p/ n will get the smaller
 elements at each process is already sorted, the redistribution can be done efficiently as follows. Each process sends its p/ n the block of
block to the other process. Now, each process merges the two sorted blocks and retains only the appropriate half of the merged block. We
Figure . The compare-split operation is illustrated in compare-split refer to this operation of comparing and splitting two sorted blocks as
. 9.2
 to the other p/ n Figure 9.2. A compare-split operation. Each process sends its block of size
process. Each process merges the received block with its own block and retains only the
 retains the smaller elementsi P appropriate half of the merged block. In this example, process
 retains the larger elements. j P and process
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 are neighbors and that the communication channels are bidirectional, then the communication cost j P  andi P If we assume that processes
 decreases, and for sufficiently large blocks it st ). As the block size increases, the significance of p/ n wt  + st of a compare-split operation is (
/p). n ( Q  elements is p/ n can be ignored. Thus, the time required to merge two sorted blocks of
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
9.2 Sorting Networks
n ( Q  elements in time significantly smaller than n In the quest for fast sorting methods, a number of networks have been designed that sort
). These sorting networks are based on a comparison network model, in which many comparison operations are performed n log
simultaneously.
'. For an y ' and x  and two outputs y  and x . A comparator is a device with two inputs comparator The key component of these networks is a
 gives Figure 9.3 }. y , x ' = min{ y } and y , x ' = max{ x decreasing comparator }; for a y , x ' = max{ y } and y , x ' = min{ x , increasing comparator
the schematic representation of the two types of comparators. As the two elements enter the input wires of the comparator, they are
 and a compared and, if necessary, exchanged before they go to the output wires. We denote an increasing comparator by
. A sorting network is usually made up of a series of columns, and each column contains a number of decreasing comparator by
comparators connected in parallel. Each column of comparators performs a permutation, and the output obtained from the final column is
 of a network is the number of columns it depth  illustrates a typical sorting network. The Figure 9.4 sorted in increasing or decreasing order.
contains. Since the speed of a comparator is a technology-dependent constant, the speed of the network is proportional to its depth.
Figure 9.3. A schematic representation of comparators: (a) an increasing comparator, and (b) a
decreasing comparator.
Figure 9.4. A typical sorting network. Every sorting network is made up of a series of columns,
and each column contains a number of comparators connected in parallel.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
We can convert any sorting network into a sequential sorting algorithm by emulating the comparators in software and performing the
 are compared y  and x comparisons of each column sequentially. The comparator is emulated by a compare-exchange operation, where
and, if necessary, exchanged.
 is a n ) time. To simplify the presentation, we assume that n 2 (log Q  elements in n The following section describes a sorting network that sorts
power of two.
9.2.1 Bitonic Sort
) time. The key operation of the bitonic sorting network is the rearrangement of a n 2 (log Q  elements in n A bitonic sorting network sorts
> with the property that either (1) -1 n a , ..., 1 a , 0 a  is a sequence of elements < bitonic sequence bitonic sequence into a sorted sequence. A
> is monotonically -1 n a  +1, ...,i a  > is monotonically increasing and <i a , ..., 0 a  - 1, such that < n i , 0 i there exists an index
decreasing, or (2) there exists a cyclic shift of indices so that (1) is satisfied. For example, <1, 2, 4, 7, 6, 0> is a bitonic sequence, because
it first increases and then decreases. Similarly, <8, 9, 2, 1, 0, 4> is another bitonic sequence, because it is a cyclic shift of <0, 4, 8, 9, 2, 1>.
> be a -1 n a , ..., 1 a , 0 a  = < s We present a method to rearrange a bitonic sequence to obtain a monotonically increasing sequence. Let
. Consider the following -1 n a  ... /2+1 n a /2 n a  and /2-1 n a  ... 1 a 0 a bitonic sequence such that
: s subsequences of
1  Equation 9.
 are from the increasing part of the originali b } such that all the elements beforei /2+ n a , i a  = min{i b , there is an element 1 s In sequence
, the element 2 s  are from the decreasing part. Also, in sequencei b sequence and all the elements after
 are from the  are from the decreasing part of the original sequence and all the elements after is such that all the elements before
 are bitonic sequences. Furthermore, every element of the first sequence is smaller than 2 s  and 1 s increasing part. Thus, the sequences
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is less than or equal to all , 1 s  is greater than or equal to all elements ofi b every element of the second sequence. The reason is that
n . Thus, we have reduced the initial problem of rearranging a bitonic sequence of size i b  is greater than or equal to , and 2 s elements of
to that of rearranging two smaller bitonic sequences and concatenating the results. We refer to the operation of splitting a bitonic
 we assumed 2 s  and 1 s . Although in obtaining bitonic split  as a Equation 9.1  into the two bitonic sequences defined by n sequence of size
that the original sequence had increasing and decreasing sequences of the same length, the bitonic split operation also holds for any
bitonic sequence (Problem 9.3).
 for each of the bitonic subsequences until we obtain Equation 9.1 We can recursively obtain shorter bitonic sequences using
subsequences of size one. At that point, the output is sorted in monotonically increasing order. Since after each bitonic split operation the
. The n size of the problem is halved, the number of splits required to rearrange the bitonic sequence into a sorted sequence is log
. The recursive bitonic merge procedure is illustrated in bitonic merge procedure of sorting a bitonic sequence using bitonic splits is called
. Figure 9.5
Figure 9.5. Merging a 16-element bitonic sequence through a series of log 16 bitonic splits.
We now have a method for merging a bitonic sequence into a sorted sequence. This method is easy to implement on a network of
n . The network contains log Figure 9.6 , it is illustrated in bitonic merging network comparators. This network of comparators, known as a
/2 comparators and performs one step of the bitonic merge. This network takes as input the bitonic n columns. Each column contains
BM[n]. If we replace the  inputs by n sequence and outputs the sequence in sorted order. We denote a bitonic merging network with
 comparators, the input will be sorted in monotonically decreasing order; such a network is denoted  by Figure 9.6  comparators in
BM[n]. by
 - 1, and n  = 16. The input wires are numbered 0, 1 ..., n Figure 9.6. A bitonic merging network for
the binary representation of these numbers is shown. Each column of comparators is drawn
BM[16] bitonic merging network. The network takes separately; the entire figure represents a
a bitonic sequence and outputs it in sorted order.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 unordered elements. This is done by repeatedly merging bitonic n Armed with the bitonic merging network, consider the task of sorting
. Figure 9.7 sequences of increasing length, as illustrated in
Figure 9.7. A schematic representation of a network that converts an input sequence into a
BM[k] denote bitonic merging networks of BM[k] and bitonic sequence. In this example,
 comparators, respectively. The last merging network (  and  that use k input size
 = 16. n BM[16]) sorts the input. In this example,
, in which case y x  forms a bitonic sequence, since either y  and x Let us now see how this method works. A sequence of two elements
, in which case the bitonic y x  in the increasing part and no elements in the decreasing part, or y  and x the bitonic sequence has
 in the decreasing part and no elements in the increasing part. Hence, any unsorted sequence of elements is a y  and x sequence has
 merges adjacent bitonic sequences in Figure 9.7 concatenation of bitonic sequences of size two. Each stage of the network shown in
increasing and decreasing order. According to the definition of a bitonic sequence, the sequence obtained by concatenating the increasing
 is a concatenation of bitonic sequences Figure 9.7 and decreasing sequences is bitonic. Hence, the output of each stage in the network in
that are twice as long as those at the input. By merging larger and larger bitonic sequences, we eventually obtain a bitonic sequence of
 and the network as a bitonic sort . Merging this sequence sorts the input. We refer to the algorithm embodied in this method as n size
 is Figure 9.7 . The last stage of Figure 9.8  are shown explicitly in Figure 9.7 . The first three stages of the network in bitonic sorting network
. Figure 9.6 shown explicitly in
Figure 9.8. The comparator network that transforms an input sequence of 16 unordered
, the columns of comparators in each Figure 9.6 numbers into a bitonic sequence. In contrast to
bitonic merging network are drawn in a single box, separated by a dashed line.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. The other n  inputs. This has a depth of log n -element bitonic sorting network contains a bitonic merging network with n The last stage of an
 is given by the following recurrence Figure 9.7 ), of the network in n ( d /2 elements. Hence, the depth, n stages perform a complete sort of
relation:
2  Equation 9.
. This network , we obtain Equation 9.2 Solving
) sorting algorithm. The bitonic sorting network can also be adapted and n 2  log n ( Q can be implemented on a serial computer, yielding a
used as a sorting algorithm for parallel computers. In the next section, we describe how this can be done for hypercube-and
mesh-connected parallel computers.
9.2.2 Mapping Bitonic Sort to a Hypercube and a Mesh
In this section we discuss how the bitonic sort algorithm can be mapped on general-purpose parallel computers. One of the key aspects of
the bitonic algorithm is that it is communication intensive, and a proper mapping of the algorithm must take into account the topology of the
underlying interconnection network. For this reason, we discuss how the bitonic sort algorithm can be mapped onto the interconnection
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
network of a hypercube- and mesh-connected parallel computers.
/2 comparators. As Figures n  columns ofi  consists ofi  stages, and stage n  elements contains log n The bitonic sorting network for sorting
 wires. On a parallel computer, the n  show, each column of comparators performs compare-exchange operations on 9.8  and 9.6
compare-exchange function is performed by a pair of processes.
One Element Per Process
 processes contains one element of the input sequence. Graphically, each wire of the bitonic sorting network n In this mapping, each of the
represents a distinct process. During each step of the algorithm, the compare-exchange operations performed by a column of comparators
/2 pairs of processes. One important question is how to map processes to wires in order to minimize the distance that n are performed by
the elements travel during a compare-exchange operation. If the mapping is poor, the elements travel a long distance before they can be
compared, which will degrade performance. Ideally, wires that perform a compare-exchange should be mapped onto neighboring
n processes. Then the parallel formulation of bitonic sort will have the best possible performance over all the formulations that require
processes.
To obtain a good mapping, we must further investigate the way that input wires are paired during each stage of bitonic sort. Consider
 = 16. In each of the (1 + log 16)(log 16)/2 = 10 comparator columns, n , which show the full bitonic sorting network for 9.8  and 9.6 Figures
certain wires compare-exchange their elements. Focus on the binary representation of the wire labels. In any step, the compare-exchange
operation is performed between two wires only if their labels differ in exactly one bit. During each of the four stages, wires whose labels
differ in the least-significant bit perform a compare-exchange in the last step of each stage. During the last three stages, wires whose
labels differ in the second-least-significant bit perform a compare-exchange in the second-to-last step of each stage. In general, wires
 + 1) times. This observation helps us efficiently mapi  - n  least-significant bit perform a compare-exchange (log thi whose labels differ in the
wires onto processes by mapping wires that perform compare-exchange operations more frequently to processes that are close to each
other.
e  Mapping wires onto the processes of a hypercube-connected parallel computer is straightforward. Compare-exchang Hypercube
operations take place between wires whose labels differ in only one bit. In a hypercube, processes whose labels differ in only one bit are
 tol ). Thus, an optimal mapping of input wires to hypercube processes is the one that maps an input wire with label Section 2.4.3 neighbors (
 - 1. n  = 0, 1, ...,l  wherel a process with label
). In the final stage of d  = 2 p -dimensional hypercube (that is, d Consider how processes are paired for their compare-exchange steps in a
 bit th d bitonic sort, the input has been converted into a bitonic sequence. During the first step of this stage, processes that differ only in the
of the binary representation of their labels (that is, the most significant bit) compare-exchange their elements. Thus, the
 dimension. Similarly, during the second step of the algorithm, th d compare-exchange operation takes place between processes along the
 step of the final thi  dimension. In general, during the th  - 1) d the compare-exchange operation takes place among the processes along the (
 illustrates the communication during the last stage of the Figure 9.9  dimension. th  - 1))i  - ( d stage, processes communicate along the (
bitonic sort algorithm.
Figure 9.9. Communication during the last stage of bitonic sort. Each wire is mapped to a
hypercube process; each connection represents a compare-exchange between processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
-dimensional subcube, with each such sequence assigned to a different k  can be performed on a k A bitonic merge of sequences of size 2
 step of this bitonic merge, the processes that compare their elements are neighbors thi subcube (Problem 9.5). Furthermore, during the
, showing the communication characteristics of the bitonic sort Figure 9.7  is a modification of Figure 9.10  dimension. th  - 1))i  - ( k along the (
algorithm on a hypercube.
Figure 9.10. Communication characteristics of bitonic sort on a hypercube. During each stage
of the algorithm, processes communicate along the dimensions shown.
 and comp_exchange_max(i) . The algorithm relies on the functions Algorithm 9.1 The bitonic sort algorithm for a hypercube is shown in
 dimension and thi . These functions compare the local element with the element on the nearest process along the comp_exchange_min(i)
. Algorithm 9.1 retain either the minimum or the maximum of the two elements. Problem 9.6 explores the correctness of
 processes. In this d  = 2 n Algorithm 9.1 Parallel formulation of bitonic sort on a hypercube with
 is the dimension of the hypercube. d  is the process's label and label algorithm,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
) d , label  BITONIC_SORT( procedure 1.
begin 2.
do  - 1 d to  := 0i for 3.
do  0 downto i  := j for 4.
then label  bit of th j label  bit of st  + 1)i  ( if 5.
; comp_exchange max(j) 6.
else 7.
; comp_exchange min(j) 8.
 BITONIC_SORT end 9.
During each step of the algorithm, every process performs a compare-exchange operation. The algorithm performs a total of (1 + log
)/2 such steps; thus, the parallel run time is n )(log n
3  Equation 9.
This parallel formulation of bitonic sort is cost optimal with respect to the sequential implementation of bitonic sort (that is, the process-time
)), but it is not cost-optimal with respect to an optimal comparison-based sorting algorithm, which has a serial time n 2  log n ( Q product is
). n  log n ( Q complexity of
-process mesh. Unfortunately, the n  Consider how the input wires of the bitonic sorting network can be mapped efficiently onto an Mesh
connectivity of a mesh is lower than that of a hypercube, so it is impossible to map wires to processes such that each compare-exchange
operation occurs only between neighboring processes. Instead, we map wires such that the most frequent compare-exchange operations
occur between neighboring processes.
. Each process in this Figure 9.11 There are several ways to map the input wires onto the mesh processes. Some of these are illustrated in
figure is labeled by the wire that is mapped onto it. Of these three mappings, we concentrate on the row-major shuffled mapping, shown in
. We leave the other two mappings as exercises (Problem 9.7). Figure 9.11(c)
Figure 9.11. Different ways of mapping the input wires of the bitonic sorting network to a mesh
of processes: (a) row-major mapping, (b) row-major snakelike mapping, and (c) row-major
shuffled mapping.
The advantage of row-major shuffled mapping is that processes that perform compare-exchange operations reside on square subsections
of the mesh whose size is inversely related to the frequency of compare-exchanges. For example, processes that perform
compare-exchange during every stage of bitonic sort (that is, those corresponding to wires that differ in the least-significant bit) are
 least-significant bit are mapped onto mesh processes that are thi neighbors. In general, wires that differ in the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
communication links away. The compare-exchange steps of the last stage of bitonic sort for the row-major shuffled mapping are shown in
. Note that each earlier stage will have only some of these steps. Figure 9.12
 = 16 on a mesh, using the n Figure 9.12. The last stage of the bitonic sort algorithm for
row-major shuffled mapping. During each step, process pairs compare-exchange their elements.
Arrows indicate the pairs of processes that perform compare-exchange operations.
)/2 steps of the algorithm, processes that are a certain distance apart compare-exchange their elements. The n )(log n During the (1 + log
distance between processes determines the communication overhead of the parallel formulation. The total amount of communication
) (Problem 9.7). During each step of ( Q , which is performed by each process is
). This n 2 (log Q the algorithm, each process performs at most one comparison; thus, the total computation performed by each process is
yields a parallel run time of
). n  log n ( Q ), but the sequential complexity of sorting is 1.5 n ( Q This is not a cost-optimal formulation, because the process-time product is
Although the parallel formulation for a hypercube was optimal with respect to the sequential complexity of bitonic sort, the formulation for
 elements, one per mesh process, for certain inputs the element stored in the n mesh is not. Can we do any better? No. When sorting
process at the upper-left corner will end up in the process at the lower-right corner. For this to happen, this element must travel along
 Our. ) ( W  communication links before reaching its destination. Thus, the run time of sorting on a mesh is bounded by
parallel formulation achieves this lower bound; thus, it is asymptotically optimal for the mesh architecture.
A Block of Elements Per Process
In the parallel formulations of the bitonic sort algorithm presented so far, we assumed there were as many processes as elements to be
sorted. Now we consider the case in which the number of elements to be sorted is greater than the number of processes.
p/ n . Each process is assigned a block of n  < p  be the number of elements to be sorted, such that n  be the number of processes and p Let
elements and cooperates with the other processes to sort them. One way to obtain a parallel formulation with our new setup is to think of
 processes by using a single process. The run p/ n  smaller processes. In other words, imagine emulating p/ n each process as consisting of
 processes. This virtual process p/ n  because each process is doing the work of p/ n time of this formulation will be greater by a factor of
p ) leads to a poor parallel implementation of bitonic sort. To see this, consider the case of a hypercube with Section 5.3 approach (
). n 2  log n ( Q ), which is not cost-optimal because the process-time product is p )/ n 2  log n (( Q processes. Its run time will be
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Think of the Section 9.1 An alternate way of dealing with blocks of elements is to use the compare-split operation presented in
 blocks is identical to that of p )-element blocks as elements to be sorted using compare-split operations. The problem of sorting the p/ n (
 blocks using compare-split operations instead of compare-exchange operations (Problem 9.8). Since the p performing a bitonic sort on the
)/2 steps. Because compare-split operations preserve the p )(log p , the bitonic sort algorithm has a total of (1 + log p total number of blocks is
 elements will be sorted. The main difference between this n initial sorted order of the elements in each block, at the end of these steps the
 elements assigned to each process are initially sorted locally, using a p/ n formulation and the one that uses virtual processes is that the
fast sequential sorting algorithm. This initial local sort makes the new formulation more efficient and cost-optimal.
p  processes is similar to the one-element-per-process case, but now we have p  The block-based algorithm for a hypercube with Hypercube
 elements. Furthermore, the compare-exchange operations are replaced by compare-split operations, each p , instead of p/ n blocks of size
 elements (using merge sort) in time p/ n ) communication time. Initially the processes sort their p/ n ( Q ) computation time and p/ n ( Q taking
) compare-split steps. The parallel run time of this formulation is p 2 (log Q )) and then perform p/ n ) log( p/ n (( Q
), the speedup and efficiency are as follows: n  log n ( Q Because the sequential complexity of the best sorting algorithm is
4  Equation 9.
(1). Thus, this algorithm can efficiently use up to O ) = n )/(log p 2 , for a cost-optimal formulation (log Equation 9.4 From
, the isoefficiency function due to both communication and extra work is Equation 9.4  processes. Also from
. Hence, this parallel formulation of bitonic p ), which is worse than any polynomial isoefficiency function for sufficiently large p 2  log p log p ( Q
sort has poor scalability.
s  The block-based mesh formulation is also similar to the one-element-per-process case. The parallel run time of this formulation is a Mesh
follows:
 to the communication Note that comparing the communication overhead of this mesh-based parallel bitonic sort
. This )), we can see that it is higher by a factor of p )/ p 2  log n (( O overhead of the hypercube-based formulation (
 difference in the bisection bandwidth of these architectures. This illustrates that a proper mapping factor is smaller than the
of the bitonic sort on the underlying mesh can achieve better performance than that achieved by simply mapping the hypercube algorithm
on the mesh.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The speedup and efficiency are as follows:
5  Equation 9.
p  elements on n Table 9.1. The performance of parallel formulations of bitonic sort for
processes.
Isoefficiency Function Corresponding Parallel Run Time (1) Q  = E Maximum Number of Processes for Architecture
Hypercube
) p 2  log p log p ( Q
Mesh
) n / log n ( Q ) n 2 (log Q
) n ( Q ) n (log Q Ring
) p p (2 Q
 = p . Thus, this formulation can efficiently use up to , for a cost-optimal formulation Equation 9.5 From
. The isoefficiency function of this formulation is , the isoefficiency function Equation 9.5 ) processes. Also from n 2 (log Q
exponential, and thus is even worse than that for the hypercube.
From the analysis for hypercube and mesh, we see that parallel formulations of bitonic sort are neither very efficient nor very scalable. This
is primarily because the sequential algorithm is suboptimal. Good speedups are possible on a large number of processes only if the
number of elements to be sorted is very large. In that case, the efficiency of the internal sorting outweighs the inefficiency of the bitonic
 summarizes the performance of bitonic sort on hypercube-, mesh-, and ring-connected parallel computer. Table 9.1 sort.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
9.3 Bubble Sort and its Variants
). We now turn our attention to more n 2 (log Q  elements in a time of n The previous section presented a sorting network that could sort
) processes to n ( Q ) time complexity exist, we should be able to use n  log n ( Q traditional sorting algorithms. Since serial algorithms with
). As we will see, this is difficult to achieve. We can, however, easily parallelize many sequential sorting n (log Q  elements in time n sort
. bubble sort ) complexity. The algorithms we present are based on 2 n ( Q algorithms that have
, 1 a The sequential bubble sort algorithm compares and exchanges adjacent elements in the sequence to be sorted. Given a sequence <
). This step n a , -1 n a ), ..., ( 3 a , 2 a ), ( 2 a , 1 a  - 1 compare-exchange operations in the following order: ( n  >, the algorithm first performs n a , ..., 2 a
moves the largest element to the end of the sequence. The last element in the transformed sequence is then ignored, and the sequence
 - 1 iterations. n . The sequence is sorted after of compare-exchanges is applied to the resulting sequence
We can improve the performance of bubble sort by terminating when no exchanges take place during an iteration. The bubble sort
. Algorithm 9.2 algorithm is shown in
) iterations; thus, the complexity of bubble sort is n ( Q ), and we perform a total of n ( Q An iteration of the inner loop of bubble sort takes time
). Bubble sort is difficult to parallelize. To see this, consider how compare-exchange operations are performed during each phase of 2 n ( Q
). Bubble sort compares all adjacent pairs in order; hence, it is inherently sequential. In the Algorithm 9.2 the algorithm (lines 4 and 5 of
following two sections, we present two variants of bubble sort that are well suited to parallelization.
Algorithm 9.2 Sequential bubble sort algorithm.
) n  BUBBLE_SORT( procedure 1.
begin 2.
do  1 downto  - 1 n  :=i for 3.
do i to  := 1 j for 4.
); + 1 j a , j a ( compare-exchange 5.
 BUBBLE_SORT end 6.
9.3.1 Odd-Even Transposition
/2 compare-exchange n  is even), each of which requires n  phases ( n  elements in n  algorithm sorts odd-even transposition The
> be the sequence to be n a , ..., 2 a , 1 a operations. This algorithm alternates between two phases, called the odd and even phases. Let <
sorted. During the odd phase, elements with odd indices are compared with their right neighbors, and if they are out of sequence they are
 is even). Similarly, during the even phase, n ) are compare-exchanged (assuming n a , -1 n a ), ..., ( 4 a , 3 a ), ( 2 a , 1 a exchanged; thus, the pairs (
elements with even indices are compared with their right neighbors, and if they are out of sequence they are exchanged; thus, the pairs
 phases of odd-even exchanges, the sequence is sorted. Each phase of n ) are compare-exchanged. After -1 n a , -2 n a ), ..., ( 5 a , 4 a ), ( 3 a , 2 a (
). 2 n ( Q  phases; thus, the sequential complexity is n ) comparisons, and there are a total of n ( Q the algorithm (either odd or even) requires
. Figure 9.13  and is illustrated in Algorithm 9.3 The odd-even transposition sort is shown in
 = 8 elements, using the odd-even transposition sort algorithm. During n Figure 9.13. Sorting
 = 8 elements are compared. n each phase,
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Algorithm 9.3 Sequential odd-even transposition sort algorithm.
) n  ODD-EVEN( procedure 1.
begin 2.
do n to  := 1i for 3.
begin 4.
then  is oddi if 5.
do /2 - 1 n to  := 0 j for 6.
); + 2 j 2 a , + 1 j 2 a ( compare-exchange 7.
then  is eveni if 8.
do /2 - 1 n to  := 1 j for 9.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
); + 1 j 2 a , j 2 a ( compare-exchange 10.
end for 11.
 ODD-EVEN end 12.
Parallel Formulation
It is easy to parallelize odd-even transposition sort. During each phase of the algorithm, compare-exchange operations on pairs of
 be the number of processes (also the n elements are performed simultaneously. Consider the one-element-per-process case. Let
 initially resides oni a number of elements to be sorted). Assume that the processes are arranged in a one-dimensional array. Element
. During the odd phase, each process that has an odd label compare-exchanges its element with the element n  = 1, 2, ...,i  fori P process
residing on its right neighbor. Similarly, during the even phase, each process with an even label compare-exchanges its element with the
. Algorithm 9.4 element of its right neighbor. This parallel formulation is presented in
-process ring. n Algorithm 9.4 The parallel formulation of odd-even transposition sort on an
) n  ODD-EVEN_PAR ( procedure 1.
begin 2.
 := process's label id 3.
do n to  := 1i for 4.
begin 5.
then  is oddi if 6.
then  is odd id if 7.
 + 1); id ( compare-exchange_min 8.
else 9.
 - 1); id ( compare-exchange_max 10.
then  is eveni if 11.
then  is even id if 12.
 + 1); id ( compare-exchange_min 13.
else 14.
 - 1); id ( compare-exchange_max 15.
end for 16.
 ODD-EVEN_PAR end 17.
e During each phase of the algorithm, the odd or even processes perform a compare- exchange step with their right neighbors. As w
 such phases are performed; thus, the parallel run time of this formulation is n (1). A total of Q , this requires time Section 9.1 know from
), this formulation of odd-even transposition n  log n ( Q  elements is n ). Since the sequential complexity of the best sorting algorithm for n ( Q
). 2 n ( Q sort is not cost-optimal, because its process-time product is
. Initially, each n  < p  be the number of processes, where p To obtain a cost-optimal parallel formulation, we use fewer processes. Let
)) time. After this, the p/ n ) log( p/ n (( Q  elements, which it sorts internally (using merge sort or quicksort) in p/ n process is assigned a block of
/2 even), performing compare-split operations. At the end of these phases, the list is sorted p /2 odd and p  phases ( p processes execute
) is spent communicating. p/ n ( Q ) comparisons are performed to merge two blocks, and time p/ n ( Q (Problem 9.10). During each phase,
Thus, the parallel run time of the formulation is
), the speedup and efficiency of this formulation are as follows: n  log n ( Q Since the sequential complexity of sorting is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
6  Equation 9.
). The isoefficiency function of this parallel formulation is n (log O  = p , odd-even transposition sort is cost-optimal when Equation 9.6 From
), which is exponential. Thus, it is poorly scalable and is suited to only a small number of processes. p  2 p ( Q
9.3.2 Shellsort
The main limitation of odd-even transposition sort is that it moves elements only one position at a time. If a sequence has just a few
) to 2 n ( Q ) distance from their proper positions, then the sequential algorithm still requires time n ( Q elements out of order, and if they are
sort the sequence. To make a substantial improvement over odd-even transposition sort, we need an algorithm that moves elements
long distances. Shellsort is one such serial sorting algorithm.
 be the number of processes. To simplify the presentation we will assume that the p  be the number of elements to be sorted and n Let
, but the algorithm can be easily extended to work for an arbitrary number of d  = 2 p number of processes is a power of two, that is,
 elements. The processes are considered to be arranged in a logical p/ n processes as well. Each process is assigned a block of
one-dimensional array, and the ordering of the processes in that array defines the global ordering of the sorted sequence. The algorithm
consists of two phases. During the first phase, processes that are far away from each other in the array compare-split their elements.
Elements thus move long distances to get close to their final destinations in a few steps. During the second phase, the algorithm
switches to an odd-even transposition sort similar to the one described in the previous section. The only difference is that the odd and
even phases are performed only as long as the blocks on the processes are changing. Because the first phase of the algorithm moves
elements close to their final destinations, the number of odd and even phases performed by the second phase may be substantially
. p smaller than
)) time. Then, each process is paired with its corresponding p/ n  log( p/ n ( Q  elements internally in p/ n Initially, each process sorts its block of
. Each pair of processes -1 i - p P /2, is paired with process p  <i , where i P process in the reverse order of the array. That is, process
/2 processes and the p performs a compare-split operation. Next, the processes are partitioned into two groups; one group has the first
/2 processes and the above scheme of p /2 processes. Now, each group is treated as a separate set of p other group has the last
 steps, d process-pairing is applied to determine which processes will perform the compare-split operation. This process continues for
 = 3. d  for Figure 9.14 until each group contains only a single process. The compare-split operations of the first phase are illustrated in
Note that it is not a direct parallel formulation of the sequential shellsort, but it relies on similar ideas.
Figure 9.14. An example of the first phase of parallel shellsort on an eight-process array.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
/2 p  compare-split operations. In each compare-split operation a total of p  = log d In the first phase of the algorithm, each process performs
 elements. The communication time required by these compare-split p/ n pairs of processes need to exchange their locally stored
), the amount of time p ( Q operations depend on the bisection bandwidth of the network. In the case in which the bisection bandwidth is
 odd and even phases arel ). In the second phase, p )/ p  log n (( Q ). Thus, the complexity of this phase is p/ n ( Q required by each operation is
. Thus, the parallel run time of the algorithm is p)/ n ( Q performed, each requiring time
7  Equation 9.
 is small, then the algorithm performs significantly better than odd-evenl . If l The performance of shellsort depends on the value of
. l ), then both algorithms perform similarly. Problem 9.13 investigates the worst-case value of p ( Q  isl transposition sort; if
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
9.4 Quicksort
 log n ( Q All the algorithms presented so far have worse sequential complexity than that of the lower bound for comparison-based sorting,
). Quicksort is one of the most common n  log n ( Q  algorithm, which has an average complexity of quicksort ). This section examines the n
sorting algorithms for sequential computers because of its simplicity, low overhead, and optimal average complexity.
Quicksort is a divide-and-conquer algorithm that sorts a sequence by recursively dividing it into smaller subsequences. Assume that the
]. Quicksort consists of two steps: divide and conquer. During the divide step, n [1... A -element sequence to be sorted is stored in the array n
] such that each element of the first r  + 1... s[ A ] and s ... q[ A ] is partitioned (rearranged) into two nonempty subsequences r ... q[ A a sequence
subsequence is smaller than or equal to each element of the second subsequence. During the conquer step, the subsequences are sorted
] are sorted and the first subsequence has smaller r  + 1... s[ A ] and s ... q[ A by recursively applying quicksort. Since the subsequences
elements than the second, the entire sequence is sorted.
y ] partitioned into two parts – one with all elements smaller than the other? This is usually accomplished b r ... q[ A How is the sequence
r ] into two parts – one with elements less than o r ... q[ A ] and using this element to partition the sequence r ... q[ A  from x selecting one element
. Algorithm 9.5 . The quicksort algorithm is presented in pivot  is called the x .Element x  and the other with elements greater than x equal to
Figure ] as the pivot. The operation of quicksort is illustrated in r ... q[ A This algorithm arbitrarily chooses the first element of the sequence
. 9.15
 = 8. n Figure 9.15. Example of the quicksort algorithm sorting a sequence of size
Algorithm 9.5 The sequential quicksort algorithm.
) r , q , A  QUICKSORT ( procedure 1.
begin 2.
then r  < q if 3.
begin 4.
]; q[ A  := x 5.
; q  := s 6.
do r to  + 1 q  :=i for 7.
then x ]i[ A if 8.
begin 9.
 + 1; s  := s 10.
]);i[ A ], s[ A 11. swap(
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
end if 12.
]); s[ A ], q[ A 13. swap(
); s , q , A 14. QUICKSORT (
); r  + 1, s , A 15. QUICKSORT (
end if 16.
 QUICKSORT end 17.
). Quicksort's performance is greatly affected by the way it partitions a sequence. k ( Q  is k The complexity of partitioning a sequence of size
 - 1. The run time in this case is k  is split poorly, into two subsequences of sizes 1 and k Consider the case in which a sequence of size
). Alternatively, consider the case in which the 2 n ( Q ) = n ( T ), whose solution is n ( Q  - 1) + n ( T ) = n ( T given by the recurrence relation
 elements. In this case, the run time is given by the  and sequence is split well, into two roughly equal-size subsequences of
). The second split yields an optimal algorithm. Although n  log n ( Q ) = n ( T ), whose solution is n ( Q /2) + n ( T ) = 2 n ( T recurrence relation
) worst-case complexity, its average complexity is significantly better; the average number of compare-exchange 2 n ( O quicksort can have
, which is asymptotically optimal. There are n  log n operations needed by quicksort for sorting a randomly-ordered input sequence is 1.4
several ways to select pivots. For example, the pivot can be the median of a small number of elements of the sequence, or it can be an
element selected at random. Some pivot selection strategies have advantages over others for certain input sequences.
9.4.1 Parallelizing Quicksort
Section Quicksort can be parallelized in a variety of ways. First, consider a naive parallel formulation that was also discussed briefly in
 show that, during each call of QUICKSORT, the array is Algorithm 9.5  in the context of recursive decomposition. Lines 14 and 15 of 3.2.1
partitioned into two parts and each part is solved recursively. Sorting the smaller arrays represents two completely independent
subproblems that can be solved in parallel. Therefore, one way to parallelize quicksort is to execute it initially on a single process; then,
when the algorithm performs its recursive calls (lines 14 and 15), assign one of the subproblems to another process. Now each of these
processes sorts its array by using quicksort and assigns one of its subproblems to other processes. The algorithm terminates when the
arrays cannot be further partitioned. Upon termination, each process holds an element of the array, and the sorted order can be recovered
 elements. Its major n  processes to sort n by traversing the processes as we will describe later. This parallel formulation of quicksort uses
 ], is done by a single process. Since one r  + 1 ... s[ A ] and s  ... q[ A  ] into two smaller arrays, r  ... q[ A drawback is that partitioning the array
). This formulation is not n ( W ], the run time of this formulation is bounded below by n [1 ... A process must partition the original array
). 2 n ( W cost-optimal, because its process-time product is
The main limitation of the previous parallel formulation is that it performs the partitioning step serially. As we will see in subsequent
formulations, performing partitioning in parallel is essential in obtaining an efficient parallel quicksort. To see why, consider the recurrence
) is due to the partitioning n ( Q ), which gives the complexity of quicksort for optimal pivot selection. The term n ( Q /2) + n  ( T ) = 2 n  ( T equation
). From these two complexities, we can think of n  log n ( Q of the array. Compare this complexity with the overall complexity of the algorithm,
g ) – that of splitting the array. Therefore, if the partitionin n ( Q ) steps, each requiring time n (log Q the quicksort algorithm as consisting of
), which leads to a n (log Q ) processes, it is possible to obtain an overall parallel run time of n ( Q (1), using Q step is performed in time
cost-optimal formulation. However, without parallelizing the partitioning step, the best we can do (while maintaining cost-optimality) is to
) (Problem 9.14). Hence, parallelizing the partitioning step has the potential to n ( Q  elements in time n ) processes to sort n (log Q use only
yield a significantly faster parallel formulation.
) processes. n ( Q (1) by using Q  into two smaller arrays in time n In the previous paragraph, we hinted that we could partition an array of size
However, this is difficult for most parallel computing models. The only known algorithms are for the abstract PRAM models. Because of
(1) on realistic shared-address-space and message-passing parallel Q communication overhead, the partitioning step takes longer than
computers. In the following sections we present three distinct parallel formulations: one for a CRCW PRAM, one for a
shared-address-space architecture, and one for a message-passing platform. Each of these formulations parallelizes quicksort by
performing the partitioning step in parallel.
9.4.2 Parallel Formulation for a CRCW PRAM
Section -process arbitrary CRCW PRAM. Recall from n  elements on an n We will now present a parallel formulation of quicksort for sorting
 that an arbitrary CRCW PRAM is a concurrent-read, concurrent-write parallel random-access machine in which write conflicts are 2.4.1
resolved arbitrarily. In other words, when more than one process tries to write to the same memory location, only one arbitrarily chosen
process is allowed to write, and the remaining writes are ignored.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Executing quicksort can be visualized as constructing a binary tree. In this tree, the pivot is the root; elements smaller than or equal to the
 illustrates the binary tree constructed by Figure 9.16 pivot go to the left subtree, and elements larger than the pivot go to the right subtree.
. The sorted sequence can be obtained from this tree by performing an Figure 9.15 the execution of the quicksort algorithm illustrated in
in-order traversal. The PRAM formulation is based on this interpretation of quicksort.
Figure 9.16. A binary tree generated by the execution of the quicksort algorithm. Each level of the tree represents a
), which is also n (log Q different array-partitioning iteration. If pivot selection is optimal, then the height of the tree is
the number of iterations.
d The algorithm starts by selecting a pivot element and partitioning the array into two parts – one with elements smaller than the pivot an
s the other with elements larger than the pivot. Subsequent pivot elements, one for each new subarray, are then selected in parallel. Thi
o formulation does not rearrange elements; instead, since all the processes can read the pivot in constant time, they know which of the tw
. subarrays (smaller or larger) the elements assigned to them belong to. Thus, they can proceed to the next iteration
 is assignedi ] and process n [1... A . The array to be sorted is stored in Algorithm 9.6 The algorithm that constructs the binary tree is shown in
] keep track of the children of a given pivot. For each process, the local variable n [1... rightchild ] and n [1... leftchild ]. The arraysi[ A element
root  stores the label of the process whose element is the pivot. Initially, all the processes write their process labels into the variablei parent
] is root[ A . The value root in line 5. Because the concurrent write operation is arbitrary, only one of these labels will actually be written into
] writei parent[ A  . Next, processes that have elements smaller thani  for each processi parent  is copied into root used as the first pivot and
]. Thus, all processesi parent[ rightchild ], and those with larger elements write their process label intoi parent[ leftchild their process labels into
], and those with elements in the larger partitioni parent[ leftchild whose elements belong in the smaller partition have written their labels into
r ]. Because of the arbitrary concurrent-write operations, only two values – one foi parent[ rightchild have written their labels into
t ] – are written into these locations. These two values become the labels of the processes thai parent[ rightchild ] and one fori parent[ leftchild
 pivot n hold the pivot elements for the next iteration, in which two smaller arrays are being partitioned. The algorithm continues until
. Figure 9.17 elements are selected. A process exits when its element becomes a pivot. The construction of the binary tree is illustrated in
(1). Thus, the average complexity of the binary tree Q During each iteration of the algorithm, a level of the tree is constructed in time
(log n) (Problem 9.16). Q ) as the average height of the tree is n (log Q building algorithm is
 are shown rightchild  and leftchild Figure 9.17. The execution of the PRAM algorithm on the array shown in (a). The arrays
in (c), (d), and (e) as the algorithm progresses. Figure (f) shows the binary tree constructed by the algorithm. Each
node is labeled by the process (in square brackets), and the element is stored at that process (in curly brackets). The
element is the pivot. In each node, processes with smaller elements than the pivot are grouped on the left side of the
node, and those with larger elements are grouped on the right side. These two groups form the two partitions of the
original array. For each partition, a pivot element is selected at random from the two groups that form the children of
the node.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Algorithm 9.6 The binary tree construction procedure for the CRCW PRAM parallel quicksort formulation.
]) n [1... A  BUILD TREE ( procedure 1.
begin 2.
do i  each process for 3.
begin 4.
; root := i 5.
; root  :=i parent 6.
 + 1; n ] :=i[ rightchild ] :=i[ leftchild 7.
end for 8.
do r oot i  for each process repeat 9.
begin 10.
or ])i parent[ A ] <i[ A  ( if 11.
then )i parent  <i and ]i parent[ A ]=i[ A  (
begin 12.
 ;i ] :=i parent[ leftchild 13.
then exit ]i parent[ leftchild  =i if 14.
];i parent[ leftchild  :=i parent else 15.
end for 16.
else 17.
begin 18.
; i ] :=i parent[ rightchild 19.
then exit ]i parent[ rightchild  =i if 20.
];i parent[ rightchild  :=i parent else 21.
end else 22.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
end repeat 23.
 BUILD_TREE end 24.
After building the binary tree, the algorithm determines the position of each element in the sorted array. It traverses the tree and keeps a
count of the number of elements in the left and right subtrees of any element. Finally, each element is placed in its proper position in time
(1), and the array is sorted. The algorithm that traverses the binary tree and computes the position of each element is left as an exercise Q
 log n ( Q process PRAM. Thus, its overall process-time product is - ) on an n n (log Q (Problem 9.15). The average run time of this algorithm is
), which is cost-optimal. n
9.4.3 Parallel Formulation for Practical Architectures
-process system connected via an interconnection network. p We now turn our attention to a more realistic parallel architecture – that of a
Initially, our discussion will focus on developing an algorithm for a shared-address-space system and then we will show how this algorithm
can be adapted to message-passing systems.
Shared-Address-Space Parallel Formulation
 elements that need to be sorted and n  be an array of A The quicksort formulation for a shared-address-space system works as follows. Let
 elements, and the labels of the processes define the p/ n  be the number of processes. Each process is assigned a consecutive block of p
 .i P  be the block of elements assigned to processi A global order of the sorted sequence. Let
, upon receiving the pivot,i P The algorithm starts by selecting a pivot element, which is broadcast to all processes. Each process
 and one with elements largeri S rearranges its assigned block of elements into two sub-blocks, one with elements smaller than the pivot
 approach of quicksort. The next step of the collapsing the loops  rearrangement is done in place using the local . Thisi L than the pivot
)  so that all the elements that are smaller than the pivot (i.e., A algorithm is to rearrange the elements of the original array
) are stored at the end of are stored at the beginning of the array, and all the elements that are larger than the pivot (i.e.,
the array.
 rearrangement is done, then the algorithm proceeds to partition the processes into two groups, and assign to the first global Once this
 . Each of these steps is L , and to the second group the task of sorting the larger elements S group the task of sorting the smaller elements
performed by recursively calling the parallel quicksort algorithm. Note that by simultaneously partitioning both the processes and the
original array each group of processes can proceed independently. The recursion ends when a particular sub-block of elements is
assigned to only a single process, in which case the process sorts the elements using a serial quicksort algorithm.
 blocks. In particular, the first L  and S The partitioning of processes into two groups is done according to the relative sizes of the
, and the rest of the processes are assigned to sort the larger S  processes are assigned to sort the smaller elements
. Note that the 0.5 term in the above formula is to ensure that the processes are assigned in the most balanced fashion. L elements
Example 9.1 Efficient parallel quicksort
 illustrates this algorithm using an example of 20 integers and five processes. In the first step, each process Figure 9.18
locally rearranges the four elements that it is initially responsible for, around the pivot element (seven in this example), so
that the elements smaller or equal to the pivot are moved to the beginning of the locally assigned portion of the array (and
are shaded in the figure). Once this local rearrangement is done, the processes perform a global rearrangement to obtain
the third array shown in the figure (how this is performed will be discussed shortly). In the second step, the processes are
} and is responsible for sorting the elements that are smaller than or 1 P , 0 P partitioned into two groups. The first contains {
} and is responsible for sorting the elements that are 4 P , 3 P , 2 P equal to seven, and the second group contains processes {
greater than seven. Note that the sizes of these process groups were created to match the relative size of the smaller than
and larger than the pivot arrays. Now, the steps of pivot selection, local, and global rearrangement are recursively
repeated for each process group and sub-array, until a sub-array is assigned to a single process, in which case it proceeds
to sort it locally. Also note that these final local sub-arrays will in general be of different size, as they depend on the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
elements that were selected to act as pivots.
Figure 9.18. An example of the execution of an efficient shared-address-space quicksort algorithm.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 will end A  into the smaller and larger sub-arrays we need to know where each element of A In order to globally rearrange the elements of
. In this Figure 9.19 up going at the end of that rearrangement. One way of doing this rearrangement is illustrated at the bottom of
 is L  blocks over all the processes, in increasing order of process label. Similarly,i S  is obtained by concatenating the various S approach,
 sub-block will bei S  th element of its j , thei P  blocks in the same order. As a result, for processi L obtained by concatenating the various
.  sub-block will be stored at locationi L  th element of its j , and the stored at location
Figure 9.19. Efficient global rearrangement of the array.
. Two prefix-sums are computed, one Section 4.3 These locations can be easily computed using the prefix-sum operation described in
 that store these prefix p  be the arrays of size R  and Q  sub-blocks. Leti L  sub-blocks and the other the sizes of thei S involving the sizes of the
sums, respectively. Their elements will be
 is thei R  is the starting location in the final array where its lower-than-the-pivot element will be stored, andi Q ,i P Note that for each process
ending location in the final array where its greater-than-the-pivot elements will be stored. Once these locations have been determined, the
. Note Figure 9.19 . These steps are illustrated in n ' of size A  can be easily performed by using an auxiliary array A overall rearrangement of
, in the sense that the value that is computed for Section 4.3 that the above definition of prefix-sum is slightly different from that described in
 prefix-sum. non-inclusive ) itself. This type of prefix-sum is sometimes referred to asi L  (ori S ) does not includei R  (ori Q location
e  The complexity of the shared-address-space formulation of the quicksort algorithm depends on two things. The first is th Analysis
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
amount of time it requires to split a particular array into the smaller-than- and the greater-than-the-pivot sub-arrays, and the second is the
degree to which the various pivots being selected lead to balanced partitions. In this section, to simplify our analysis, we will assume that
pivot selection always results in balanced partitions. However, the issue of proper pivot selection and its impact on the overall parallel
. Section 9.4.4 performance is addressed in
 processes, the shared-address-space formulation of the quicksort algorithm needs to perform four p  elements and n Given an array of
steps: (i) determine and broadcast the pivot; (ii) locally rearrange the array assigned to each process; (iii) determine the locations in the
globally rearranged array that the local elements will go to; and (iv) perform the global rearrangement. The first step can be performed in
) using an efficient recursive doubling approach for shared-address-space broadcast. The second step can be done in time p (log Q time
) using two prefix p (log Q ) using the traditional quicksort algorithm for splitting around a pivot element. The third step can be done in p/ n ( Q
) as it requires us to copy the local elements to their final p/ n ( Q sum operations. Finally, the fourth step can be done in at least time
 This process is repeated for each of the two. ) p (log Q ) + p/ n ( Q -element array is n destination. Thus, the overall complexity of splitting an
 parts, at which point each process sorts the elements of the array p subarrays recursively on half the processes, until the array is split into
assigned to it using the serial quicksort algorithm. Thus, the overall complexity of the parallel algorithm is:
8  Equation 9.
2  log p ( Q  p) term, which leads to an overall isoefficiency of 2 (log Q The communication overhead in the above formulation is reflected in the
p). It is interesting to note that the overall scalability of the algorithm is determined by the amount of time required to perform the pivot
broadcast and the prefix sum operations.
Message-Passing Parallel Formulation
The quicksort formulation for message-passing systems follows the general structure of the shared-address-space formulation. However,
' are stored in shared memory and can be A  and the globally rearranged array A unlike the shared-address-space case in which array
A accessed by all the processes, these arrays are now explicitly distributed among the processes. This makes the task of splitting
somewhat more involved.
. This array is also A  elements of array p/ n In particular, in the message-passing version of the parallel quicksort, each process stores
partitioned around a particular pivot element using a two-phase approach. In the first phase (which is similar to the shared-address-space
i L  andi S  is partitioned into the smaller-than- and larger-than-the-pivot sub-arraysi P  at processi A formulation), the locally stored array
locally. In the next phase, the algorithm first determines which processes will be responsible for recursively sorting the
) and which process will be responsible for recursively sorting the smaller-than-the-pivot sub-arrays (i.e.,
 arrays to thei L  andi S ). Once this is done, then the processes send their larger-than-the-pivot sub-arrays (i.e.,
, and the algorithm proceeds L  and one for S corresponding processes. After that, the processes are partitioned into the two groups, one for
recursively. The recursion terminates when each sub-array is assigned to a single process, at which point it is sorted locally.
 is identical to that for the shared-address-space L  and S The method used to determine which processes will be responsible for sorting
 be the number of L p  and S p formulation, which tries to partition the processes to match the relative size of the two sub-arrays. Let
 elements of the S p |/ S  processes will end up storing | S p , respectively. Each one of the L  and S processes assigned to sort
 elements of the larger-than-the-pivot L p |/ L  processes will end up storing | L p smaller-than-the-pivot sub-array, and each one of the
 elements follows the same overall strategy as thei L  andi S  will send itsi P sub-array. The method used to determine where each process
) based on the L  (or S ) sub-arrays will be stored in consecutive locations ini L  (ori S shared-address-space formulation. That is, the various
) L p  (or S p ) into L  (or S process number. The actual processes that will be responsible for these elements are determined by the partition of
)i L  (ori S  may need to split itsi P equal-size segments, and can be computed using a prefix-sum operation. Note that each process
sub-arrays into multiple segments and send each one to different processes. This can happen because its elements may be assigned to
) that span more than one process. In general, each process may have to send its elements to two different processes; L  (or S locations in
however, there may be cases in which more than two partitions are required.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
e  Our analysis of the message-passing formulation of quicksort will mirror the corresponding analysis of the shared-address-spac Analysis
formulation.
) bisection bandwidth. The amount of time required to split an p  ( O  processes and p Consider a message-passing parallel computer with
) for p (log Q ) for splitting the locally assigned portion of the array, p/ n ( Q ) for broadcasting the pivot element, p (log Q  is n array of size
 sub-arrays, and thei L  andi S performing the prefix sums to determine the process partition sizes and the destinations of the various
amount of time required for sending and receiving the various arrays. This last step depends on how the processes are mapped on the
underlying architecture and on the maximum number of processes that each process needs to communicate with. In general, this
communication step involves all-to-all personalized communication (because a particular process may end-up receiving elements from all
 which is, ) p (log Q ) + p/ n ( Q ). Thus, the overall complexity for the split is p/ n ( Q other processes), whose complexity has a lower bound of
, Equation 9.8 asymptotically similar to that of the shared-address-space formulation. As a result, the overall runtime is also the same as in
). p 2  log p ( Q and the algorithm has a similar isoefficiency function of
9.4.4 Pivot Selection
In the parallel quicksort algorithm, we glossed over pivot selection. Pivot selection is particularly difficult, and it significantly affects the
algorithm's performance. Consider the case in which the first pivot happens to be the largest element in the sequence. In this case, after
 - 1 elements. n  - 1 processes will be assigned p the first split, one of the processes will be assigned only one element, and the remaining
 - 1 processes will participate in the p Hence, we are faced with a problem whose size has been reduced only by one element but only
sorting operation. Although this is a contrived example, it illustrates a significant problem with parallelizing the quicksort algorithm. Ideally,
the split should be done such that each partition has a non-trivial fraction of the original array.
 split, one process in each of the process groups randomly thi One way to select pivots is to choose them at random as follows. During the
selects one of its elements to be the pivot for this partition. This is analogous to the random pivot selection in the sequential quicksort
algorithm. Although this method seems to work for sequential quicksort, it is not well suited to the parallel formulation. To see this,
consider the case in which a bad pivot is selected at some point. In sequential quicksort, this leads to a partitioning in which one
subsequence is significantly larger than the other. If all subsequent pivot selections are good, one poor pivot will increase the overall work
by at most an amount equal to the length of the subsequence; thus, it will not significantly degrade the performance of sequential
quicksort. In the parallel formulation, however, one poor pivot may lead to a partitioning in which a process becomes idle, and that will
persist throughout the execution of the algorithm.
p/ n If the initial distribution of elements in each process is uniform, then a better pivot selection method can be derived. In this case, the
-element p/ n  elements. In other words, the median of each n elements initially stored at each process form a representative sample of all
-element sequence. Why is this a good pivot selection scheme under the n subsequence is very close to the median of the entire
assumption of identical initial distributions? Since the distribution of elements on each process is the same as the overall distribution of the
 elements, the median selected to be the pivot during the first step is a good approximation of the overall median. Since the selected pivot n
is very close to the overall median, roughly half of the elements in each process are smaller and the other half larger than the pivot.
/2 elements. Similarly, the elements assigned to each n Therefore, the first split leads to two partitions, such that each of them has roughly
process of the group that is responsible for sorting the smaller-than-the-pivot elements (and the group responsible for sorting the
/2 smaller (or larger) elements of the original list. Thus, the split not only n larger-than-the-pivot elements) have the same distribution as the
maintains load balance but also preserves the assumption of uniform element distribution in the process group. Therefore, the application
of the same pivot selection scheme to the sub-groups of processes continues to yield good pivot selection.
 elements in each process have the same distribution as the overall sequence? The answer depends on p/ n Can we really assume that the
the application. In some applications, either the random or the median pivot selection scheme works well, but in others neither scheme
delivers good performance. Two additional pivot selection schemes are examined in Problems 9.20 and 9.21.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
9.5 Bucket and Sample Sort
bucket sort ] is the b , a  elements whose values are uniformly distributed over an interval [ n A popular serial algorithm for sorting an array of
, and each element is placed buckets  equal-sized subintervals referred to as m ] is divided into b , a algorithm. In this algorithm, the interval [
], the number of elements in each bucket is b , a  elements are uniformly distributed over the interval [ n in the appropriate bucket. Since the
(n Q . The algorithm then sorts the elements in each bucket, yielding a sorted sequence. The run time of this algorithm is m / n roughly
). Note that the reason that bucket sort can achieve such a low complexity is because it n ( Q  it exhibits linear run time,, ) n ( Q )).Form = m / n log(
]. b , a  elements to be sorted are uniformly distributed over an interval [ n assumes that the
 be the number of processes. Initially, each p  be the number of elements to be sorted and n Parallelizing bucket sort is straightforward. Let
. The parallel formulation of bucket sort p  = m  elements, and the number of buckets is selected to be p/ n process is assigned a block of
 buckets. p  sub-blocks, one for each of the p  elements into p/ n consists of three steps. In the first step, each process partitions its block of
) and thus the interval for each bucket. In the second step, each process b , a This is possible because each process knows the interval [
sends sub-blocks to the appropriate processes. After this step, each process has only the elements belonging to the bucket assigned to it.
In the third step, each process sorts its bucket internally by using an optimal sequential sorting algorithm.
] is not realistic. In most cases, the b , a Unfortunately, the assumption that the input elements are uniformly distributed over an interval [
actual input may not have such a distribution or its distribution may be unknown. Thus, using bucket sort may result in buckets that have a
 will yield sample sort significantly different number of elements, thereby degrading performance. In such situations an algorithm called
-element sequence, and n  is selected from the s significantly better performance. The idea behind sample sort is simple. A sample of size
 - 1 elements from the result. These elements (called m the range of the buckets is determined by sorting the sample and choosing
 equal-sized buckets. After defining the buckets, the algorithm proceeds in the same way as bucket sort. m ) divide the sample into splitters
-element sequence. n  and the way it is selected from the s The performance of sample sort depends on the sample size
Consider a splitter selection scheme that guarantees that the number of elements ending up in each bucket is roughly the same for all
n  be the number of buckets. The scheme works as follows. It divides the m  be the number of elements to be sorted and n buckets. Let
 - 1 evenly spaced m  each, and sorts each block by using quicksort. From each sorted block it chooses m / n  blocks of size m elements into
 - 1) elements selected from all the blocks represent the sample used to determine the buckets. This scheme m ( m elements. The
 (Problem 9.28). m / n guarantees that the number of elements ending up in each bucket is less than 2
; thus, at the end of the p  = m  be the number of processes. As in bucket sort, set p How can we parallelize the splitter selection scheme? Let
 elements, which p/ n algorithm, each process contains only the elements belonging to a single bucket. Each process is assigned a block of
o  - 1 sample elements t p  - 1 evenly spaced elements from the sorted block. Each process sends its p it sorts sequentially. It then chooses
0 P  - 1 splitters. Finally, process p  - 1) sample elements and selects the p ( p  then sequentially sorts the 0 P . Process 0 P one process – say
 - 1 splitters to all the other processes. Now the algorithm proceeds in a manner identical to that of bucket sort. This p broadcasts the
. Figure 9.20 algorithm is illustrated in
Figure 9.20. An example of the execution of sample sort on an array with 24 elements on three
processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
) bisection bandwidth. p  ( O  processes and p  We now analyze the complexity of sample sort on a message-passing computer with Analysis
 1 p - ). Sending p ( Q  - 1 sample elements requires time p )), and the selection of p/ n ) log( p/ n (( Q  elements requires time p/ n The internal sort of
 - 1) p ( p ). The time to internally sort the 2 p ( Q ); the time required is Section 4.4  is similar to a gather operation ( 0 P elements to process
 1 splitters are sent to all the other processes by p - ). The p ( Q  1 splitters is p - ), and the time to select p  log 2 p ( Q  is 0 P sample elements at
 1 splitters in its local sorted block p -  these insert ). Each process can p  log p ( Q ), which requires time Section 4.1 using one-to-all broadcast (
 sub-blocks, one for each bucket. The time p  - 1 binary searches. Each process thus partitions its block into p  by performing p/ n of size
)). Each process then sends sub-blocks to the appropriate processes (that is, buckets). The p/ n  log( p ( Q required for this partitioning is
communication time for this step is difficult to compute precisely, as it depends on the size of the sub-blocks to be communicated. These
). p  log p  ( O ) + n  ( O . Thus, the upper bound on the communication time is p/ n sub-blocks can vary arbitrarily between 0 and
) elements. In this 2 p/ n ( Q If we assume that the elements stored in each process are uniformly distributed, then each sub-block has roughly
case, the parallel run time is
9  Equation 9.
 - 1) sample elements, then the time for sorting the p ( p ). If bitonic sort is used to sort the p  log 3 p ( Q In this case, the isoefficiency function is
) (Problem 9.30). p  log 2 p ( Q ), and the isoefficiency will be reduced to p  log p ( Q sample would be
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
9.6 Other Sorting Algorithms
As mentioned in the introduction to this chapter, there are many sorting algorithms, and we cannot explore them all in this chapter.
However, in this section we briefly present two additional sorting algorithms that are important both practically and theoretically. Our
) for references on these and other algorithms. Section 9.7 discussion of these schemes will be brief. Refer to the bibliographic remarks (
9.6.1 Enumeration Sort
All the sorting algorithms presented so far are based on compare-exchange operations. This section considers an algorithm based on
, which does not use compare-exchange. The basic idea behind enumeration sort is to determine the rank of each enumeration sort
 can be used toi a  in the sequence to be sorted. The rank ofi a  is the number of elements smaller thani a  of an element rank element. The
place it in its correct position in the sorted sequence. Several parallel algorithms are based on enumeration sort. Here we present one
(1). Q  processes in time 2 n  elements by using n such algorithm that is suited to the CRCW PRAM model. This formulation sorts
Assume that concurrent writes to the same memory location of the CRCW PRAM result in the sum of all the values written being stored
 processes as being arranged in a two-dimensional grid. The algorithm consists of two 2 n ). Consider the Section 2.4.1 at that location (
 . During the second step, each j a  of processes computes the number of elements smaller than j steps. During the first step, each column
. It uses an Algorithm 9.7  in its proper position as determined by its rank. The algorithm is shown in j a  of the first row places j 1, P process
, j, i P ] to store the rank of each element. The crucial steps of this algorithm are lines 7 and 9. There, each process n  [1... C auxiliary array
] and writes 0 otherwise. Because of the additive-write conflict resolution scheme, the j[ A ] is smaller thani[ A ] if the element j  [ C writes 1 in
] and thus compute its rank. The run time of this algorithm j[ A effect of these instructions is to count the number of elements smaller than
(1). Modifications of this algorithm for various parallel architectures are discussed in Problem 9.26. Q is
Algorithm 9.7 Enumeration sort on a CRCW PRAM with additive-write conflict resolution.
) n  ENUM SORT ( procedure 1.
begin 2.
do j 1, P  each process for 3.
] :=0; j[ C 4.
do j,i P  each process for 5.
then ) j  <i and ] j[ A ]=i[ A  ( or ]) j[ A ] <i[ A  ( if 6.
] := 1; j[ C 7.
else 8.
] := 0; j[ C 9.
do j 1, P  each process for 10.
]; j[ A ]] := j[ C[ A 11.
 ENUM_SORT end 12.
9.6.2 Radix Sort
 be the number of bits in the binary b  algorithm relies on the binary representation of the elements to be sorted. Let radix sort The
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Radix sort b  < r  bits at a time, where r representation of an element. The radix sort algorithm examines the elements to be sorted
 bits. For radix sort to work r  least significant block of thi  , it sorts the elements according to theiri  iterations. During iteration r / b requires
 if its output preserves the order of input elements with the stable  sorts must be stable. A sorting algorithm is r / b properly, each of the
 -bit blocks when these blocks are equal. The most common r same value. Radix sort is stable if it preserves the input order of any two
 - r ) because the range of possible values [0...2 Section 9.6.1  sorts uses enumeration sort ( r  radix-2 r / b implementation of the intermediate
1] is small. For such cases, enumeration sort significantly outperforms any comparison-based sorting algorithm.
 processes. The parallel radix sort n  elements on a message-passing computer with n Consider a parallel formulation of radix sort for
-bit blocks. The r  enumeration sorts of the r / b . The main loop of the algorithm (lines 3–17) performs the Algorithm 9.8 algorithm is shown in
 functions. These functions are similar to those described in parallel_sum()  and prefix_sum() enumeration sort is performed by using the
-bit r . During each iteration of the inner loop (lines 6–15), radix sort determines the position of the elements with an 4.3  and 4.1 Sections
 holds the rank  . It does this by summing all the elements with the same value and then assigning them to processes. The variable j value of
position of each element. At the end of the loop (line 16), each process sends its element to the appropriate process. Process labels
determine the global order of sorted elements.
] to be n [1... A Algorithm 9.8 A parallel radix sort algorithm, in which each element of the array
 computes the prefix sum of the prefix sum() sorted is assigned to one process. The function
 variable. flag  returns the total sum of the parallel sum()  variable, and the function flag
) r , A  RADIX SORT( procedure 1.
begin 2.
do  - 1 r / b to  := 0i for 3.
begin 4.
; offset := 0 5.
do  - 1 r  2 to  := 0 j for 6.
begin 7.
; flag := 0 8.
then j ] = k P[ A -bit block of r  least significant thi  the if 9.
; flag := 1 10.
index := prefix_sum(flag) 11.
then  = 1 flag if 12.
; rank := offset + index 13.
; offset := parallel_sum(flag) 14.
endfor 15.
; rank P ] to process k P[ A  send its element k P 16. each process
endfor 17.
 RADIX_SORT end 18.
) on a n (log Q  operations is prefix_sum()  and parallel_sum() , the complexity of both the 4.3  and 4.1 As shown in Sections
). Thus, the parallel run time of n ( Q rocesses. The complexity of the communication step on line 16 is p message-passing computer with n
this algorithm is
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
9.7 Bibliographic Remarks
 (log O  elements in time n ] discusses sorting networks and their history. The question of whether a sorting network could sort Knu73 Knuth [
 elements n ] discovered a sorting network that could sort AKS83 ) remained open for a long time. In 1983, Ajtai, Komlos, and Szemeredi [ n
) comparators. Unfortunately, the constant of their sorting network is quite large (many thousands), n  log n  ( O ) by using n  (log O in time
], who also discovered the network for odd-even Bat68 and thus is not practical. The bitonic sorting network was discovered by Batcher [
] maps the bitonic sort onto a Sto71 ). Stone [ n 2  (log O  elements in time n sort. These were the first networks capable of sorting
]shows that bitonic sort Sie77 ). Siegel [ n 2  (log O  processes in time n  elements by using n perfect-shuffle interconnection network, sorting
). The block-based hypercube formulation of bitonic sort is discussed in n 2  (log O can also be performed on the hypercube in time
]. The shuffled row-major indexing formulation of bitonic 88 + FJL  is adopted from [ Algorithm 9.1 ]. 88 + FJL  [ et al. ] and Fox Joh84 Johnsson [
]. They also show how the odd-even merge sort can be TK77 sort on a mesh-connected computer is presented by Thompson and Kung [
] present a row-major indexed bitonic sort formulation for a mesh with NS79 used with snakelike row-major indexing. Nassimi and Sahni [
the same performance as shuffled row-major indexing. An improved version of the mesh odd-even merge is proposed by Kumar and
] describe one way to BS78 ]. The compare-split operation can be implemented in many ways. Baudet and Stevenson [ KH83 Hirschberg [
perform this operation. An alternative way of performing a compare-split operation based on a bitonic sort (Problem 9.1) that requires no
]. HM80 additional memory was discovered by Hsiao and Menon [
]. Several early references to parallel sorting by odd-even transposition are Knu73 The odd-even transposition sort is described by Knuth [
]. Another BS78 ]. The block-based extension of the algorithm is due to Baudet and Stevenson [ Kun80 ] and Kung [ Knu73 given by Knuth [
variation of block-based odd-even transposition sort that uses bitonic merge-split is described by DeWitt, Friedland, Hsiao, and Menon
)). In contrast to the algorithm of Baudet and Stevenson p/ n  log( n  + n  ( O  processes and runs in time p ]. Their algorithm uses DFHM82[
) + 1 storage p/ n  requires only ( et al.  storage locations in each process, the algorithm of DeWitt p/ n ], which is faster but requires 4 BS78[
locations to perform the compare-split operation.
 increases, the probability that the n ]. They show that, as 88 + FJL  [ et al.  is due to Fox Section 9.3.2 The shellsort algorithm described in
 phases) diminishes. A different shellsort p final odd-even transposition will exhibit worst-case performance (in other words, will require
]. Qui88 ] is described by Quinn [ She59 algorithm based on the original sequential algorithm [
] provides a good reference on the details of the Sed78 ]. Sedgewick [ Hoa62 The sequential quicksort algorithm is due to Hoare [
]. Rob75 implementation and how they affect its performance. The random pivot-selection scheme is described and analyzed by Robin [
] and used in parallel formulations by Sed78 The algorithm for sequence partitioning on a single process was suggested by Sedgewick [
]. CV91 ) is due to Chlebus and Vrto [ Section 9.4.2 ]. The CRCW PRAM algorithm ( Qui88 ], and Quinn [ Dem82 ], Deminet [ Ras78 Raskin [
n Many other quicksort-based algorithms for PRAM and shared-address-space parallel computers have been developed that can sort
] developed a quicksort algorithm for a CRCW PRAM that MG89 ) processes. Martel and Gusfield [ n ( Q ) by using n (log Q elements in time
) on the average. An algorithm suited to shared-address-space parallel computers with fetch-and-add capabilities 3 n  ( O requires space
) on the average and can be n (log Q ]. Their algorithm runs in time HNR90 was discovered by Heidelberger, Norton, and Robinson [
adapted for commercially available shared-address-space computers. The hypercube formulation of quicksort described in Problem 9.17
]. His hyperquicksort algorithm uses the median-based pivot-selection scheme and assumes that the elements in Wag87 is due to Wagar [
each process have the same distribution. His experimental results show that hyperquicksort is faster than bitonic sort on a hypercube. An
]. This scheme significantly improves the 88 + FJL  [ et al. alternate pivot-selection scheme (Problem 9.20) was implemented by Fox
] describes a quicksort Pla89 performance of hyperquicksort when the elements are not evenly distributed in each process. Plaxton [
)). This algorithm uses a time p/ n  log( p 3  +log p )/ p 2 / 3  log n  +( p )/ n  log n  (( O  elements in time n -process hypercube that sorts p algorithm on a
)) parallel selection algorithm to determine the perfect pivot selection. The mesh formulation of quicksort p/ n  log( p 2  + log p ) log log p/ n  (( O
]. They also describe a modification to the algorithm that reduces SKAT91a (Problem 9.24) is due to Singh, Kumar, Agha, and Tomlinson [
). p (log Q the complexity of each step by a factor of
] proposed a bucket sort Hir78 The sequential bucket sort algorithm was first proposed by Isaac and Singleton in 1956. Hirschberg [
 processes. A side n ) by using n (log Q  - 1] in time n  elements in the range [0... n algorithm for the EREW PRAM model. This algorithm sorts
] generalizes this Hir78 ). Hirschberg [ 2 (n Q effect of this algorithm is that duplicate elements are eliminated. Their algorithm requires space
) by using n  log k ( Q  elements in time n algorithm so that duplicate elements remain in the sorted array. The generalized algorithm sorts
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
n
1+1/k
 is an arbitrary integer. k  processes, where
) Section 9.5 ]. The parallel sample sort algorithm ( FM70 The sequential sample sort algorithm was discovered by Frazer and McKellar [
]. Several parallel formulations of sample sort for different parallel architectures have been SS90 was discovered by Shi and Schaeffer [
] presented a splitter selection scheme that guarantees that the number of elements AOB93 proposed. Abali, Ozguner, and Bataineh [
-process p  elements on a n ), on average, to sort n 2  log p  + p )/ n  log n  (( O . Their algorithm requires time p/ n ending up in each bucket is
-process hypercube-connected n  elements on an n ] present a sample sort algorithm that sorts RV87 hypercube. Reif and Valiant [
] present parallel formulations of a SG88 ] and Seidel and George [ WS88 ) with high probability. Won and Sahni [ n  (log O computer in time
]. FKO86  [ bin sort variation of sample sort called
Many other parallel sorting algorithms have been proposed. Various parallel sorting algorithms can be efficiently implemented on a
], and Bitton, SV81 ], Shiloach and Vishkin [ BH82 ], Borodin and Hopcroft [ Akl85 PRAM model or on shared-address-space computers. Akl [
] proposed a sorting algorithm for a Val75 ] provide a good survey of the subject. Valiant [ BDHM84 DeWitt, Hsiao, and Menon [
/2 processes. n ) by using n  log log n  (log O  elements in time n shared-address-space SIMD computer that sorts by merging. It sorts
] Col88 process PRAM. Cole [ - ) for an n n (log Q  elements in time n ] was the first to develop an algorithm that sorted Rei81 Reischuk [
] has shown that the Nat90 ) on an EREW PRAM. Natvig [ n (log Q  elements in time n developed a parallel merge-sort algorithm that sorts
) merge sort as n (log Q ) bitonic sort outperforms the n 2 (log Q constants hidden behind the asymptotic notation are very large. In fact, the
, that runs smoothsort ] has developed a hypercube sorting algorithm, called Pla89 ! Plaxton [ 22  is smaller than 7.6 x 10 n long as
] proposed a sorting algorithm, called Lei85a asymptotically faster than any previously known algorithm for that architecture. Leighton [
, that consists of a sequence of sorts followed by elementary matrix operations. Columnsort is a generalization of Batcher's columnsort
] presented an algorithm based on Leighton's columnsort for reconfigurable meshes with buses NS93 odd-even sort. Nigam and Sahni [
 (1). O -process mesh in time 2 n  elements on an n that sorts
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
 be the elements stored k x , ..., 2 x , 1 x  Consider the following technique for performing the compare-split operation. Let 9.1
 in decreasing order. Processj P  be the elements stored at process k y , ..., 2 y , 1 y  in increasing order, and leti P at process
 and keeps thei P  and then sends the larger element back to process 1 y  with 1 x  comparesj P . Process j P  to 1 x  sendsi P
 )l y  ,l x  ). If for any pair ( k y  , k x ), ..., ( 3 y , 3 x ), ( 2 y , 2 x smaller element for itself. The same procedure is repeated for pairs (
, then no more exchanges are needed. Finally, each process sorts its elements. Show thatl y l x , k l for 1
this method correctly performs a compare-split operation. Analyze its run time, and compare the relative merits of this
method to those of the method presented in the text. Is this method better suited for MIMD or SIMD parallel
computers?
 for blocks of elements, is a partial ordering relation. Section 9.1  relation, as defined in  Show that the 9.2
 if it is reflexive, antisymmetric, and transitive. partial ordering  A relation is a Hint:
 is a power of 2. In the following cases, prove that the n },where -1 n a , ..., 1 a , 0 a  = { s  Consider the sequence 9.3
, s , on the sequence Section 9.2.1  obtained by performing the bitonic split operation described in 2 s  and 1 s sequences
 are smaller than the elements 1 s  are bitonic sequences, and (2) the elements of 2 s  and 1 s satisfy the properties that (1)
: 2 s of
. -1 n a  ··· /2+1 n a 2 n/  and a /2-1 n  a  ··· 1 a 0 a  is a bitonic sequence such that s . a
, 0 i  for some -1 n a  ··· +2 i a +1 i a  andi a  ··· 1 a 0 a  is a bitonic sequence such that s
 - 1. n i
. b
 is a bitonic sequence that becomes increasing-decreasing after shifting its elements. s . c
 items. Show how n  processes available to sort n  In the parallel formulations of bitonic sort, we assumed that we had 9.4
/2 processes are available. n the algorithm needs to be modified when only
 is performed on a k  Show that, in the hypercube formulation of bitonic sort, each bitonic merge of sequences of size 2 9.5
-dimensional hypercube and each sequence is assigned to a separate hypercube. k
 is correct. In particular, show that the Algorithm 9.1  Show that the parallel formulation of bitonic sort shown in 9.6
algorithm correctly compare-exchanges elements and that the elements end up in the appropriate processes.
 Consider the parallel formulation of bitonic sort for a mesh-connected parallel computer. Compute the exact parallel 9.7
run time of the following formulations:
 for a mesh with store-and-forward routing. Figure 9.11(a) One that uses the row-major mapping shown in . a
 for a mesh with store-and-forward Figure 9.11(b) One that uses the row-major snakelike mapping shown in
routing.
. b
 for a mesh with store-and-forward Figure 9.11(c) One that uses the row-major shuffled mapping shown in
routing.
. c
Also, determine how the above run times change when cut-through routing is used.
 Show that the block-based bitonic sort algorithm that uses compare-split operations is correct. 9.8
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processes. Show how to map the input wires of the bitonic n  Consider a ring-connected parallel computer with 9.9
sorting network onto the ring so that the communication cost is minimized. Analyze the performance of your mapping.
 processes are available. Analyze the performance of your parallel formulation for this p Consider the case in which only
case. What is the largest number of processes that can be used while maintaining a cost-optimal parallel formulation?
What is the isoefficiency function of your scheme?
 Prove that the block-based odd-even transposition sort yields a correct algorithm. 9.10
 This problem is similar to Problem 9.8. Hint:
-process mesh-connected computer. p ) to a Section 9.3.2  Show how to apply the idea of the shellsort algorithm ( 9.11
Your algorithm does not need to be an exact copy of the hypercube formulation.
-process hypercube. Note that the shellsort p  Show how to parallelize the sequential shellsort algorithm for a 9.12
 is not an exact parallelization of the sequential algorithm. Section 9.3.2 algorithm presented in
, which is the l . Its performance depends on the value of Section 9.3.2  Consider the shellsort algorithm presented in 9.13
number of odd and even phases performed during the second phase of the algorithm. Describe a worst-case initial key
) phases. What is the probability of this worst-case scenario? p ( Q  =l distribution that will require
 we discussed a parallel formulation of quicksort for a CREW PRAM that is based on assigning Section 9.4.1  In 9.14
 elements. Based on this approach, n  processes to sort n each subproblem to a separate process. This formulation uses
). Derive expressions for the parallel run time, n  < p  processes, where ( p derive a parallel formulation that uses
efficiency, and isoefficiency function. What is the maximum number of processes that your parallel formulation can use
and still remain cost-optimal?
 and Algorithm 9.6  Derive an algorithm that traverses the binary search tree constructed by the algorithm in 9.15
 processes and solve the n determines the position of each element in the sorted array. Your algorithm should use
) on an arbitrary CRCW PRAM. n (log Q problem in time
). Compute the average height of the Section 9.4.2  Consider the PRAM formulation of the quicksort algorithm ( 9.16
binary tree generated by the algorithm.
-process hypercube p  Consider the following parallel quicksort algorithm that takes advantage of the topology of a 9.17
 be the number of processes in a d  = 2 p  be the number of elements to be sorted and n connected parallel computer. Let
 elements, and the labels of the processes define the p/ n -dimensional hypercube. Each process is assigned a block of d
global order of the sorted sequence. The algorithm starts by selecting a pivot element, which is broadcast to all
processes. Each process, upon receiving the pivot, partitions its local elements into two blocks, one with elements
th d smaller than the pivot and one with elements larger than the pivot. Then the processes connected along the
communication link exchange appropriate blocks so that one retains elements smaller than the pivot and the other
 bit (the most significant bit) position th d retains elements larger than the pivot. Specifically, each process with a 0 in the
 bit th d of the binary representation of its process label retains the smaller elements, and each process with a 1 in the
 label bit is 0 th d  - 1)-dimensional hypercube whose d retains the larger elements. After this step, each process in the (
 - 1)-dimensional hypercube will have d will have elements smaller than the pivot, and each process in the other (
elements larger than the pivot. This procedure is performed recursively in each subcube, splitting the subsequences
g  such splits – one along each dimension – the sequence is sorted with respect to the global orderin d further. After
s imposed on the processes. This does not mean that the elements at each process are sorted. Therefore, each proces
Algorithm sorts its local elements by using sequential quicksort. This hypercube formulation of quicksort is shown in
. Figure 9.21 . The execution of the algorithm is illustrated in 9.9
e  = 3. The three splits – on d Figure 9.21. The execution of the hypercube formulation of quicksort for
e along each communication link – are shown in (a), (b), and (c). The second column represents th
-element sequence into subcubes. The arrows between subcubes indicate the n partitioning of the
movement of larger elements. Each box is marked by the binary representation of the process labels
in that subcube. A * denotes that all the binary combinations are included.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
-element p/ n  is the B -dimensional hypercube. d Algorithm 9.9 A parallel formulation of quicksort on a
subsequence assigned to each process.
) n , B  HYPERCUBE_QUICKSORT ( procedure 1.
begin 2.
 := process's label; id 3.
do d to  := 1i for 4.
begin 5.
; pivot  := x 6.
; 2 B  < x 1 B  such that 2 B  and 1 B  into B 7. partition
then  bit is 0 thi if 8.
begin 9.
 communication link; th i  to the process along the 2 B 10. send
 communication link; th i  := subsequence received along the C 11.
 ; C 1 B  := B 12.
endif 13.
else 14.
 communication link; thi  to the process along the 1 B 15. send
 communication link; thi  := subsequence received along the C 16.
 ; C 2 B  := B 17.
endelse 18.
endfor 19.
 using sequential quicksort; B 20. sort
 HYPERCUBE_QUICKSORT end 21.
Analyze the complexity of this hypercube-based parallel quicksort algorithm. Derive expressions for the parallel
runtime, speedup, and efficiency. Perform this analysis assuming that the elements that were initially assigned at each
process are distributed uniformly.
-dimensional hypercube described in Problem 9.17. Show that d  Consider the parallel formulation of quicksort for a 9.18
y  splits – one along each communication link – the elements are sorted according to the global order defined b d after
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. the process's labels
-dimensional hypercube described in Problem 9.17. Compare d  Consider the parallel formulation of quicksort for a 9.19
. Which algorithm is more Section 9.4.3 this algorithm against the message-passing quicksort algorithm described in
scalable? Which algorithm is more sensitive on poor selection of pivot elements?
-dimensional hypercube d  An alternative way of selecting pivots in the parallel formulation of quicksort for a 9.20
 - 1 pivots at once as follows: d (Problem 9.17) is to select all the 2
 elements at random.l Each process picks a sample of . a
). Section 9.3.2  items by using the shellsort algorithm ( d  x 2l All processes together sort the sample of . b
 - 1 equally distanced pivots from this list. d Choose 2 . c
Broadcast pivots so that all the processes know the pivots. . d
? Under what n  should be a function ofl ? Do you thinkl How does the quality of this pivot selection scheme depend on
assumptions will this scheme select good pivots? Do you think this scheme works when the elements are not
identically distributed on each process? Analyze the complexity of this scheme.
 Another pivot selection scheme for parallel quicksort for hypercube (Section 9.17) is as follows. During the split 9.21
 pairs of processes exchange elements. The pivot is selected in two steps. In the first step, -1 i  dimension, 2 thi along the
 pairs of processes compute the median of their combined sequences. In the second step, the median -1 i each of the 2
 communication thi  medians is computed. This median of medians becomes the pivot for the split along the -1 i of the 2
link. Subsequent pivots are selected in the same way among the participating subcubes. Under what assumptions will
this scheme yield good pivot selections? Is this better than the median scheme described in the text? Analyze the
complexity of selecting the pivot.
(log Q  in time B A  elements, then we can find the median of n  are two sorted sequences, each having B  and A  If Hint:
). n
 In the parallel formulation of the quicksort algorithm on shared-address-space and message-passing architectures 9.22
) each iteration is followed by a barrier synchronization. Is barrier synchronization necessary to ensure Section 9.4.3 (
the correctness of the algorithm? If not, then how does the performance change in the absence of barrier
synchronization?
. Compute the Section 9.4.3  Consider the message-passing formulation of the quicksort algorithm presented in 9.23
) parallel run time and efficiency of the algorithm under the assumption of perfect pivots. ct , and wt , st exact (that is, using
Compute the various components of the isoefficiency function of your formulation when
 = 1 st  = 1, wt  = 1, ct . a
 = 10 st  = 1, wt  = 1, ct . b
 = 100 st  = 10, wt  = 1, ct . c
for cases in which the desired efficiency is 0.50, 0.75, and 0.95. Does the scalability of this formulation depend on the
desired efficiency and the architectural characteristics of the machine?
 Consider the following parallel formulation of quicksort for a mesh-connected message-passing parallel computer. 9.24
Assume that one element is assigned to each process. The recursive partitioning step consists of selecting the pivot
and then rearranging the elements in the mesh so that those smaller than the pivot are in one part of the mesh and
those larger than the pivot are in the other. We assume that the processes in the mesh are numbered in row-major
order. At the end of the quicksort algorithm, elements are sorted with respect to this order. Consider the partitioning
, ..., +1 m P , m P  be the length of this sequence, and let k . Let Figure 9.22(a) step for an arbitrary subsequence illustrated in
 be the mesh processes storing it. Partitioning consists of the following four steps: k + m P
Figure 9.22. (a) An arbitrary portion of a mesh that holds part of the sequence to be sorted at some
point during the execution of quicksort, and (b) a binary tree embedded into the same portion of the
mesh.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processes by k  broadcasts this pivot to all m P . Process m P A pivot is selected at random and sent to process
) transmits the pivot toward the leaves. The m P . The root ( Figure 9.22(b) using an embedded tree, as shown in
tree embedding is also used in the following steps.
. 1
Information is gathered at each process and passed up the tree. In particular, each process counts the
number of elements smaller and larger than the pivot in both its left and right subtrees. Each process knows
the pivot value and therefore can determine if its element is smaller or larger. Each process propagates two
values to its parent: the number of elements smaller than the pivot and the number of elements larger than
the pivot in the process's subtree. Because the tree embedded in the mesh is not complete, some nodes will
 elements are k  knows how many of the m P not have left or right subtrees. At the end of this step, process
 is the number of the elements smaller than the pivot, then the position of s smaller and larger than the pivot. If
. s + m P the pivot in the sorted sequence is
. 2
Information is propagated down the tree to enable each element to be moved to its proper position in the
smaller or larger partitions. Each process in the tree receives from its parent the next empty position in the
smaller and larger partitions. Depending on whether the element stored at each process is smaller or larger
than the pivot, the process propagates the proper information down to its subtrees. Initially, the position for
. +1 s + m P  and the position for elements larger than the pivot is m P elements smaller than the pivot is
. 3
The processes perform a permutation, and each element moves to the proper position in the smaller or larger
partition.
. 4
. Figure 9.23 This algorithm is illustrated in
Figure 9.23. Partitioning a sequence of 13 elements on a 4 x 4 mesh: (a) row-major numbering of the
mesh processes, (b) the elements stored in each process (the shaded element is the pivot), (c) the tree
embedded on a portion of the mesh, (d) the number of smaller or larger elements in the process of the
first column after the execution of the second step, (e) the destination of the smaller or larger
elements propagated down to the processes in the first column during the third step, (f) the
destination of the elements at the end of the third step, and (g) the locations of the elements after
one-to-one personalized communication.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Analyze the complexity of this mesh-based parallel quicksort algorithm. Derive expressions for the parallel runtime,
speedup, and efficiency. Perform this analysis assuming that the elements that were initially assigned at each process
are distributed uniformly.
 Consider the quicksort formulation for a mesh described in Problem 9.24. Describe a scaled-down formulation that 9.25
 processes. Analyze its parallel run time, speedup, and isoefficiency function. n  < p uses
. Show how the algorithm can be implemented Section 9.6.1  Consider the enumeration sort algorithm presented in 9.26
on each of the following:
a CREW PRAM . a
a EREW PRAM . b
a hypercube-connected parallel computer . c
a mesh-connected parallel computer. . d
Analyze the performance of your formulations. Furthermore, show how you can extend this enumeration sort to a
 processes. p  elements using n hypercube to sort
 Derive expressions for the speedup, efficiency, and isoefficiency function of the bucket sort parallel formulation 9.27
. Compare these expressions with the expressions for the other sorting algorithms presented in Section 9.5 presented in
this chapter. Which parallel formulations perform better than bucket sort, and which perform worse?
 guarantees that the number of elements in each of Section 9.5  Show that the splitter selection scheme described in 9.28
. m / n  buckets is less than 2 m the
 Derive expressions for the speedup, efficiency, and isoefficiency function of the sample sort parallel formulation 9.29
 sub-blocks at each p . Derive these metrics under each of the following conditions: (1) the Section 9.5 presented in
. p  sub-blocks at each process can vary by a factor of log p process are of equal size, and (2) the size of the
, which sorts 0 P  - 1 elements to process p , all processes send Section 9.5  In the sample sort algorithm presented in 9.30
p ( p  - 1) elements and distributes splitters to all the processes. Modify the algorithm so that the processes sort the p ( p the
- 1) elements in parallel using bitonic sort. How will you choose the splitters? Compute the parallel run time, speedup,
and efficiency of your formulation.
 that r ? Compute the value of r ) depend on the value of Section 9.6.2  How does the performance of radix sort ( 9.31
minimizes the run time of the algorithm.
n ) are used to sort n  < p  processes ( p  to the case in which Section 9.6.2  Extend the radix sort algorithm presented in 9.32
elements. Derive expressions for the speedup, efficiency, and isoefficiency function for this parallel formulation. Can
you devise a better ranking mechanism?
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 10. Graph Algorithms
Graph theory plays an important role in computer science because it provides an easy and systematic way to model many problems.
Many problems can be expressed in terms of graphs, and can be solved using standard graph algorithms. This chapter presents parallel
formulations of some important and fundamental graph algorithms.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.1 Definitions and Representation
 is an E e . An edge edges  is a finite set of E  and vertices  is a finite set of points called V ), where E , V  is a pair ( G undirected graph An
, V , is a pair ( G directed graph  are connected. Similarly, a v  and u ) indicates that vertices v , u . An edge ( V v , u ), where v , u unordered pair (
 is an ordered pair; that is, it indicates that there is a connection E ) v , u  is the set of vertices as we just defined, but an edge ( V ), where E
 to refer to both directed and undirected graph  illustrates an undirected and a directed graph. We use the term Figure 10.1 . v  to u from
graphs.
Figure 10.1. (a) An undirected graph and (b) a directed graph.
) is v , u Many definitions are common to directed and undirected graphs, although certain terms have slightly different meanings for each. If (
incident from ) is v , u . However, if a graph is directed, then edge ( v  and u  vertices incident on ) is v , u an edge in an undirected graph, (
 isf , edge Figure 10.1(b)  is incident on vertices 5 and 4, but in e , edge Figure 10.1(a) . For example, in v  vertex incident to  and is u vertex
adjacent  are said to be v  and u ), vertices E , V  = ( G ) is an edge in a undirected graph v , u incident from vertex 5 and incident to vertex 2. If (
. u  vertex adjacent to  is said to be v to each other. If the graph is directed, vertex
 - 1. k  = 0, 1, ...,i  for E ) +1i v , i v , and ( u  = k v , v  = 0 v > of vertices where k v , ..., 2 v , 1 v , 0 v  is a sequence < u  to a vertex v  from a vertex path A
. A path is v  from reachable  is u , then u  to v The length of a path is defined as the number of edges in the path. If there exists a path from
. A graph with no k v  = 0 v  if its starting and ending vertices are the same – that is, cycle  if all of its vertices are distinct. A path forms a simple
, the sequence <3, 6, 5, Figure 10.1(a)  if all the intermediate vertices are distinct. For example, in simple . A cycle is acyclic cycles is called
, the Figure 10.1(a)  there is a directed simple cycle <1, 5, 6, 4, 1>. Additionally, in Figure 10.1(b) 4> is a path from vertex 3 to vertex 4, and in
sequence <1, 2, 5, 3, 6, 5, 4, 1> is an undirected cycle that is not simple because it contains the loop <5, 3, 6, 5>.
, V  = ( G  of subgraph ') is a E ', V  = (' G  if every pair of vertices is connected by a path. We say that a graph connected An undirected graph is
v , u| E ) v , u  = {(' E  '), where E  ', V  = (' G  ' is the graph V  by induced G , the subgraph of V ' V . Given a set E ' E  and V ' V  ) if E
V
 is a connected acyclic tree  is an acyclic graph, and a forest  is a graph in which each pair of vertices is adjacent. A complete graph }. A'
| - 1. V | = | E ) is a tree, then | E , V  = ( G graph. Note that if
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Weights are usually real numbers representing the cost or benefit of traversing the E Sometimes weights are associated with each edge in
associated edge. For example, in an electronic circuit a resistor can be represented by an edge whose weight is its resistance. A graph that
 are as we just defined E  and V ), where w , E , V  = ( G  and is denoted by weighted graph has weights associated with each edge is called a
. The weight of a graph is defined as the sum of the weights of its edges. The E  is a real-valued function defined on E  : w and
weight of a path is the sum of the weights of its edges.
There are two standard methods for representing a graph in a computer program. The first method is to use a matrix, and the second
method is to use a linked list.
), which is j , i a  = ( A  array n  x n  of this graph is an adjacency matrix . The n  vertices numbered 1, 2, ..., n ) with E , V  = ( G Consider a graph
defined as follows:
 illustrates an adjacency matrix representation of an undirected graph. Note that the adjacency matrix of an undirected graph is Figure 10.2
) is defined as follows: j, i a  = ( A symmetric. The adjacency matrix representation can be modified to facilitate weighted graphs. In this case,
Figure 10.2. An undirected graph and its adjacency matrix representation.
. The space required to store the adjacency matrix of a weighted adjacency matrix We refer to this modified adjacency matrix as the
). 2 n ( Q  vertices is n graph with
] is a linked list of all v  [ Adj , V v |] of lists. For each V [1..| Adj  ) consists of an array E , V  = ( G  representation of a graph adjacency list The
 shows an example Figure 10.3 . v ] is a list of all vertices adjacent to v  [ Adj . In other words, E ) u , v  contains an edge ( G  such that u vertices
of the adjacency list representation. The adjacency list representation can be modified to accommodate weighted graphs by storing the
|). E (| Q . The space required to store the adjacency list is v  in the adjacency list of vertex E ) u , v weight of each edge (
Figure 10.3. An undirected graph and its adjacency list representation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
); 2| V  (| O | is much smaller than E  if | sparse  ) is E , V  = ( G The nature of the graph determines which representation should be used. A graph
. The adjacency matrix representation is useful for dense graphs, and the adjacency list representation is often more dense otherwise it is
efficient for sparse graphs. Note that the sequential run time of an algorithm using an adjacency matrix and needing to traverse all the
) because the entire array must be accessed. However, if the adjacency list representation 2| V (| W edges of the graph is bounded below by
), the 2| V | is much smaller than | E |) for the same reason. Thus, if the graph is sparse (| E | + | V (| W is used, the run time is bounded below by
adjacency list representation is better than the adjacency matrix representation.
The rest of this chapter presents several graph algorithms. The first four sections present algorithms for dense graphs, and the last section
discusses algorithms for sparse graphs. We assume that dense graphs are represented by an adjacency matrix, and sparse graphs by an
 denotes the number of vertices in the graph. n adjacency list. Throughout this chapter,
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.2 Minimum Spanning Tree: Prim's Algorithm
. In a weighted graph, the weight G  that is a tree containing all the vertices of G  is a subgraph of G  of an undirected graph spanning tree A
 (MST) for a weighted undirected graph minimum spanning tree of a subgraph is the sum of the weights of the edges in the subgraph. A
is a spanning tree with minimum weight. Many problems require finding an MST of an undirected graph. For example, the minimum length
of cable necessary to connect a set of computers in a network can be determined by finding the MST of the undirected graph containing
 shows an MST of an undirected graph. Figure 10.4 all the possible connections.
Figure 10.4. An undirected graph and its minimum spanning tree.
. For simplicity in describing the MST algorithm, we spanning forest  is not connected, it cannot have a spanning tree. Instead, it has a G If
) and apply the MST algorithm on Section 10.6  is not connected, we can find its connected components ( G  is connected. If G assume that
each of them. Alternatively, we can modify the MST algorithm to output a minimum spanning forest.
Prim's algorithm for finding an MST is a greedy algorithm. The algorithm begins by selecting an arbitrary starting vertex. It then grows the
minimum spanning tree by choosing a new vertex and edge that are guaranteed to be in a spanning tree of minimum cost. The algorithm
continues until all the vertices have been selected.
) be its weighted j , i a  = ( A ) be the weighted undirected graph for which the minimum spanning tree is to be found, and let w , E , V  = ( G Let
 to hold the vertices of the minimum spanning T V . The algorithm uses the set Algorithm 10.1 adjacency matrix. Prim's algorithm is shown in
] holds the weight of the edge with the v  [ d  ), T V  - V  ( v ] in which, for each vertex n [1.. d tree during its construction. It also uses an array
] r[ d  that becomes the root of the MST. Furthermore, r  contains an arbitrary vertex T V . Initially, v  to vertex T V least weight from any vertex in
. During each iteration of the algorithm, ] = v[ d ) if such an edge exists; otherwise v , r ( w ] = v[ d  ), T V  - V  ( v  such that v = 0, and for all
) are T V  - V  ( v ] such that v[ d  )}. After this vertex is added, all values of T V  - V  ( v ]| v  [ d ] = min{ u[ d  such that T V  is added to u a new vertex
. The algorithm u updated because there may now be an edge with a smaller weight between vertex v and the newly added vertex
 illustrates the algorithm. Upon termination of Prim's algorithm, the cost of the minimum spanning tree Figure 10.5 . V  = T V terminates when
. Algorithm 10.1 can be easily modified to store the edges that belong in the minimum spanning tree. is
. For each b Figure 10.5. Prim's minimum spanning tree algorithm. The MST is rooted at vertex
] v [ d  as well as the edges selected so far are shown in bold. The array T V iteration, vertices in
 after they have been updated. T V  - V shows the values of the vertices in
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 )} (line 10), T V  - V  ( v ]| v[ d -1 times. Both the computation of min{ n  loop (lines 10–13) is executed while , the body of the Algorithm 10.1 In
). 2 (n Q ) steps. Thus, the overall complexity of Prim's algorithm is n  ( O  loop (lines 12 and 13) execute in for and the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Algorithm 10.1 Prim's sequential minimum spanning tree algorithm.
) r , w , E , V  PRIM_MST( procedure 1.
begin 2.
}; r  := { T V 3.
] := 0; r[ d 4.
do  ) T V  - V  ( v  all for 5.
); v , r ( w ] := v[ d ) exists set v , r  edge ( if 6.
; ] := v[ d  set else 7.
do V T V while 8.
begin 9.
 )}; T V  - V  ( v ]| v[ d ] :=min{ u[ d  such that u 10. find a vertex
}; u  { T V  := T V 11.
do  ) T V  - V  ( v  all for 12.
)}; v , u ( w ], v[ d ] := min{ v[ d 13.
endwhile 14.
 PRIM_MST end 15.
Parallel Formulation
 may v ] for a vertex v[ d Prim's algorithm is iterative. Each iteration adds a new vertex to the minimum spanning tree. Since the value of
 , it is hard to select more than one vertex to include in the minimum spanning tree. For T V  is added in u change every time a new vertex
, after selecting vertex b, if both vertices d and c are selected, the MST will not be found. That is Figure 10.5 example, in the graph of
 loop while ] is updated from 5 to 2. Thus, it is not easy to perform different iterations of the c[ d because, after selecting vertex d, the value of
in parallel. However, each iteration can be performed in parallel as follows.
 subsets using the 1-D p  is partitioned into V  be the number of vertices in the graph. The set n  be the number of processes, and let p Let
 consecutive vertices, and the work associated with each subset is assigned to a p/ n ). Each subset has Section 3.4.1 block mapping (
d  stores the part of the arrayi P  - 1. Each process p  = 0, 1, ...,i  fori P  be the subset of vertices assigned to processi V different process. Let
 computesi P  illustrates the partitioning. Each process Figure 10.6(a) ). i V v ] such that v  [ d  storesi P  (that is, processi V that corresponds to
] by using the u[ i d  loop. The global minimum is then obtained over all while } during each iteration of thei V ) T V  - V  ( v ]| v[ i d ] = min{ u[ i d
, which will be inserted into u  now holds the new vertex 0 P . Process 0 P ) and is stored in process Section 4.1 all-to-one reduction operation (
u  marks u  responsible for vertexi P ). The process Section 4.1  to all processes by using one-to-all broadcast ( u  broadcasts 0 P . Process T V
] for its local vertices. v[ d . Finally, each process updates the values of T V as belonging to set
p  among A  and the adjacency matrix d Figure 10.6. The partitioning of the distance array
processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 must know the v ) must be updated. The process responsible for T V  - V  ( v ] for v[ d , the values of T V  is inserted into u When a new vertex
 ofi V  needs to store the columns of the weighted adjacency matrix corresponding to seti P ). Hence, each process v , u weight of the edge (
). The space to store the required part of the Section 3.4.1 vertices assigned to it. This corresponds to 1-D block mapping of the matrix (
 illustrates the partitioning of the weighted adjacency matrix. Figure 10.6(b) ). p/ 2 n ( Q adjacency matrix at each process is
). The communication p/ n ( Q ] during each iteration is v[ d The computation performed by a process to minimize and update the values of
-process message-passing parallel p performed in each iteration is due to the all-to-one reduction and the one-to-all broadcast. For a
). Finding the global minimum of one word at each Section 4.1  ( p ) log wt  + st computer, a one-to-all broadcast of one word takes time (
). The parallel run time p (log Q ). Thus, the total communication cost of each iteration is Section 4.1 process takes the same amount of time (
of this formulation is given by
), the speedup and efficiency are as follows: 2 n ( Q  = W Since the sequential run time is
1  Equation 10.
 (1). Thus, this formulation of Prim's algorithm can use O  = n )/ p  log p  we see that for a cost-optimal parallel formulation ( Equation 10.1 From
n ). Since p 2  log 2 p ( Q , the isoefficiency function due to communication is Equation 10.1 ) processes. Furthermore, from n /log n  ( O  = p only
). Thus, the overall isoefficiency of 2 (p Q  in this formulation, the isoefficiency function due to concurrency is p must grow at least as fast as
). p 2  log 2 p ( Q this formulation is
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.3 Single-Source Shortest Paths: Dijkstra's Algorithm
 to all V v  problem is to find the shortest paths from a vertex single-source shortest paths ), the w , E , V  = ( G For a weighted graph
 is a minimum-weight path. Depending on the application, edge weights may represent v  to u  from shortest path . A V other vertices in
time, cost, penalty, loss, or any other quantity that accumulates additively along a path and is to be minimized. In the following section,
we present Dijkstra's algorithm, which solves the single-source shortest-paths problem on both directed and undirected graphs with
non-negative weights.
, is similar to Prim's minimum spanning tree algorithm. Like s Dijkstra's algorithm, which finds the shortest paths from a single vertex
. It is also greedy; that is, it always chooses an G  to the other vertices of s Prim's algorithm, it incrementally finds the shortest paths from
 shows Dijkstra's algorithm. Comparing this algorithm with Prim's minimum Algorithm 10.2 edge to a vertex that appears closest.
 ), Dijkstra's T V  - V  ( u spanning tree algorithm, we see that the two are almost identical. The main difference is that, for each vertex
], the cost of u  [ d ; Prim's algorithm stores T V  by means of vertices in s  from vertex u ], the minimum cost to reach vertex u[ l algorithm stores
). 2 n ( Q . The run time of Dijkstra's algorithm is u  to T V the minimum-cost edge connecting a vertex in
Algorithm 10.2 Dijkstra's sequential single-source shortest paths algorithm.
) s , w , E , V  DIJKSTRA_SINGLE_SOURCE_SP( procedure 1.
begin 2.
}; s  := { T V 3.
do  ) T V  - V  ( v  all for 4.
); v , s ( w ] := v[ l ) exists set v , s  ( if 5.
; ] := v[ l  set else 6.
do V T V while 7.
begin 8.
 )}; T V  - V  ( v ]| v[ l ] := min{ u[ l  such that u 9. find a vertex
}; u  { T V  := T V 10.
do  ) T V  - V  ( v  all for 11.
)}; v , u ( w ] + u[ l ], v[ l ] := min{ v[ l 12.
endwhile 13.
 DIJKSTRA_SINGLE_SOURCE_SP end 14.
Parallel Formulation
The parallel formulation of Dijkstra's single-source shortest path algorithm is very similar to the parallel formulation of Prim's algorithm for
). Each Section 3.4.1 ). The weighted adjacency matrix is partitioned using the 1-D block mapping ( Section 10.2 minimum spanning trees (
. During l  values of the array p/ n  consecutive columns of the weighted adjacency matrix, and computes p/ n  processes is assigned p of the
each iteration, all processes perform computation and communication similar to that performed by the parallel formulation of Prim's
algorithm. Consequently, the parallel performance and scalability of Dijkstra's single-source shortest path algorithm is identical to that of
Prim's minimum spanning tree algorithm.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.4 All-Pairs Shortest Paths
 to every other vertex, we are sometimes interested in finding the shortest paths v Instead of finding the shortest paths from a single vertex
 problem is to find the shortest all-pairs shortest paths ), the w , E , V ( G between all pairs of vertices. Formally, given a weighted graph
 vertices, the output of an all-pairs shortest paths n . For a graph with j i  such that V j , v i paths between all pairs of vertices v
. j  to vertex vi  is the cost of the shortest path from vertex v j, i d ) such that j, i d  = ( D  matrix n  x n algorithm is an
The following sections present two algorithms to solve the all-pairs shortest paths problem. The first algorithm uses Dijkstra's
single-source shortest paths algorithm, and the second uses Floyd's algorithm. Dijkstra's algorithm requires non-negative edge weights
(Problem 10.4), whereas Floyd's algorithm works with graphs having negative-weight edges provided they contain no negative-weight
cycles.
10.4.1 Dijkstra's Algorithm
 to all the other vertices in a graph. This v  we presented Dijkstra's algorithm for finding the shortest paths from a vertex Section 10.3 In
algorithm can also be used to solve the all-pairs shortest paths problem by executing the single-source algorithm on each process, for
. We refer to this algorithm as Dijkstra's all-pairs shortest paths algorithm. Since the complexity of Dijkstra's single-source v each vertex
). 3 n ( Q ), the complexity of the all-pairs algorithm is 2 n ( Q algorithm is
Parallel Formulations
Dijkstra's all-pairs shortest paths problem can be parallelized in two distinct ways. One approach partitions the vertices among different
processes and has each process compute the single-source shortest paths for all vertices assigned to it. We refer to this approach as the
. Another approach assigns each vertex to a set of processes and uses the parallel formulation of the source-partitioned formulation
source-parallel ) to solve the problem on each set of processes. We refer to this approach as the Section 10.3 single-source algorithm (
. The following sections discuss and analyze these two approaches. formulation
i P  processes. Each process n  The source-partitioned parallel formulation of Dijkstra's algorithm uses Source-Partitioned Formulation
 to all other vertices by executing Dijkstra's sequential single-source shortest paths algorithm. Iti finds the shortest paths from vertex v
requires no interprocess communication (provided that the adjacency matrix is replicated at all processes). Thus, the parallel run time of
this formulation is given by
), the speedup and efficiency are as follows: 3 n ( Q  = W Since the sequential run time is
2  Equation 10.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
It might seem that, due to the absence of communication, this is an excellent parallel formulation. However, that is not entirely true. The
), which is the overall isoefficiency 3 p ( Q  processes. Therefore, the isoefficiency function due to concurrency is n algorithm can use at most
)), then this algorithm has p ( Q  = n function of the algorithm. If the number of processes available for solving the problem is small (that is,
, other algorithms will eventually outperform this algorithm n good performance. However, if the number of processes is greater than
because of its poor scalability.
 processes busy n  The major problem with the source-partitioned parallel formulation is that it can keep only Source-Parallel Formulation
) is used to Section 10.3 doing useful work. Performance can be improved if the parallel formulation of Dijkstra's single-source algorithm (
. The source-parallel formulation is similar to the source-partitioned formulation, except that the v solve the problem for each vertex
single-source algorithm runs on disjoint subsets of processes.
n ). Each of the n  > p  processes (this formulation is of interest only if n / p  partitions, each with n  processes are divided into p Specifically,
 partitions. In other words, we first parallelize the all-pairs shortest paths n single-source shortest paths problems is solved by one of the
n / p problem by assigning each vertex to a separate set of processes, and then parallelize the single-source algorithm by using the set of
). 2 n  ( O processes to solve it. The total number of processes that can be used efficiently by this formulation is
 can be used to derive the performance of this formulation of Dijkstra's all-pairs algorithm. Assume Section 10.3 The analysis presented in
n / p  groups of size n  processes are partitioned into p . The n  is a multiple of p -process message-passing computer such that p that we have a
 process group, the parallel run time is n / p each. If the single-source algorithm is executed on each
3  Equation 10.
 processes forms a n / p . These similarities are not surprising because each set of 10.2  and 10.3 Notice the similarities between Equations
 processes to solve the n / p different group and carries out the computation independently. Thus, the time required by each set of
), the speedup and efficiency are as 3 n ( Q  = W single-source problem determines the overall run time. Since the sequential run time is
follows:
4  Equation 10.
) n /log 2 n  ( O  (1). Hence, this formulation can use up to O  = 2 n / p  log p  we see that for a cost-optimal formulation Equation 10.4 From
). The isoefficiency 1.5 ) p  log p (( Q  also shows that the isoefficiency function due to communication is Equation 10.4 processes efficiently.
). 1.5 ) p  log p (( Q ). Thus, the overall isoefficiency function is 1.5 p ( Q function due to concurrency is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Comparing the two parallel formulations of Dijkstra's all-pairs algorithm, we see that the source-partitioned formulation performs no
). In contrast, the source-parallel formulation uses 2 n ( Q  processes, and solves the problem in time n communication, can use no more than
 processes are used. n /log 2 n ) when n  log n ( Q  processes, has some communication overhead, and solves the problem in time n /log 2 n up to
Thus, the source-parallel formulation exploits more parallelism than does the source-partitioned formulation.
10.4.2 Floyd's Algorithm
) be the weighted w , E , V  = ( G Floyd's algorithm for solving the all-pairs shortest paths problem is based on the following observation. Let
. For any pair of n k  where k } of vertices for some k v ,..., 2 v , 1 v . Consider a subset { G } be the vertices of n v ,..., 2 v, 1 v  = { V graph, and let
 be the  }. Let vk ,..., 2 v , 1 v  whose intermediate vertices belong to the set { j  to vi , consider all paths from v V j v  ,i v vertices
 is , then j  to vi  is not in the shortest path from v k v . If vertex  be the weight of minimum-weight path among them, and let
. Each j v  to k v  and one from k v  toi v  into two paths – one from , then we can break  is in k . However, if v the same as
. These observations are expressed in the }. Thus, -1 k v ,..., 2 v , 1 v of these paths uses vertices from {
following recurrence equation:
5  Equation 10.
. . In general, the solution is a matrix  is given by j v  toi v The length of the shortest path from
 shows Floyd's all-pairs algorithm. Algorithm 10.3 . k  bottom-up in the order of increasing values of Equation 10.5 Floyd's algorithm solves
(1); thus, the Q  loops in lines 4–7. Each execution of line 7 takes time for The run time of Floyd's algorithm is determined by the triple-nested
. However, when computing n  x n  matrices of size n  seems to imply that we must store Algorithm 10.3 ). 3 n ( Q complexity of the algorithm is
 matrices must be stored. Therefore, the overall space n  x n  is needed. Consequently, at most two -1) k ( D , only matrix ) k ( D matrix
 is used (Problem 10.6). D ). Furthermore, the algorithm works correctly even when only one copy of 2 n ( Q complexity is
Algorithm 10.3 Floyd's all-pairs shortest paths algorithm. This program computes the all-pairs
. A  ) with adjacency matrix E , V  = ( G shortest paths of the graph
) A  FLOYD_ALL_PAIRS_SP( procedure 1.
begin 2.
; A  = (0) D 3.
do n to  := 1 k for 4.
do n to  := 1i for 5.
do n to  := 1 j for 6.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
; 7.
 FLOYD_ALL_PAIRS_SP end 8.
Parallel Formulation
p  to a set of processes. Let k  for each value of ) k ( D A generic parallel formulation of Floyd's algorithm assigns the task of computing matrix
 parts, and each part is assigned to a process. Each process p  is partitioned into ) k ( D be the number of processes available. Matrix
 row and th k  values of its partition. To accomplish this, a process must access the corresponding segments of the ) k ( D computes the
. Another technique is considered in ) k ( D . The following section describes one technique for partitioning matrix -1) k ( D column of matrix
Problem 10.8.
 is divided ) k ( D ). Specifically, matrix Section 3.4.1  is to use the 2-D block mapping ( ) k ( D  One way to partition matrix 2-D Block Mapping
p  processes. It is helpful to think of the p , and each block is assigned to one of the  blocks of size p into
. Note that this is only a conceptual layout and does not necessarily processes as arranged in a logical grid of size
 is assigned a subblock j, i P . Process j, i P  column as th j  row and thi reflect the actual interconnection network. We refer to the process on the
 and whose lower-right corner is  whose upper-left corner is ) k ( D of
 illustrates the 2-D block Figure 10.7(a)  Each process updates its part of the matrix during each iteration..
mapping technique.
 subblocks, and  distributed by 2-D block mapping into ) k ( D Figure 10.7. (a) Matrix
. j,i P  assigned to process ) k ( D (b) the subblock of
 matrix. For -1) k ( D  column of the th k  row and th k  needs certain segments of the j, i P  iteration of the algorithm, each process th k During the
 resides on a process along the  illustrates, Figure 10.8 . As  and  it must get example, to compute
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
th k . Segments are transferred as follows. During the j, i P  resides on a process along the same column as same row, and element
 processes in the same  row sends it to the th k  processes containing part of the iteration of the algorithm, each of the
 processes in the same row.  column sends it to the th k  processes containing part of the column. Similarly, each of the
, Figure 10.8. (a) Communication patterns used in the 2-D block mapping. When computing
information must be sent to the highlighted process from two other processes along the same
 row and column th k  processes that contain the row and column. (b) The row and column of
send them along process columns and rows.
 shows the parallel formulation of Floyd's algorithm using the 2-D block mapping. We analyze the performance of this Algorithm 10.4
). During each iteration of the algorithm, the p ( Q -process message-passing computer with a cross-bisection bandwidth of p algorithm on a
k
th
 processes. Each such process has  column of processes perform a one-to-all broadcast along a row or a column of th k  row and
Q  elements. This broadcast requires time  row or column, so it sends th k  elements of the
 elements of p/ 2 n ). Since each process is assigned p (log Q . The synchronization step on line 7 requires time
). Therefore, the parallel run time of the 2-D block mapping p/ 2 n ( Q  values is ) k ( D  matrix, the time to compute corresponding ) k ( D the
formulation of Floyd's algorithm is
), the speedup and efficiency are as follows: 3 n ( Q  = W Since the sequential run time is
6  Equation 10.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
; thus, 2-D block mapping can  we see that for a cost-optimal formulation Equation 10.6 From
 can also be used to derive the isoefficiency function due to communication, Equation 10.6 ) processes. n 2 /log 2 n ( O efficiently use up to
). p 3  log 1.5 p ( Q ). Thus, the overall isoefficiency function is 5. 1 p ( Q ). The isoefficiency function due to concurrency is p 3  log 1.5 p ( Q which is
e  In the 2-D block mapping formulation of Floyd's algorithm, a synchronization step ensures that all processes hav Speeding Things Up
th k ). In other words, the Algorithm 10.4  (line 7 in ) k ( D  before computing elements of matrix -1) k ( D the appropriate segments of matrix
 have been transmitted to all -1) k ( D  iteration has completed and the relevant parts of matrix th  - 1) k iteration starts only when the (
processes. The synchronization step can be removed without affecting the correctness of the algorithm. To accomplish this, a process
 matrix. This -1) k ( D  iteration and has the relevant parts of the th  -1) k  iteration as soon as it has computed the ( th k starts working on the
 to improve the performance of Gaussian Section 8.3 . A similar technique is used in pipelined 2-D block mapping formulation is called
elimination.
 denotes all the j *, P Algorithm 10.4 Floyd's parallel formulation using the 2-D block mapping.
 is (0) D  row. The matrix thi  denotes all the processes in the ,*i P  column, and th j processes in the
the adjacency matrix.
) (0) D  FLOYD_2DBLOCK( procedure 1.
begin 2.
do n to  := 1 k for 3.
begin 4.
; -1) k ( D  row of th k  that has a segment of the j, i P 5. each process
 processes; j *, P  broadcasts it to the
; -1) k ( D  column of th k  that has a segment of the j, i P 6. each process
 processes; ,* i P  broadcasts it to the
7. each process waits to receive the needed segments;
 matrix; ) k ( D  computes its part of the j, i P 8. each process
end 9.
 FLOYD_2DBLOCK end 10.
 iteration as soon th k  starts working on the j, i P -process system arranged in a two-dimensional topology. Assume that process p Consider a
th k  has elements of the j, i P  matrix. When process -1) k ( D  iteration and has received the relevant parts of the th  - 1) k as it has finished the (
. It does this +1 j , i P  and -1 j, i P  stored locally to processes -1) k ( D  iteration, it sends the part of matrix th  - 1) k row and has finished the (
 column and th k  has elements of the j, i P  matrix. Similarly, when process ) k ( D  matrix is used to compute the -1) k ( D because that part of the
j, i P . When process j +1, i P  and j -1, i P  stored locally to processes -1) k ( D  iteration, it sends the part of matrix th  - 1) k has finished the (
 from a process along its row in the logical mesh, it stores them locally and forwards them to the process ) k ( D receives elements of matrix
 are not ) k ( D on the side opposite from where it received them. The columns follow a similar communication protocol. Elements of matrix
 illustrates this communication and termination protocol for processes within a Figure 10.9 forwarded when they reach a mesh boundary.
row (or a column).
Figure 10.9. Communication protocol followed in the pipelined 2-D block mapping formulation
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
th k  has just computed a segment of thet of Floyd's algorithm. Assume that process 4 at time
 matrix. It sends the segment to processes 3 and 5. These processes receive -1) k ( D column of the
 + 1 (where the time unit is the time it takes for a matrix segment to travelt the segment at time
over the communication link between adjacent processes). Similarly, processes farther away
from process 4 receive the segment later. Process 1 (at the boundary) does not forward the
segment after receiving it.
i P  to j, i P  elements of the first row are sent from process Consider the movement of values in the first iteration. In each step,
Q  After. ) ( Q . Each such step takes time +1 j, i P  to process j, i P . Similarly, elements of the first column are sent from process j +1,
). The values of successive n ( Q  gets the relevant elements of the first row and first column in time  steps, process
 finishes its share of the shortest path ) in a pipelined mode. Hence, process /p 2 n ( Q rows and columns follow after time
th n  iteration, it sends the relevant values of the th  - 1) n  has finished the (  When process. ) n ( Q ) + p/ 3 n ( Q computation in time
). The overall parallel run time of this formulation is n ( Q  in time 1,1 P row and column to the other processes. These values reach process
), the speedup and efficiency are as follows: 3 n ( Q  = W Since the sequential run time is
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
7  Equation 10.
Table 10.1. The performance and scalability of the all-pairs shortest paths algorithms on various
 cube d  - k ) bisection bandwidth. Similar run times apply to all p  ( O architectures with
architectures, provided that processes are properly mapped to the underlying processors.
 = E Maximum Number of Processes for
(1) Q
Corresponding Parallel Run
Time
Isoefficiency
Function
Dijkstra
source-partitioned
) n ( Q
) 3 p ( Q ) 2 n ( Q
Dijkstra source-parallel
) 1.5 ) p  log p (( Q ) n  log n ( Q ) n /log 2 n ( Q
) n /log n ( Q Floyd 1-D block
) 3 ) p  log p (( Q ) n  log 2 n ( Q
Floyd 2-D block
) p 3  log 1.5 p ( Q ) n 2  log n ( Q ) n 2 /log 2 n ( Q
Floyd pipelined 2-D block
) 1.5 p ( Q ) n ( Q ) 2 n ( Q
O  (1). Thus, the pipelined formulation of Floyd's algorithm uses up to O  = 2 n / p  we see that for a cost-optimal formulation Equation 10.7 From
). This is 1.5 p ( Q , we can derive the isoefficiency function due to communication, which is Equation 10.7 ) processes efficiently. Also from 2 n (
the overall isoefficiency function as well. Comparing the pipelined formulation to the synchronized 2-D block mapping formulation, we see
that the former is significantly faster.
10.4.3 Performance Comparisons
 for a parallel architecture with Table 10.1 The performance of the all-pairs shortest paths algorithms previously presented is summarized in
) processes to solve the problem in 2 n ( Q ) bisection bandwidth. Floyd's pipelined formulation is the most scalable and can use up to p  ( O
, such as a O ). Moreover, this parallel formulation performs equally well even on architectures with bisection bandwidth n ( Q time
mesh-connected computer. Furthermore, its performance is independent of the type of routing (store-and-forward or cut-through).
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.5 Transitive Closure
In many applications we wish to determine if any two vertices in a graph are connected. This is usually done by finding the transitive
, i v  = {( * E ), where * E , V  = ( * G  is defined as the graph G  of transitive closure ) is a graph, then the E , V  = ( G closure of a graph. Formally, if
. The * A }. We compute the transitive closure of a graph by computing the connectivity matrix G  in j v  toi v )| there is a path from j v
, and j  =i  or j v  toi v  if there is a path from  such that  is a matrix G  of connectivity matrix
 otherwise.
 and use any of the all-pairs shortest paths algorithms on this weighted graph. E  we assign a weight of 1 to each edge of * A To compute
 is the solution to the all-pairs shortest paths problem, as follows: D , where D  can be obtained from matrix * A Matrix
 and + operations in line 7 of min , replacing the G  is to use Floyd's algorithm on the adjacency matrix of * A Another method for computing
 = 0 otherwise. i,j a , and E ) j v , i v  or ( j  =i  = 1 if j, i a  operations. In this case, we initially set and  and logical or  by logical Algorithm 10.3
 otherwise. The complexity of computing the transitive  = 0 and i,j d  if  is obtained by setting * A Matrix
). 3 n ( Q closure is
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.6 Connected Components
 ... 2 C 1 C  = V  such that k C , ..., 2 C , 1 C ) are the maximal disjoint sets E , V  = ( G  of an undirected graph connected components The
. The connected components of an undirected graph are u  is reachable from v and v is reachable from u  if and only ifi C v , u  , and k C
 shows a graph with three connected Figure 10.10 the equivalence classes of vertices under the "is reachable from" relation. For example,
components.
Figure 10.10. A graph with three connected components: {1, 2, 3, 4}, {5, 6, 7}, and {8, 9}.
10.6.1 A Depth-First Search Based Algorithm
We can find the connected components of a graph by performing a depth-first traversal on the graph. The outcome of this depth-first
Figure traversal is a forest of depth-first trees. Each tree in the forest contains vertices that belong to a different connected component.
 illustrates this algorithm. The correctness of this algorithm follows directly from the definition of a spanning tree (that is, a depth-first 10.11
 is undirected. G tree is also a spanning tree of a graph induced by the set of vertices in the depth-first tree) and from the fact that
|) because the depth-first traversal E (| Q Assuming that the graph is stored using a sparse representation, the run time of this algorithm is
. algorithm traverses all the edges in G
Figure 10.11. Part (b) is a depth-first forest obtained from depth-first traversal of the graph in
part (a). Each of these trees is a connected component of the graph in part (a).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Parallel Formulation
 parts and assigning each part to p  into G The connected-component algorithm can be parallelized by partitioning the adjacency matrix of
 are the edges that correspond to the portion of thei E ) and i E , V  = (i G , where G  ofi G  has a subgraphi P  processes. Each process p one of
 computes the depth-first spanningi P adjacency matrix assigned to this process. In the first step of this parallel formulation, each process
 spanning forests have been constructed. During the second step, spanning forests are p . At the end of this step, i G forest of the graph
merged pairwise until only one spanning forest remains. The remaining spanning forest has the property that two vertices are in the same
 illustrates this algorithm. Figure 10.12  if they are in the same tree. G connected component of
Figure 10.12. Computing connected components in parallel. The adjacency matrix of the graph
 as G  in (a) is partitioned into two parts as shown in (b). Next, each process gets a subgraph of G
shown in (c) and (e). Each process then computes the spanning forest of the subgraph, as
shown in (d) and (f). Finally, the two spanning trees are merged to form the solution.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
To merge pairs of spanning forests efficiently, the algorithm uses disjoint sets of edges. Assume that each tree in the spanning forest of a
 is represented by a set. The sets for different trees are pairwise disjoint. The following operations are defined on the disjoint G subgraph of
sets:
. Each set has its own unique x  returns a pointer to the representative element of the set containing ) find(x
representative.
. The two sets are assumed to be disjoint prior to the y  and x  unites the sets containing the elements ) y , union(x
operation.
 are B  and A  - 1 edges (since n  be the two spanning forests to be merged. At most B  and A The spanning forests are merged as follows. Let
, a find A ) of v , u . For each edge ( B  into forest A forests) of one are merged with the edges of the other. Suppose we want to merge forest
 . If not, then the two trees (sets) of B operation is performed for each vertex to determine if the two vertices are already in the same tree of
 requires at most B  and A  are united by a union operation. Otherwise, no union operation is necessary. Hence, merging v  and u  containing B
 - 1) union operations. We can implement the disjoint-set data structure by using disjoint-set forests with n  - 1) find operations and ( n 2(
). A detailed n  ( O  - 1) unions is n  - 1) finds and ( n ranking and path compression. Using this implementation, the cost of performing 2(
) for references. Section 10.8 description of the disjoint-set forest is beyond the scope of this book. Refer to the bibliographic remarks (
 and G Having discussed how to efficiently merge two spanning forests, we now concentrate on how to partition the adjacency matrix of
 processes. The next section discusses a formulation that uses 1-D block mapping. An alternative partitioning scheme p distribute it among
is discussed in Problem 10.12.
 consecutive p/ n ). Each stripe is composed of Section 3.4.1  stripes ( p  adjacency matrix is partitioned into n  x n  The 1-D Block Mapping
 processes. To compute the connected components, each process first computes a spanning forest p rows and is assigned to one of the
 rows of the adjacency matrix assigned to it. p/ n -vertex graph represented by the n for the
 adjacency matrix assigned to n ) x p/ n -process message-passing computer. Computing the spanning forest based on the ( p Consider a
y ). The second step of the algorithm–the pairwise merging of spanning forests – is performed b p/ 2 n ( Q each process requires time
n ( Q ). Thus, the cost due to merging is n ( Q  merging stages, and each takes time p embedding a virtual tree on the processes. There are log
) edges of the spanning n ( Q ). Finally, during each merging stage, spanning forests are sent between nearest neighbors. Recall that p log
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). The parallel run time of the connected-component algorithm is p  log n ( Q forest are transmitted. Thus, the communication cost is
), the speedup and efficiency are as follows: 2 n ( Q  = W Since the sequential complexity is
8  Equation 10.
, we derive the isoefficiency function, Equation 10.8 ). Also from n /log n  ( O  = p  we see that for a cost-optimal formulation Equation 10.8 From
). This is the isoefficiency function due to communication and due to the extra computations performed in the merging p 2  log 2 p ( Q which is
). The performance of this p 2  log 2 p ( Q ); thus, the overall isoefficiency function is 2 p ( Q stage. The isoefficiency function due to concurrency is
parallel formulation is similar to that of Prim's minimum spanning tree algorithm and Dijkstra's single-source shortest paths algorithm on a
message-passing computer.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.7 Algorithms for Sparse Graphs
The parallel algorithms in the previous sections are based on the best-known algorithms for dense-graph problems. However, we have yet
Figure 10.13 . 2| V | is much smaller than | E ) is sparse if | E , V  = ( G to address parallel algorithms for sparse graphs. Recall that a graph
shows some examples of sparse graphs.
Figure 10.13. Examples of sparse graphs: (a) a linear graph, in which each vertex has two
incident edges; (b) a grid graph, in which each vertex has four incident vertices; and (c) a
random sparse graph.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Any dense-graph algorithm works correctly on sparse graphs as well. However, if the sparseness of the graph is taken into account, it is
Section usually possible to obtain significantly better performance. For example, the run time of Prim's minimum spanning tree algorithm (
) regardless of the number of edges in the graph. By modifying Prim's algorithm to use adjacency lists and a binary heap, the 2 n ( Q ) is 10.2
). n /log 2 n  ( O |= E ). This modified algorithm outperforms the original algorithm as long as | n | log E (| Q complexity of the algorithm reduces to
An important step in developing sparse-graph algorithms is to use an adjacency list instead of an adjacency matrix. This change in
), and is independent of the number of 2 n ( W representation is crucial, since the complexity of adjacency-matrix-based algorithms is usually
), which depends on the sparseness of the graph. E|  | n + ( W edges. Conversely, the complexity of adjacency-list-based algorithms is usually
In the parallel formulations of sequential algorithms for dense graphs, we obtained good performance by partitioning the adjacency matrix
of a graph so that each process performed roughly an equal amount of work and communication was localized. We were able to achieve
this largely because the graph was dense. For example, consider Floyd's all-pairs shortest paths algorithm. By assigning equal-sized
blocks from the adjacency matrix to all processes, the work was uniformly distributed. Moreover, since each block consisted of
consecutive rows and columns, the communication overhead was limited.
However, it is difficult to achieve even work distribution and low communication overhead for sparse graphs. Consider the problem of
partitioning the adjacency list of a graph. One possible partition assigns an equal number of vertices and their adjacency lists to each
process. However, the number of edges incident on a given vertex may vary. Hence, some processes may be assigned a large number of
edges while others receive very few, leading to a significant work imbalance among the processes. Alternately, we can assign an equal
number of edges to each process. This may require splitting the adjacency list of a vertex among processes. As a result, the time spent
communicating information among processes that store separate parts of the adjacency list may increase dramatically. Thus, it is hard to
derive efficient parallel formulations for general sparse graphs (Problems 10.14 and 10.15). However, we can often derive efficient parallel
. The graph Figure 10.14 formulations if the sparse graph has a certain structure. For example, consider the street map shown in
. grid graphs corresponding to the map is sparse: the number of edges incident on any vertex is at most four. We refer to such graphs as
Other types of sparse graphs for which an efficient parallel formulation can be developed are those corresponding to well-shaped finite
element meshes, and graphs whose vertices have similar degrees. The next two sections present efficient algorithms for finding a maximal
independent set of vertices, and for computing single-source shortest paths for these types of graphs.
Figure 10.14. A street map (a) can be represented by a graph (b). In the graph shown in (b), each
street intersection is a vertex and each edge is a street segment. The vertices of (b) are the
intersections of (a) marked by dots.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
10.7.1 Finding a Maximal Independent Set
, V  = ( G Consider the problem of finding a maximal independent set (MIS) of vertices of a graph. We are given a sparse undirected graph
. An independent set is called G  is connected via an edge inI  if no pair of vertices in independent  is called V I ). A set of vertices E
. Figure 10.15 , the independence property is violated. These definitions are illustrated inI  if by including any other vertex not in maximal
Note that as the example illustrates, maximal independent sets are not unique. Maximal independent sets of vertices can be used to
determine which computations can be done in parallel in certain types of task graphs. For example, maximal independent sets can be
used to determine the sets of rows that can be factored concurrently in parallel incomplete factorization algorithms, and to compute a
coloring of a graph in parallel.
Figure 10.15. Examples of independent and maximal independent sets.
Many algorithms have been proposed for computing a maximal independent set of vertices. The simplest class of algorithms starts by
 . Then theI  set of vertices for inclusion in candidate  that acts as the C  to be empty, and assigning all vertices to a setI initially setting
. This process terminates C  from v  and removing all vertices adjacent toI  into C  from v algorithm proceeds by repeatedly moving a vertex
 will contain an independent set of vertices,I  is a maximal independent set. The resulting setI  becomes empty, in which case C when
 all the vertices whose subsequent inclusion will violate the independence C  we remove fromI because every time we add a vertex into
. I  is adjacent to at least one of the vertices inI condition. Also, the resulting set is maximal, because any other vertex that is not already in
Even though the above algorithm is very simple, it is not well suited for parallel processing, as it is serial in nature. For this reason parallel
MIS algorithms are usually based on the randomized algorithm originally developed by Luby for computing a coloring of a graph. Using
 is initially setI  a graph is computed in an incremental fashion as follows. The set V  of verticesI Luby's algorithm, a maximal independent set
, and if a C . A unique random number is assigned to each vertex in V , is set to be equal to C to be empty, and the set of candidate vertices,
 is C . The set I vertex has a random number that is smaller than all of the random numbers of the adjacent vertices, it is included in
 and their adjacent vertices are removed from it. Note that the verticesI updated so that all the vertices that were selected for inclusion in
,I  was inserted in v  are indeed independent (i.e., not directly connected via an edge). This is because, ifI that are selected for inclusion in
u  is the smallest among the random numbers assigned to its adjacent vertices; thus, no other vertex v then the random number assigned to
 will have been selected for inclusion. Now the above steps of random number assignment and vertex selection are repeated v adjacent to
 becomes empty. On the average, C  ends whenI  is augmented similarly. This incremental augmentation ofI , and C for the vertices left in
Figure |) such augmentation steps. The execution of the algorithm for a small graph is illustrated in V  (log | O this algorithm converges after
. In the rest of this section we describe a shared-address-space parallel formulation of Luby's algorithm. A message-passing 10.16
adaption of this algorithm is described in the message-passing chapter.
Figure 10.16. The different augmentation steps of Luby's randomized maximal independent set
algorithm. The numbers inside each vertex correspond to the random number assigned to the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
vertex.
Shared-Address-Space Parallel Formulation
|. At V  be an array of size |I A parallel formulation of Luby's MIS algorithm for a shared-address-space parallel computer is as follows. Let
 are set toI  is part of the MIS, or zero otherwise. Initially, all the elements ini v ] will store one, if vertexi  [I the termination of the algorithm,
|. V  be an array of size | C zero, and during each iteration of Luby's algorithm, some of the entries of that array will be changed to one. Let
 are C  is part of the candidate set, or zero otherwise. Initially, all the elements ini v ] is one if vertexi  [ C During the course of the algorithm,
| that stores the random numbers assigned to each vertex. V  be an array of size | R set to one. Finally, let
 processes. Each process generates a random number for its assigned p  is logically partitioned among the C During each iteration, the set
. When all the processes finish generating these random numbers, they proceed to determine which vertices can be C vertices from
. In particular, for each vertex assigned to them, they check to see if the random number assigned to it is smaller than the I included in
 is shared and R  to one. BecauseI random numbers assigned to all of its adjacent vertices. If it is true, they set the corresponding entry in
 is quite straightforward.I can be accessed by all the processes, determining whether or not a particular vertex can be included in
 will be v  can also be updated in a straightforward fashion as follows. Each process, as soon as it determines that a particular vertex C Array
 corresponding to its adjacent vertices. Note that even though more than one process may be C , will set to zero the entries ofI part of
), such concurrent writes willI  (because it may be adjacent to more than one vertex that was inserted in C setting to zero the same entry of
not affect the correctness of the results, because the value that gets concurrently written is the same.
The complexity of each iteration of Luby's algorithm is similar to that of the serial algorithm, with the extra cost of the global
synchronization after each random number assignment. The detailed analysis of Luby's algorithm is left as an exercise (Problem 10.16).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
10.7.2 Single-Source Shortest Paths
It is easy to modify Dijkstra's single-source shortest paths algorithm so that it finds the shortest paths for sparse graphs efficiently. The
modified algorithm is known as Johnson's algorithm. Recall that Dijkstra's algorithm performs the following two steps in each iteration.
), it T V  - V  ( v . Second, for each vertex T V )} and inserts it into set T V  - V  ( v ]| v[ l ] = min{ u[ l ) such that T V  - V  ( u First, it extracts a vertex
 need to be u )}. Note that, during the second step, only the vertices in the adjacency list of vertex v , u ( w ] + u[ l ], v[ l ] = min{ v[ l computes
); thus, using the n ( Q  is considerably smaller than u considered. Since the graph is sparse, the number of vertices adjacent to vertex
adjacency-list representation improves performance.
). The priority queue is constructed so that the T V  - V  ( v ] for each vertex v[ l  to store the value Q Johnson's algorithm uses a priority queue
 is always at the front of the queue. A common way to implement a priority queue is as a binary min-heap.l vertex with the smallest value in
 shows Johnson's algorithm. Initially, Algorithm 10.5 ). n  (log O  in time v ] for each vertex v[ l A binary min-heap allows us to update the value
] = 0. At each step of s[ l  it inserts s  in the priority queue. For the source vertex ] = v[ l  other than the source, it inserts v for each vertex
 is traversed, u  is removed from the priority queue. The adjacency list forl ) with the minimum value in T V  - V  ( u the algorithm, the vertex
 is updated in the heap. Updating vertices in the heap dominates the overall run time of v ] to vertex v[ l ) the distance v , u and for each edge (
| log E (| Q the algorithm. The total number of updates is equal to the number of edges; thus, the overall complexity of Johnson's algorithm is
). n
Algorithm 10.5 Johnson's sequential single-source shortest paths algorithm.
) s , E , V  JOHNSON_SINGLE_SOURCE_SP( procedure 1.
begin 2.
 ; V  := Q 3.
do Q v  all for 4.
; ] := v[ l 5.
] := 0; s[ l 6.
do  0 Q while 7.
begin 8.
); Q ( extract min  := u 9.
do ] u  [ Adj v  each for 10.
then ] v[ l ) < v , u ] + w( u[ l and Q v if 11.
); v , u ( w ] + u[ l ] := v[ l 12.
endwhile 13.
 JOHNSON_SINGLE_SOURCE_SP end 14.
Parallelization Strategy
 efficiently. A simple strategy is for a single Q An efficient parallel formulation of Johnson's algorithm must maintain the priority queue
 to 0 P ), and give them to T V  - V  ( v ] for v[ l . All other processes will then compute new values of Q , to maintain 0 P process, for example,
update the priority queue. There are two main limitation of this scheme. First, because a single process is responsible for maintaining the
)). This leads n  (log O |) queue updates and each update takes time E  (| O ) (there are n | log E  (| O priority queue, the overall parallel run time is
) is the same as the sequential run time. Second, during each n | log E  (| O to a parallel formulation with no asymptotic speedup, since
| processes can be kept busy at any given time, V |/| E | vertices. As a result, no more than | V |/| E iteration, the algorithm updates roughly |
which is very small for most of the interesting classes of sparse graphs, and to a large extent, independent of the size of the graphs.
The first limitation can be alleviated by distributing the maintainance of the priority queue to multiple processes. This is a non-trivial task,
and can only be done effectively on architectures with low latency, such as shared-address-space computers. However, even in the best
), which is quite small. n  (log O  (1), the maximum speedup that can be achieved is O case, when each priority queue update takes only time
 value of the vertices at the top of the priority queue,l The second limitation can be alleviated by recognizing the fact that depending on the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 such u  is the vertex at the top of the priority queue, all vertices v more than one vertex can be extracted at the same time. In particular, if
] can also be extracted, and their adjacency lists processed concurrently. This is because the vertices that are at the same v[ l ] = u[ l that
minimum distance from the source can be processed in any order. Note that in order for this approach to work, all the vertices that are at
the same minimum distance must be processed in lock-step. An additional degree of concurrency can be extracted if we know that the
 can be processed concurrently m ] + v[ l ] u[ l  such that u . In that case, all vertices m minimum weight over all the edges in the graph is
 vertices. However, this additional concurrency can lead to asymptotically better safe (and in lock-step). We will refer to these as the
) only if more than one update operation of the priority queue can proceed concurrently, substantially complicating n  (log O speedup than
the parallel algorithm for maintaining the single priority queue.
Our discussion thus far was focused on developing a parallel formulation of Johnson's algorithm that finds the shortest paths to the
vertices in the same order as the serial algorithm, and explores concurrently only safe vertices. However, as we have seen, such an
approach leads to complicated algorithms and limited concurrency. An alternate approach is to develop a parallel algorithm that processes
 vertices concurrently, as long as these unsafe vertices can be reached from the source via a path involving vertices unsafe both safe and
-value in the priority queue is not infinite). In particular, in thisl whose shortest paths have already been computed (i.e., their corresponding
 values of the vertices adjacent to it.l  top vertices and proceeds to update the p  processes extracts one of the p algorithm, each one of the
 values of the vertices extracted from the priority queuel Of course, the problem with this approach is that it does not ensure that the
] < v[ l  that are at the top of the priority queue, with u  and v correspond to the cost of the shortest path. For example, consider two vertices
 value is the cost of the shortest pathl ]. According to Johnson's algorithm, at the point a vertex is extracted from the priority queue, its u[ l
], then the correct value of the u[ l ) < u , v ( w ] + v[ l , such that u  and v from the source to that vertex. Now, if there is an edge connecting
]. However, the correctness of the results can be ensured by detecting when we have u[ l ), and not u , v ( w ] + v[ l  is u shortest path to
 value. We canl incorrectly computed the shortest path to a particular vertex, and inserting it back into the priority queue with the updated
 that v  be a vertex adjacent to u  that has just been extracted from the queue, and let v detect such instances as follows. Consider a vertex
 has been incorrectly computed, u ], then the shortest path to u[ l ) is smaller than u , v ( w ] + v[ l has already been extracted from the queue. If
). u , v ( w ] + v[ l ] = u[ l  needs to be inserted back into the priority queue with u and
. In this example, there are three processes and Figure 10.17 To see how this approach works, consider the example grid graph shown in
 will be reachable from the source. In d  and b . After initialization of the priority queue, vertices a we want to find the shortest path from vertex
. In the d  and b  values of the vertices adjacent tol  and proceed to update the d  and b  extract vertices 1 P  and 0 P the first step, process
 values of the vertices adjacent to them. Note thatl , and proceed to update the g , and c , e  extract 2 P , and 1 P , 0 P second step, processes
) > d , e ( w ] + e[ l ]. In this particular example, d[ l ) is smaller or greater than d , e ( w ] + e[ l  checks to see if 0 P , process e when processing vertex
 is considered, and all computations so e  does not change when d ], indicating that the previously computed value of the shortest path to d[ l
) = 5 g , h ( w ] + h[ l  compares 0 P , respectively. Now, when process f  and h  work on 1 P  and 0 P far are correct. In the third step, processes
] = 10 that was extracted in the previous iteration, it finds it to be smaller. As a result, it inserts back into the priority g[ l against the value of
] value. Finally, in the fourth and last step, the remaining two vertices are extracted from the priority g[ l  with the updated g queue vertex
queue, and all single-source shortest paths have been computed.
Figure 10.17. An example of the modified Johnson's algorithm for processing unsafe vertices
concurrently.
. Section 3.2.4 This approach for parallelizing Johnson's algorithm falls into the category of speculative decomposition discussed in
 vertices in the priority queue will not change as a result of processing p [] values of the top l Essentially, the algorithm assumes that the
some of these vertices, and proceeds to perform the computations required by Johnson's algorithm. However, if at some later point it
detects that its assumptions were wrong, it goes back and essentially recomputes the shortest paths of the affected vertices.
In order for such a speculative decomposition approach to be effective, we must also remove the bottleneck of working with a single
priority queue. In the rest of this section we present a message-passing algorithm that uses speculative decomposition to extract
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
concurrency and in which there is no single priority queue. Instead, each process maintains its own priority queue for the vertices that it is
assigned to. Problem 10.13 discusses another approach.
Distributed Memory Formulation
, p V , ..., 2 V , 1 V  disjoint sets p  into V ) be a sparse graph. We partition the set of vertices E , V  = ( G  be the number of processes, and let p Let
 processes. Each process maintains a priority queue for p and assign each set of vertices and its associated adjacency lists to one of the
 is partitioned into Q the vertices assigned to it, and computes the shortest paths from the source to these vertices. Thus, the priority queue
 alsoi P , each assigned to a separate process. In addition to the priority queue, each process p Q , ..., 2 Q , 1 Q  disjoint priority queues p
] is v[ sp . The cost i V v  for each vertex v ] stores the cost of the shortest path from the source vertex to v[ sp  such that sp maintains an array
 other than the source, and we v  for every vertex ] = v[ sp  is extracted from the priority queue. Initially, v ] each time vertex v[ l updated to
. Each process executes Johnson's algorithm on its local priority queue. s ] into the appropriate priority queue for the source vertex s[ l insert
. v ] stores the length of the shortest path from source to vertex v[ sp At the end of the algorithm,
i P  values of vertices assigned to processes other thanl , the i Q ] from u[ l  with the smallest valuei V u  extracts the vertexi P When process
, notifying them of the new values. u  sends a message to processes that store vertices adjacent toi P may need to be updated. Process
v  andi V u ) such that v , u . For example, assume that there is an edge ( l Upon receiving these values, processes update the values of
 containing the potential j P  then sends a message toi P  from its priority queue. Process u  has just extracted vertexi P , and that process j V
], v[ l ] stored in its priority queue to min{ v[ l , upon receiving this message, sets the value of j P ). Process v , u ( w ] + u[ l ], which is v[ l new value of
)}. v , u ( w ] + u[ l
 from its priority v  has already extracted vertex j P  execute Johnson's algorithm, it is possible that process j P  andi P Since both processes
. Then there are two v ] from the source to vertex v[ sp  might have already computed the shortest path j P queue. This means that process
 passing v ). The first case means that there is a longer path to vertex v , u ( w ] + u[ l ] > v[ sp ), or v , u ( w ] + u[ l ] v[ sp possible cases: either
j P . For the first case, process u  passing through vertex v , and the second case means that there is a shorter path to vertex u through vertex
 must update the cost of the shortest j P  does not change. For the second case, process v needs to do nothing, since the shortest path to
]. v[ sp ) and disregarding the value of v , u ( w ] + u[ l ] = v[ l  back into the priority queue with v . This is done by inserting the vertex v path to vertex
 can be reinserted into the priority queue, the algorithm terminates only when all the queues become empty. v Since a vertex
Initially, only the priority queue of the process with the source vertex is non-empty. After that, the priority queues of other processes
l  values are created and sent to adjacent processes. When processes receive newl become populated as messages containing new
values, they insert them into their priority queues and perform computations. Consider the problem of computing the single-source shortest
paths in a grid graph where the source is located at the bottom-left corner. The computations propagate across the grid graph in the form
of a wave. A process is idle before the wave arrives, and becomes idle again after the wave has passed. This process is illustrated in
. At any time during the execution of the algorithm, only the processes along the wave are busy. The other processes have Figure 10.18
-process p either finished their computations or have not yet started them. The next sections discuss three mappings of grid graphs onto a
mesh.
Figure 10.18. The wave of activity in the priority queues.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). Specifically, we Section 3.4.1  processors is to use the 2-D block mapping ( p  grid graph onto n  x n  One way to map an 2-D Block Mapping
Figure  vertices to each process.  processes as a logical mesh and assign a different block of p can view the
 illustrates this mapping. 10.19
Figure 10.19. Mapping the grid graph (a) onto a mesh, and (b) by using the 2-D block mapping.
. The shaded vertices are mapped onto the shaded  = 16 and n In this example,
process.
At any time, the number of busy processes is equal to the number of processes intersected by the wave. Since the wave moves
 be the overall work performed by the sequential algorithm. If we W ) processes are busy at any time. Let  ( O diagonally, no more than
 processes are performing computations, and if we ignore the overhead due to inter-process assume that, at any time,
communication and extra work, then the maximum speedup and efficiency are as follows:
The efficiency of this mapping is poor and becomes worse as the number of processes increases.
e  The main limitation of the 2-D block mapping is that each process is responsible for only a small, confined area of th 2-D Cyclic Mapping
). Section 3.4.1 grid. Alternatively, we can make each process responsible for scattered areas of the grid by using the 2-D cyclic mapping (
 blocks, each of p/ 2 n  grid graph is divided into n  x n This increases the time during which a process stays busy. In 2-D cyclic mapping, the
 illustrates this mapping. Each Figure 10.20  process mesh. . Each block is mapped onto the size
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 vertices apart. Each process is  vertices. These vertices belong to diagonals of the graph that are p/ 2 n process contains a block of
 such diagonals. assigned roughly
Figure 10.20. Mapping the grid graph (a) onto a mesh, and (b) by using the 2-D cyclic mapping.
 = 4. The shaded graph vertices are mapped onto the  = 16 and n In this example,
correspondingly shaded mesh processes.
Now each process is responsible for vertices that belong to different parts of the grid graph. As the wave propagates through the graph,
the wave intersects some of the vertices on each process. Thus, processes remain busy for most of the algorithm. The 2-D cyclic mapping,
though, incurs a higher communication overhead than does the 2-D block mapping. Since adjacent vertices reside on separate processes,
]. The analysis of this u[ l  from its priority queue it must notify other processes of the new value of u every time a process extracts a vertex
mapping is left as an exercise (Problem 10.17).
)  ( O  The two mappings discussed so far have limitations. The 2-D block mapping fails to keep more than 1-D Block Mapping
 processes as a p processes busy at any time, and the 2-D cyclic mapping has high communication overhead. Another mapping treats the
 illustrates this Figure 10.21  stripes of the grid graph to each processor by using the 1-D block mapping. p/ n linear array and assigns
mapping.
n Figure 10.21. Mapping the grid graph (a) onto a linear array of processes (b). In this example,
 = 4. The shaded vertices are mapped onto the shaded process. p = 16 and
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Initially, the wave intersects only one process. As computation progresses, the wave spills over to the second process so that two
processes are busy. As the algorithm continues, the wave intersects more processes, which become busy. This process continues until all
 processes are busy (that is, until they all have been intersected by the wave). After this point, the number of busy processes decreases. p
/2 processes (on the p  illustrates the propagation of the wave. If we assume that the wave propagates at a constant rate, then Figure 10.22
average) are busy. Ignoring any overhead, the speedup and efficiency of this mapping are as follows:
Figure 10.22. The number of busy processes as the computational wave propagates across the
grid graph.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Thus, the efficiency of this mapping is at most 50 percent. The 1-D block mapping is substantially better than the 2-D block mapping but
) processes. n  ( O cannot use more than
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
10.8 Bibliographic Remarks
] provides a good reference Gib85 Detailed discussions of graph theory and graph algorithms can be found in numerous texts. Gibbons [
] provide a CLR90 ], and Cormen, Leiserson, and Rivest [ AHU74 to the algorithms presented in this chapter. Aho, Hopcroft, and Ullman [
detailed description of various graph algorithms and issues related to their efficient implementation on sequential computers.
] and Deo and Yoo Ben80 ]. Bentley [ Pri57  is due to Prim [ Section 10.2 The sequential minimum spanning tree algorithm described in
] present parallel formulations of Prim's MST algorithm. Deo and Yoo's algorithm is suited to a shared-address-space computer. It DY81[
) processes. Bentley's algorithm works on a tree-connected systolic array and finds the MST in time 0.5 n ( Q ) using 1.5 n ( Q finds the MST in
 is similar to Bentley's algorithm. Section 10.2  processes. The hypercube formulation of Prim's MST algorithm in n /log n ) using n  log n ( Q
] sequential algorithms. The complexity of Sol77 ] or Sollin's [ Kru56 The MST of a graph can be also computed by using either Kruskal's [
] have developed a formulation of Sollin's algorithm for the SJ81 ). Savage and Jaja [ n  log 2 n ( Q Sollin's algorithm (Problem 10.21) is
] have CLC82 ). Chin, Lam, and Chen [ n 2 (log Q  processes and solves the problem in time 2 n CREW PRAM. Their algorithm uses
 processes and finds the MST in time developed a formulation of Sollin's algorithm for a CREW PRAM that uses
) 2 n ( Q ] present a formulation of Sollin's algorithm for the shuffle-exchange network that uses AS87 ). Awerbuch and Shiloach [ n 2 (log Q
process ring-connected p- ) time algorithm for a p/ 2 n ( Q ] present a DV87  Doshi and Varman [. ) n 2 (log Q processes and runs in time
] present parallel formulations of Sollin's NMB83 ] and Nath, Maheshwari, and Bhatt [ Lei83 computer for Sollin's algorithm. Leighton [
 mesh of n  x n ) for an n 4 (log Q ) and the second algorithm runs in n 2 (log Q algorithm for a mesh of trees network. The first algorithm runs in
 mesh of trees. ) on a p/ 2 n ( Q ] describes a formulation of Sollin's algorithm that runs in Hua85 trees. Huang [
]. Due to the similarity between Dijkstra's Dij59  was discovered by Dijkstra [ Section 10.3 The single-source shortest paths algorithm in
algorithm and Prim's MST algorithm, all the parallel formulations of Prim's algorithm discussed in the previous paragraph can also be
] independently developed a single-source shortest FR62 ] and Ford [ Bel58 applied to the single-source shortest paths problem. Bellman [
paths algorithm that operates on graphs with negative weights but without negative-weight cycles. The Bellman-Ford single-source
] present parallel formulations of both the Dijkstra and PK89 |). Paige and Kruskal [ E || V  (| O algorithm has a sequential complexity of
) n ( Q Bellman-Ford single-source shortest paths algorithm. Their formulation of Dijkstra's algorithm runs on an EREW PRAM of
) on a p-process EREW p  log n  + p |/ E| n ( Q ). Their formulation of Bellman-Ford's algorithm runs in time n  log n ( Q processes and runs in time
]. PK89 |. They also present algorithms for the CRCW PRAM [ E  | p PRAM where
Significant work has been done on the all-pairs shortest paths problem. The source-partitioning formulation of Dijkstra's all-pairs shortest
]. The source parallel formulation of Dijkstra's all-pairs shortest KS91b ] and Kumar and Singh [ JS87 paths is discussed by Jenq and Sahni [
]. The Floyd's all-pairs shortest paths algorithm KS91b ] and Kumar and Singh [ PK89 paths algorithm is discussed by Paige and Kruskal [
]. The 1-D and 2-D block mappings (Problem 10.8) are presented by Jenq and Sahni Flo62  is due to Floyd [ Section 10.4.2 discussed in
]. KS91b ] and Kumar and Singh [ BT89 ], and the pipelined version of Floyd's algorithm is presented by Bertsekas and Tsitsiklis [ JS87[
] present isoefficiency analysis and performance comparison of different parallel formulations for the all-pairs KS91b Kumar and Singh [
 is based upon the work of Kumar and Section 10.4.3 shortest paths on hypercube- and mesh-connected computers. The discussion in
]. Levitt and JS87  is adopted from the paper by Jenq and Sahni [ Algorithm 10.4 ]. In particular, JS87 ] and of Jenq and Sahni [ KS91b Singh [
). n ( Q  processes and runs in time 2 n ] present a formulation of Floyd's algorithm for two-dimensional cellular arrays that uses LK72 Kautz [
) on n ( Q Deo, Pank, and Lord have developed a parallel formulation of Floyd's algorithm for the CREW PRAM model that has complexity
n
] present a distributed all-pairs shortest-path algorithm based on diffusing computation. CM82  processes. Chandy and Misra [ 2
]. Cormen, Leiserson, and WS89  was discovered by Woo and Sahni [ Section 10.6 The connected-components algorithm discussed in
] discusses ways to efficiently implement disjoint-set data structures with ranking and path compression. Several CLR90 Rivest [
algorithms exist for computing the connected components; many of them are based on the technique of vertex collapsing, similar to
Sollin's algorithm for the minimum spanning tree. Most of the parallel formulations of Sollin's algorithm can also find the connected
] developed formulations of the connected-components HCS79 ] and Hirschberg, Chandra, and Sarwate [ Hir76 components. Hirschberg [
 processes, and the latter has 2 n ) on a CREW PRAM with n 2 (log Q algorithm based on vertex collapsing. The former has a complexity of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
] made the vertex collapse algorithm more CLC81  processes. Chin, Lam, and Chen [ similar complexity and uses
). n 2 (log Q  for a CREW PRAM, while keeping the run time at efficient by reducing the number of processes to
] used the vertex collapsing technique to develop a formulation for a mesh-connected computer that finds the NS80 Nassimi and Sahni [
 processes. 2 n ) by using n ( Q connected components in time
]. Paige Joh77 , was discovered by Johnson [ Section 10.7.2 The single-source shortest paths algorithm for sparse graphs, discussed in
] presented techniques to RK88a  in parallel. Rao and Kumar [ Q ] discuss the possibility of maintaining the queue PK89 and Kruskal [
perform concurrent insertions and deletions in a priority queue. The 2-D block mapping, 2-D block-cyclic mapping, and 1-D block
]. They also presented theoretical and WI89 ) are due to Wada and Ichiyoshi [ Section 10.7.2 mapping formulation of Johnson's algorithm (
experimental evaluation of these schemes on a mesh-connected parallel computer.
] and its parallel formulation on Lub86  was developed by Luby [ Section 10.7.1 The serial maximal independent set algorithm described in
]. Jones and Plassman KK99 shared-address-space architectures was motivated by the algorithm described by Karypis and Kumar [
] have developed an asynchronous variation of Luby's algorithm that is particularly suited for distributed memory parallel JP93[
computers. In their algorithm, each vertex is assigned a single random number, and after a communication step, each vertex determines
the number of its adjacent vertices that have smaller and greater random numbers. At this point each vertex gets into a loop waiting to
receive the color values of its adjacent vertices that have smaller random numbers. Once all these colors have been received, the vertex
selects a consistent color, and sends it to all of its adjacent vertices with greater random numbers. The algorithm terminates when all
vertices have been colored. Note that besides the initial communication step to determine the number of smaller and greater adjacent
vertices, this algorithm proceeds asynchronously.
] presented an algorithm for finding the maximum flow SV82 Other parallel graph algorithms have been proposed. Shiloach and Vishkin [
] GT88 -process EREW PRAM. Goldberg and Tarjan [ n ) on an n  log 2 n  ( O  vertices that runs in time n in a directed flow network with
-process EREW PRAM but requires less space. n ) on an n  log 2 n  ( O presented a different maximum-flow algorithm that runs in time
] proposed a number of algorithms for a mesh-connected parallel computer. The algorithms they considered AK84 Atallah and Kosaraju [
are for finding the bridges and articulation points of an undirected graph, finding the length of the shortest cycle, finding an MST, finding
] presented algorithms for computing the biconnected TV85 the cyclic index, and testing if a graph is bipartite. Tarjan and Vishkin [
|) processes, and their CREW PRAM V | + | E (| Q ) by using n (log Q components of a graph. Their CRCW PRAM formulation runs in time
) processes. n 2 /log 2 n ( Q ) by using n 2 (log Q formulation runs in time
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
), the maximum number of Section 10.2  In the parallel formulation of Prim's minimum spanning tree algorithm ( 10.1
) processes the run time is n /log n ( Q ). By using n /log n ( Q processes that can be used efficiently on a hypercube is
) processes? What is the minimum parallel run time that can be n ( Q ). What is the run time if you use n  log n ( Q
obtained on a message-passing parallel computer? How does this time compare with the run time obtained when
) processes? n /log n ( Q you use
) need to be modified in Section 10.3  Show how Dijkstra's single-source algorithm and its parallel formulation ( 10.2
order to output the shortest paths instead of the cost. Analyze the run time of your sequential and parallel
formulations.
V  are the values assigned to the vertices of G ), the breadth-first ranking of vertices of E , V  = ( G  Given a graph 10.3
 can be G . Show how the breadth-first ranking of vertices of v  from a node G in a breadth-first traversal of
-process mesh. p performed on a
) requires non-negative edge weights. Show Section 10.3  Dijkstra's single-source shortest paths algorithm ( 10.4
how Dijkstra's algorithm can be modified to work on graphs with negative weights but no negative cycles in time
-process p |). Analyze the performance of the parallel formulation of the modified algorithm on a V || E (| Q
message-passing architecture.
 Compute the total amount of memory required by the different parallel formulations of the all-pairs shortest 10.5
. Section 10.4 paths problem described in
 by the following Algorithm 10.3  is correct if we replace line 7 of Section 10.4.2  Show that Floyd's algorithm in 10.6
line:
 Compute the parallel run time, speedup, and efficiency of Floyd's all-pairs shortest paths algorithm using 2-D 10.7
-process p -process hypercube and a p -process mesh with store-and-forward routing and a p block mapping on a
mesh with cut-through routing.
 in Floyd's all-pairs shortest paths algorithm is to use the 1-D ) k ( D  An alternative way of partitioning the matrix 10.8
 matrix. ) k ( D  consecutive columns of the p/ n  processes is assigned p ). Each of the Section 3.4.1 block mapping (
Compute the parallel run time, speedup, and efficiency of 1-D block mapping on a
hypercube-connected parallel computer. What are the advantages and disadvantages of this
? Section 10.4.2 partitioning over the 2-D block mapping presented in
. a
-process mesh with p Compute the parallel run time, speedup, and efficiency of 1-D block mapping on a
-process ring. p -process mesh with cut-through routing, and a p store-and-forward routing, a
. b
 Describe and analyze the performance of a parallel formulation of Floyd's algorithm that uses 1-D block 10.9
. Section 10.4.2 mapping and the pipelining technique described in
Section  Compute the exact parallel run time, speedup, and efficiency of Floyd's pipelined formulation ( 10.10
). 10.4.2
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 Compute the parallel run time, the speedup, and the efficiency of the parallel formulation of the 10.11
-process mesh with store-and-forward routing p  for a Section 10.6 connected-component algorithm presented in
and with cut-through routing. Comment on the difference in the performance of the two architectures.
 uses 1-D block Section 10.6  The parallel formulation for the connected-component problem presented in 10.12
mapping to partition the matrix among processes. Consider an alternative parallel formulation in which 2-D block
mapping is used instead. Describe this formulation and analyze its performance and scalability on a hypercube, a
mesh with SF-routing, and a mesh with CT-routing. How does this scheme compare with 1-D block mapping?
 Consider the problem of parallelizing Johnson's single-source shortest paths algorithm for sparse graphs 10.13
 processes 2 p  processes to maintain the priority queue and 1 p ). One way of parallelizing it is to use Section 10.7.2 (
 values. How many processes can be efficiently used to maintain thel to perform the computations of the new
)? How many processes can be used to update 1 p priority queue (in other words, what is the maximum value for
 processes cost-optimal? Describe an 2 p  + 1 p  values? Is the parallel formulation that is obtained by using thel the
 processes to maintain the priority queue. 1 p algorithm that uses
). We can Section 10.7  Consider Dijkstra's single-source shortest paths algorithm for sparse graphs ( 10.14
 adjacency lists among the processes n -process hypercube by splitting the p parallelize this algorithm on a
 lists. What is the parallel run time of this formulation? Alternatively, we p/ n horizontally; that is, each process gets
can partition the adjacency list vertically among the processes; that is, each process gets a fraction of each
 elements. The p/ m  elements, then each process contains a sublist of m adjacency list. If an adjacency list contains
last element in each sublist has a pointer to the element in the next process. What is the parallel run time and
speedup of this formulation? What is the maximum number of processes that it can use?
 Repeat Problem 10.14 for Floyd's all-pairs shortest paths algorithm. 10.15
 Analyze the performance of Luby's shared-address-space algorithm for finding a maximal independent set 10.16
. What is the parallel run time and speedup of this Section 10.7.1 of vertices on sparse graphs described in
formulation?
 Compute the parallel run time, speedup, and efficiency of the 2-D cyclic mapping of the sparse graph 10.17
) for a mesh-connected computer. You may ignore the Section 10.7.2 single-source shortest paths algorithm (
overhead due to extra work, but you should take into account the overhead due to communication.
) Section 10.7.2  Analyze the performance of the single-source shortest paths algorithm for sparse graphs ( 10.18
). Compare it with the performance of the 2-D cyclic Section 3.4.1 when the 2-D block-cyclic mapping is used (
mapping computed in Problem 10.17. As in Problem 10.17, ignore extra computation but include communication
overhead.
. Describe how you will apply this mapping Section 3.4.1  Consider the 1-D block-cyclic mapping described in 10.19
to the single-source shortest paths problem for sparse graphs. Compute the parallel run time, speedup, and
efficiency of this mapping. In your analysis, include the communication overhead but not the overhead due to
extra work.
 and in Problems 10.18 and 10.19, which one has the Section 10.7.2  Of the mapping schemes presented in 10.20
smallest overhead due to extra computation?
m  isolated vertices. In each iteration, the algorith n ) starts with a forest of Section 10.8  Sollin's algorithm ( 10.21
x simultaneously determines, for each tree in the forest, the smallest edge joining any vertex in that tree to a verte
e in another tree. All such edges are added to the forest. Furthermore, two trees are never joined by more than on
e edge. This process continues until there is only one tree in the forest – the minimum spanning tree. Since th
n number of trees is reduced by a factor of at least two in each iteration, this algorithm requires at most log
) comparisons to find the smallest edge incident on 2 n  ( O iterations to find the MST. Each iteration requires at most
). Develop a parallel formulation of Sollin's algorithm on n  log 2 n ( Q each vertex; thus, its sequential complexity is
-process hypercube-connected parallel computer. What is the run time of your formulation? Is it cost optimal? n an
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 11. Search Algorithms for Discrete
Optimization Problems
Search algorithms can be used to solve discrete optimization problems (DOPs), a class of computationally expensive problems with
significant theoretical and practical interest. Search algorithms solve DOPs by evaluating candidate solutions from a finite or countably
infinite set of possible solutions to find one that satisfies a problem-specific criterion. DOPs are also referred to as combinatorial
problems.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
11.1 Definitions and Examples
 is a finite or countably infinite set of all solutions that satisfy S ). The setf , S  can be expressed as a tuple ( discrete optimization problem A
S  is the cost function that maps each element in setf . The function feasible solutions specified constraints. This set is called the set of
. R onto the set of real numbers
. S x ) for all x  (f ) opt x  (f , such that opt x The objective of a DOP is to find a feasible solution
Problems from various domains can be formulated as DOPs. Some examples are planning and scheduling, the optimal layout of VLSI
chips, robot motion planning, test-pattern generation for digital circuits, and logistics and control.
Example 11.1 The 0/1 integer-linear-programming problem
. c  x 1 vector n , and an b  x 1 vector m , an A  matrix n  x m In the 0/1 integer-linear-programming problem, we are given an
 whose elements can take on only the value 0 or 1. The vector must x  x 1 vector n The objective is to determine an
satisfy the constraint
and the function
Ax  that satisfy the equation x  is the set of all values of the vector S must be minimized. For this problem, the set
. b
Example 11.2 The 8-puzzle problem
The 8-puzzle problem consists of a 3 x 3 grid containing eight tiles, numbered one through eight. One of the grid
segments (called the "blank") is empty. A tile can be moved into the blank position from a position adjacent to it, thus
creating a blank in the tile's original position. Depending on the configuration of the grid, up to four moves are possible:
up, down, left, and right. The initial and final configurations of the tiles are specified. The objective is to determine a
 illustrates Figure 11.1 shortest sequence of moves that transforms the initial configuration to the final configuration.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
sample initial and final configurations and a sequence of moves leading from the initial configuration to the final
configuration.
Figure 11.1. An 8-puzzle problem instance: (a) initial configuration; (b) final
configuration; and (c) a sequence of moves leading from the initial to the final
configuration.
 for this problem is the set of all sequences of moves that lead from the initial to the final configurations. The S The set
 is defined as the number of moves in the sequence. S  of an element inf cost function
 is quite large. Consequently, it is not feasible to exhaustively enumerate the S In most problems of practical interest, the solution set
. Instead, a DOP can be reformulated as the problem of finding a minimum-cost path in opt x  to determine the optimal element S elements in
 can be viewed as a path from the initial S  in x a graph from a designated initial node to one of several possible goal nodes. Each element
 is defined in terms of thesef node to one of the goal nodes. There is a cost associated with each edge of the graph, and a cost function
, and the nodes of state space edge costs. For many problems, the cost of a path is the sum of the edge costs. Such a graph is called a
. The 8-puzzle nonterminal nodes  is one that has no successors. All other nodes are called terminal node . A states the graph are called
problem can be naturally formulated as a graph search problem. In particular, the initial configuration is the initial node, and the final
 illustrates the process of reformulating the 0/1 integer-linear-programming problem as a Example 11.3 configuration is the goal node.
graph search problem.
Example 11.3 The 0/1 integer-linear-programming problem revisited
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, and b , A . Let the values of Example 11.1 Consider an instance of the 0/1 integer-linear-programming problem defined in
 be given by c
 are as follows: c , and b , A The constraints corresponding to
) to be minimized is x  (f and the function
. However, many of x  = 16 possible values for 4  can take the value 0 or 1. There are 2 x Each of the four elements of vector
these values do not satisfy the problem's constraints.
The problem can be reformulated as a graph-search problem. The initial node represents the state in which none of the
 have been assigned values. In this example, we assign values to vector elements in subscript order; x elements of vector
 = 1. After a 1 x  = 0 and 1 x , and so on. The initial node generates two nodes corresponding to 2 x , then 1 x that is, first
free . All variables that are not fixed are called fixed variable  has been assigned a value, it is called ai x variable
. variables
After instantiating a variable to 0 or 1, it is possible to check whether an instantiation of the remaining free variables can
lead to a feasible solution. We do this by using the following condition:
1  Equation 11.
 that can be obtained by instantiating  is the maximum value of Equation 11.1 The left side of
, then the node may lead to a m  = 1, 2,...,i  , fori b the free variables to either 0 or 1. If this value is greater than or equal to
feasible solution.
) is selected and assigned a value. The 2 x  = 1, the next variable ( 1 x  = 0 and 1 x For each of the nodes corresponding to
nodes are then checked for feasibility. This process continues until all the variables have been assigned and the feasible
 illustrates this process. Figure 11.2 set has been generated.
Figure 11.2. The graph corresponding to the 0/1 integer-linear-programming
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
problem.
) is evaluated for each of the feasible solutions; the solution with the minimum value is the desired solution. x  (f Function
Note that it is unnecessary to generate the entire feasible set to determine the solution. Several search algorithms can
determine an optimal solution by searching only a portion of the graph.
heuristic For some problems, it is possible to estimate the cost to reach the goal state from an intermediate state. This cost is called a
 from x ) denote the cost of reaching state x ( g  and x ) denote the heuristic estimate of reaching the goal state from state x ( h . Let estimate
) is a lower bound on the cost of reaching the goal x ( h . If heuristic function  is called a h  along the current path. The function s initial state
) is a lower x ( l  is admissible, then h ). If x ( g ) + x ( h ) as the sum x ( l . We define function admissible  is called h , then x  for all x state from state
. In subsequent examples x  and s bound on the cost of the path to a goal state that can be obtained by extending the current path between
we will see how an admissible heuristic can be used to determine the least-cost sequence of moves from the initial state to a goal state.
Example 11.4 An admissible heuristic function for the 8-puzzle
Assume that each position in the 8-puzzle grid is represented as a pair. The pair (1, 1) represents the top-left grid
) is defined l , k ) and ( j , i position and the pair (3, 3) represents the bottom-right position. The distance between positions (
. The sum of the Manhattan distances between the initial Manhattan distance |. This distance is called the l  - j | + | k  -i as |
and final positions of all tiles is an estimate of the number of moves required to transform the current configuration into
) is the Manhattan distance x ( h . Note that if Manhattan heuristic the final configuration. This estimate is called the
) is also a lower bound on the number of moves from x ( h  and the final configuration, then x between configuration
 to the final configuration. Hence the Manhattan heuristic is admissible. x configuration
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Once a DOP has been formulated as a graph search problem, it can be solved by algorithms such as branch-and-bound search and
S heuristic search. These techniques use heuristics and the structure of the search space to solve DOPs without searching the set
exhaustively.
DOPs belong to the class of NP-hard problems. One may argue that it is pointless to apply parallel processing to these problems, since
we can never reduce their worst-case run time to a polynomial without using exponentially many processors. However, the average-time
complexity of heuristic search algorithms for many problems is polynomial. Furthermore, there are heuristic search algorithms that find
suboptimal solutions for specific problems in polynomial time. In such cases, bigger problem instances can be solved using parallel
computers. Many DOPs (such as robot motion planning, speech understanding, and task scheduling) require real-time solutions. For these
applications, parallel processing may be the only way to obtain acceptable performance. Other problems, for which optimal solutions are
highly desirable, can be solved for moderate-sized instances in a reasonable amount of time by using parallel search techniques (for
example, VLSI floor-plan optimization, and computer-aided design).
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
11.2 Sequential Search Algorithms
The most suitable sequential search algorithm to apply to a state space depends on whether the space forms a graph or a tree. In a tree,
each new successor leads to an unexplored part of the search space. An example of this is the 0/1 integer-programming problem. In a
graph, however, a state can be reached along multiple paths. An example of such a problem is the 8-puzzle. For such problems,
whenever a state is generated, it is necessary to check if the state has already been generated. If this check is not performed, then
). Figure 11.3 effectively the search graph is unfolded into a tree in which a state is repeated for every path that leads to it (
Figure 11.3. Two examples of unfolding a graph into a tree.
For many problems (for example, the 8-puzzle), unfolding increases the size of the search space by a small factor. For some problems,
 illustrates a graph whose corresponding tree has an Figure 11.3(b) however, unfolded graphs are much larger than the original graphs.
exponentially higher number of states. In this section, we present an overview of various sequential algorithms used to solve DOPs that
are formulated as tree or graph search problems.
11.2.1 Depth-First Search Algorithms
 (DFS) algorithms solve DOPs that can be formulated as tree-search problems. DFS begins by expanding the initial Depth-first search
node and generating its successors. In each subsequent step, DFS expands one of the most recently generated nodes. If this node has no
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
successors (or cannot lead to any solutions), then DFS backtracks and expands a different node. In some DFS algorithms, successors of a
node are expanded in an order determined by their heuristic values. A major advantage of DFS is that its storage requirement is linear in
the depth of the state space being searched. The following sections discuss three algorithms based on depth-first search.
Simple Backtracking
 is a depth-first search method that terminates upon finding the first solution. Thus, it is not guaranteed to find a Simple backtracking
minimum-cost solution. Simple backtracking uses no heuristic information to order the successors of an expanded node. A variant,
, does use heuristics to order the successors of an expanded node. ordered backtracking
Depth-First Branch-and-Bound
 (DFBB) exhaustively searches the state space; that is, it continues to search even after finding a solution Depth-first branch-and-bound
path. Whenever it finds a new solution path, it updates the current best solution path. DFBB discards inferior partial solution paths (that is,
partial solution paths whose extensions are guaranteed to be worse than the current best solution path). Upon termination, the current
best solution is a globally optimal solution.
Iterative Deepening A*
Trees corresponding to DOPs can be very deep. Thus, a DFS algorithm may get stuck searching a deep part of the search space when a
solution exists higher up on another branch. For such trees, we impose a bound on the depth to which the DFS algorithm searches. If the
node to be expanded is beyond the depth bound, then the node is not expanded and the algorithm backtracks. If a solution is not found,
iterative deepening depth-first search then the entire state space is searched again using a larger depth bound. This technique is called
(ID-DFS). Note that this method is guaranteed to find a solution path with the fewest edges. However, it is not guaranteed to find a
least-cost path.
 that for Section 11.1 -values of nodes to bound depth (recall froml  (IDA*) is a variant of ID-DFS. IDA* uses the Iterative deepening A*
)). IDA* repeatedly performs cost-bounded DFS over the search space. In each iteration, IDA* expands nodes x ( h ) + x ( g ) = x ( l , x node
-value of the node to be expanded is greater than the cost bound, then IDA* backtracks. If a solution is not found withinl depth-first. If the
the current cost bound, then IDA* repeats the entire depth-first search using a higher cost bound. In the first iteration, the cost bound is set
). In each subsequent iteration, the cost bound is s ( h ) is equal to s ( l ) is zero, s ( g . Note that since s -value of the initial statel to the
-value of the nodes that were generated but could not be expanded in thel increased. The new cost bound is equal to the minimum
previous iteration. The algorithm terminates when a goal node is expanded. IDA* is guaranteed to find an optimal solution if the heuristic
function is admissible. It may appear that IDA* performs a lot of redundant work across iterations. However, for many problems the
redundant work performed by IDA* is minimal, because most of the work is done deep in the search space.
Example 11.5 Depth-first search: the 8-puzzle
 shows the execution of depth-first search for solving the 8-puzzle problem. The search starts at the initial Figure 11.4
configuration. Successors of this state are generated by applying possible moves. During each step of the search
algorithm a new state is selected, and its successors are generated. The DFS algorithm expands the deepest node in the
tree. In step 1, the initial state A generates states B and C. One of these is selected according to a predetermined
criterion. In the example, we order successors by applicable moves as follows: up, down, left, and right. In step 2, the DFS
 can be discarded, as it is a duplicate of the D . Note that the state F , and E , D  and generates states B algorithm selects state
 can be discarded because it is a duplicate G . Again H  and G  is expanded to generate states E . In step 3, state B parent of
. The search proceeds in this way until the algorithm backtracks or the final configuration is generated. B of
Figure 11.4. States resulting from the first three steps of depth-first search applied to an instance of the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
8-puzzle.
In each step of the DFS algorithm, untried alternatives must be stored. For example, in the 8-puzzle problem, up to three untried
 is the maximum depth, then d  is the amount of storage required to store a state, and m alternatives are stored at each step. In general, if
). The state-space tree searched by parallel DFS can be efficiently represented md  ( O the total space requirement of the DFS algorithm is
as a stack. Since the depth of the stack increases linearly with the depth of the tree, the memory requirements of a stack representation
are low.
There are two ways of storing untried alternatives using a stack. In the first representation, untried alternates are pushed on the stack at
 illustrates this representation for the tree shown in Figure 11.5(b) each step. The ancestors of a state are not represented on the stack.
, untried alternatives are stored along with their parent state. It is Figure 11.5(c) . In the second representation, shown in Figure 11.5(a)
necessary to use the second representation if the sequence of transformations from the initial state to the goal state is required as a part
of the solution. Furthermore, if the state space is a graph in which it is possible to generate an ancestor state by applying a sequence of
transformations to the current state, then it is desirable to use the second representation, because it allows us to check for duplication of
ancestor states and thus remove any cycles from the state-space graph. The second representation is useful for problems such as the
 should be discarded. G  and D , using the second representation allows the algorithm to detect that nodes Example 11.5 8-puzzle. In
Figure 11.5. Representing a DFS tree: (a) the DFS tree; successor nodes shown with dashed lines have already been
explored; (b) the stack storing untried alternatives only; and (c) the stack storing untried alternatives along with their
parent. The shaded blocks represent the parent state and the block to the right represents successor states that have
not been explored.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
11.2.2 Best-First Search Algorithms
Best-first search (BFS) algorithms can search both graphs and trees. These algorithms use heuristics to direct the search to portions of the
 and open search space likely to yield solutions. Smaller heuristic values are assigned to more promising nodes. BFS maintains two lists:
 list. This list is sorted according to a heuristic evaluation function that open . At the beginning, the initial node is placed on the closed
 list is removed. If open measures how likely each node is to yield a solution. In each step of the search, the most promising node from the
 list. closed this node is a goal node, then the algorithm terminates. Otherwise, the node is expanded. The expanded node is placed on the
 list under one of the following circumstances: (1) the successor is not open The successors of the newly expanded node are placed on the
 list but has a lower heuristic value. In the second closed  or open  lists, and (2) the successor is already on the closed  or open already on the
case, the node with the higher heuristic value is deleted.
 as a heuristic evaluation function. Recalll . The A* algorithm uses the lower bound function A* algorithm A common BFS technique is the
l  list are ordered according to the value of the open ). Nodes in the x ( h ) and x ( g ) is the sum of x ( l  , x  that for each node Section 11.1 from
 list and expanded. Its open -value (that is, the best node) is removed from thel function. At each step, the node with the smallest
 list. For an admissible closed  list at the proper positions and the node itself is inserted into the open successors are inserted into the
heuristic function, A* finds an optimal solution.
The main drawback of any BFS algorithm is that its memory requirement is linear in the size of the search space explored. For many
problems, the size of the search space is exponential in the depth of the tree expanded. For problems with large search spaces, memory
becomes a limitation.
Example 11.6 Best-first search: the 8-puzzle
 illustrates four steps of best-first search on the Figure 11.6 . 11.4  and 11.2 Consider the 8-puzzle problem from Examples
)) is selected for expansion. Ties are broken x ( h ) + x ( g ) = x ( l -value (l  with the minimum x 8-puzzle. At each step, a state
closed  or open arbitrarily. BFS can check for a duplicate nodes, since all previously generated nodes are kept on either the
list.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 11.6. Applying best-first search to the 8-puzzle: (a) initial configuration; (b) final configuration;
-value h and (c) states resulting from the first four steps of best-first search. Each state is labeled with its
(that is, the Manhattan distance from the state to the final state).
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
11.3 Search Overhead Factor
Parallel search algorithms incur overhead from several sources. These include communication overhead, idle time due to load
imbalance, and contention for shared data structures. Thus, if both the sequential and parallel formulations of an algorithm do the same
. However, the amount of work done by a parallel p  processors is less than p amount of work, the speedup of parallel search on
formulation is often different from that done by the corresponding sequential formulation because they may explore different parts of the
search space.
search  processors. The p  be the total amount of work done by p W  be the amount of work done by a single processor, and W Let
 of the parallel system is defined as the ratio of the work done by the parallel formulation to that done by the sequential overhead factor
). The actual speedup, however, p W / W  x( p . Thus, the upper bound on speedup for the parallel system is given by W / p W formulation, or
may be less due to other parallel processing overhead. In most parallel search algorithms, the search overhead factor is greater than
one. However, in some cases, it may be less than one, leading to superlinear speedup. If the search overhead factor is less than one on
the average, then it indicates that the serial search algorithm is not the fastest algorithm for solving the problem.
 are the number of p W  and W To simplify our presentation and analysis, we assume that the time to expand each node is the same, and
, then the sequential run time ct nodes expanded by the serial and the parallel formulations, respectively. If the time for each expansion is
S T  and the serial run time W  = 1. Hence, the problem size ct . In the remainder of the chapter, we assume that W ct  = S T is given by
become the same.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
11.4 Parallel Depth-First Search
We start our discussion of parallel depth-first search by focusing on simple backtracking. Parallel formulations of depth-first
. 11.4.7  and 11.4.6 branch-and-bound and IDA* are similar to those discussed in this section and are addressed in Sections
The critical issue in parallel depth-first search algorithms is the distribution of the search space among the processors. Consider the tree
. Note that the left subtree (rooted at node A) can be searched in parallel with the right subtree (rooted at node B). By Figure 11.7 shown in
statically assigning a node in the tree to a processor, it is possible to expand the whole subtree rooted at that node without communicating
with another processor. Thus, it seems that such a static allocation yields a good parallel search algorithm.
Figure 11.7. The unstructured nature of tree search and the imbalance resulting from static partitioning.
. Assume that we have two processors. The root node is Figure 11.7 Let us see what happens if we try to apply this approach to the tree in
expanded to generate two nodes (A and B), and each of these nodes is assigned to one of the processors. Each processor now searches
the subtrees rooted at its assigned node independently. At this point, the problem with static node assignment becomes apparent. The
processor exploring the subtree rooted at node A expands considerably fewer nodes than does the other processor. Due to this imbalance
in the workload, one processor is idle for a significant amount of time, reducing efficiency. Using a larger number of processors worsens
the imbalance. Consider the partitioning of the tree for four processors. Nodes A and B are expanded to generate nodes C, D, E, and F.
Assume that each of these nodes is assigned to one of the four processors. Now the processor searching the subtree rooted at node E
does most of the work, and those searching the subtrees rooted at nodes C and D spend most of their time idle. The static partitioning of
unstructured trees yields poor performance because of substantial variation in the size of partitions of the search space rooted at different
nodes. Furthermore, since the search space is usually generated dynamically, it is difficult to get a good estimate of the size of the search
space beforehand. Therefore, it is necessary to balance the search space among processors dynamically.
, when a processor runs out of work, it gets more work from another processor that has work. Consider the dynamic load balancing In
. Assume that nodes A and B are assigned to the two processors as we just Figure 11.7(a) two-processor partitioning of the tree in
described. In this case when the processor searching the subtree rooted at node A runs out of work, it requests work from the other
processor. Although the dynamic distribution of work results in communication overhead for work requests and work transfers, it reduces
load imbalance among processors. This section explores several schemes for dynamically balancing the load between processors.
A parallel formulation of DFS based on dynamic load balancing is as follows. Each processor performs DFS on a disjoint part of the search
space. When a processor finishes searching its part of the search space, it requests an unsearched part from other processors. This takes
the form of work request and response messages in message passing architectures, and locking and extracting work in shared address
space machines. Whenever a processor finds a goal node, all the processors terminate. If the search space is finite and the problem has
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
no solutions, then all the processors eventually run out of work, and the algorithm terminates.
Since each processor searches the state space depth-first, unexplored states can be conveniently stored as a stack. Each processor
maintains its own local stack on which it executes DFS. When a processor's local stack is empty, it requests (either via explicit messages
or by locking) untried alternatives from another processor's stack. In the beginning, the entire search space is assigned to one processor,
and other processors are assigned null search spaces (that is, empty stacks). The search space is distributed among the processors as
 processor and to the processor that requests and receives donor they request work. We refer to the processor that sends work as the
 processor. recipient work as the
 (that is, it is trying to get work). In idle  (that is, it has work) or active , each processor can be in one of two states: Figure 11.8 As illustrated in
message passing architectures, an idle processor selects a donor processor and sends it a work request. If the idle processor receives
 message (because the reject work (part of the state space to be searched) from the donor processor, it becomes active. If it receives a
donor has no work), it selects another donor and sends a work request to that donor. This process repeats until the processor gets work or
 message. The reject all the processors become idle. When a processor is idle and it receives a work request, that processor returns a
same process can be implemented on shared address space machines by locking another processors' stack, examining it to see if it has
work, extracting work, and unlocking the stack.
Figure 11.8. A generic scheme for dynamic load balancing.
On message passing architectures, in the active state, a processor does a fixed amount of work (expands a fixed number of nodes) and
then checks for pending work requests. When a work request is received, the processor partitions its work into two parts and sends one
part to the requesting processor. When a processor has exhausted its own search space, it becomes idle. This process continues until a
solution is found or until the entire space has been searched. If a solution is found, a message is broadcast to all processors to stop
Section searching. A termination detection algorithm is used to detect whether all processors have become idle without finding a solution (
). 11.4.4
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
11.4.1 Important Parameters of Parallel DFS
Two characteristics of parallel DFS are critical to determining its performance. First is the method for splitting work at a processor, and the
second is the scheme to determine the donor processor when a processor becomes idle.
Work-Splitting Strategies
When work is transferred, the donor's stack is split into two stacks, one of which is sent to the recipient. In other words, some of the nodes
(that is, alternatives) are removed from the donor's stack and added to the recipient's stack. If too little work is sent, the recipient quickly
becomes idle; if too much, the donor becomes idle. Ideally, the stack is split into two equal pieces such that the size of the search space
. It is difficult to get a good estimate of the size of the tree rooted half-split represented by each stack is the same. Such a split is called a
at an unexpanded alternative in the stack. However, the alternatives near the bottom of the stack (that is, close to the initial node) tend to
have bigger trees rooted at them, and alternatives near the top of the stack tend to have small trees rooted at them. To avoid sending very
. cutoff depth small amounts of work, nodes beyond a specified stack depth are not given away. This depth is called the
Some possible strategies for splitting the search space are (1) send nodes near the bottom of the stack, (2) send nodes near the cutoff
depth, and (3) send half the nodes between the bottom of the stack and the cutoff depth. The suitability of a splitting strategy depends on
the nature of the search space. If the search space is uniform, both strategies 1 and 3 work well. If the search space is highly irregular,
strategy 3 usually works well. If a strong heuristic is available (to order successors so that goal nodes move to the left of the state-space
tree), strategy 2 is likely to perform better, since it tries to distribute those parts of the search space likely to contain a solution. The cost of
splitting also becomes important if the stacks are deep. For such stacks, strategy 1 has lower cost than strategies 2 and 3.
 into two subtrees using strategy 3. Note that the states beyond the Figure 11.5(a)  shows the partitioning of the DFS tree of Figure 11.9
 also shows the representation of the stack corresponding to the two subtrees. The stack Figure 11.9 cutoff depth are not partitioned.
representation used in the figure stores only the unexplored alternatives.
. The two subtrees along with their stack representations are shown in Figure 11.5 Figure 11.9. Splitting the DFS tree in
(a) and (b).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Load-Balancing Schemes
This section discusses three dynamic load-balancing schemes: asynchronous round robin, global round robin, and random polling. Each
of these schemes can be coded for message passing as well as shared address space machines.
. Whenever a target  In asynchronous round robin (ARR), each processor maintains an independent variable, Asynchronous Round Robin
 is target  as the label of a donor processor and attempts to get work from it. The value of target processor runs out of work, it uses
) where p  + 1) modulo label  at each processor is set to (( target ) each time a work request is sent. The initial value of p incremented (modulo
 is the local processor label. Note that work requests are generated independently by each processor. However, it is possible for two label
or more processors to request work from the same donor at nearly the same time.
. This variable can be stored in a globally target  Global round robin (GRR) uses a single global variable called Global Round Robin
accessible space in shared address space machines or at a designated processor in message passing machines. Whenever a processor
, either by locking, reading, and unlocking on shared address space machines or target needs work, it requests and receives the value of
) before responding to p  is incremented (modulo target ). The value of 0 P by sending a message requesting the designated processor (say
. GRR target the next request. The recipient processor then attempts to get work from a donor processor whose label is the value of
ensures that successive work requests are distributed evenly over all processors. A drawback of this scheme is the contention for access
. target to
a  Random polling (RP) is the simplest load-balancing scheme. When a processor becomes idle, it randomly selects Random Polling
donor. Each processor is selected as a donor with equal probability, ensuring that work requests are evenly distributed.
11.4.2 A General Framework for Analysis of Parallel DFS
o T To analyze the performance and scalability of parallel DFS algorithms for any load-balancing scheme, we must compute the overhead
of the algorithm. Overhead in any load-balancing scheme is due to communication (requesting and sending work), idle time (waiting for
work), termination detection, and contention for shared resources. If the search overhead factor is greater than one (i.e., if parallel search
. In this section we assume that the search overhead factor is one, i.e., o T does more work than serial search), this will add another term to
the serial and parallel versions of the algorithm perform the same amount of computation. We analyze the case in which the search
. Section 11.6.1 overhead factor is other than one in
, idle time is subsumed by communication overhead due to work requests and Section 11.4.1 For the load-balancing schemes discussed in
transfers. When a processor becomes idle, it immediately selects a donor processor and sends it a work request. The total time for which
the processor remains idle is equal to the time for the request to reach the donor and for the reply to arrive. At that point, the idle
processor either becomes busy or generates another work request. Therefore, the time spent in communication subsumes the time for
which a processor is idle. Since communication overhead is the dominant overhead in parallel DFS, we now consider a method to
compute the communication overhead for each load-balancing scheme.
It is difficult to derive a precise expression for the communication overhead of the load-balancing schemes for DFS because they are
dynamic. This section describes a technique that provides an upper bound on this overhead. We make the following assumptions in the
analysis.
. The work at any processor can be partitioned into independent pieces as long as its size exceeds a threshold . 1
 and (1 w y  at one processor is partitioned into two parts: w A reasonable work-splitting mechanism is available. Assume that work
. We w a  > w ) y  and (1 - w a  > w y  0.5), such that a  (0 < a  1. Then there exists an arbitrarily small constant y  for 0 w ) y -
. 2
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 sets a lower bound on the load imbalance that results from work a -splitting. The constant a call such a splitting mechanism
 work. w a  have at least w splitting: both partitions of
 results Section 11.4.1 The first assumption is satisfied by most depth-first search algorithms. The third work-splitting strategy described in
-splitting even for highly irregular search spaces. a in
In the load-balancing schemes to be analyzed, the total work is dynamically partitioned among the processors. Processors work on disjoint
parts of the search space independently. An idle processor polls for work. When it finds a donor processor with work, the work is split and
, then assumption 2 k w  andj w , and it is split into two pieces of sizei w a part of it is transferred to the idle processor. If the donor has work
 is less than 0.5. Therefore, after a work transfer, neither a . Note thati w a  > wk  andi w a  > wj  such that a states that there is a constant
. Assume -1 p w , ..., 1 w, 0 w  pieces of work whose sizes are p  work. Suppose there arei w ) a processor (donor and recipient) has more than (1 -
 pieces of work whose sizes are given by p . If all of these pieces are split, the splitting strategy yields 2 w that the size of the largest piece is
. w ) a . Among them, the size of the largest piece is given by (1 - -1 p w ) -1 p y , ..., (1 - 1 w ) 1 y , (1 - 0 w ) 0 y , (1 - -1 p w -1 p y , ..., 1 w 1 y, 0 w 0 y
 processors and a single piece of work is assigned to each processor. If every processor receives a work request p Assume that there are
 pieces has been split at least once. Thus, the maximum work at any of the processors has been p at least once, then each of these
) work requests, each processor receives at least one work request. p  ( V ) such that, after every p  ( V ).We define a reduced by a factor of (1 -
 units of work, and all other W  has 0 P ) depends on the load-balancing algorithm. Initially, processor p  ( V . In general, p ) p  ( V Note that
) requests, the p  ( V ; after 2 W ) a ) requests, the maximum work remaining at any processor is less than (1- p  ( V processors have no work. After
) requests, the maximum work p  ( V )) W/ ( ) a 1/(1- . Similarly, after (log W 2 ) a maximum work remaining at any processor is less than (1 -
). W ) log p  ( V  ( O . Hence, the total number of work requests is remaining at any processor is below a threshold value
Communication overhead is caused by work requests and work transfers. The total number of work transfers cannot exceed the total
number of work requests. Therefore, the total number of work requests, weighted by the total communication cost of one work request and
a corresponding work transfer, gives an upper bound on the total communication overhead. For simplicity, we assume the amount of data
associated with a work request and work transfer is a constant. In general, the size of the stack should grow logarithmically with respect to
the size of the search space. The analysis for this case can be done similarly (Problem 11.3).
 is given by o T  is the time required to communicate a piece of work, then the communication overhead commt If
2  Equation 11.
 is given by E The corresponding efficiency
. As o T  and the overhead function W  we showed that the isoefficiency function can be derived by balancing the problem size Section 5.4.2 In
 is determined by the underlying architecture, and commt ). The value of p  ( V  and commt  depends on two values: o T , Equation 11.2 shown by
) for each scheme introduced in p  ( V ) is determined by the load-balancing scheme. In the following subsections, we derive p  ( V the function
) to derive the scalability of various schemes on message-passing and p  ( V . We subsequently use these values of Section 11.4.1
shared-address-space machines.
Computation of V(p) for Various Load-Balancing Schemes
V ) is an important component of the total communication overhead. In this section, we compute the value of p  ( V  shows that Equation 11.2
) for different load-balancing schemes. p (
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
) for ARR occurs when all processors issue work requests at the same time to p  ( V  The worst case value of Asynchronous Round Robin
 - 1 had all the work and that the local p the same processor. This case is illustrated in the following scenario. Assume that processor
 - 1 to receive a work request, p  - 2) were pointing to processor zero. In this case, for processor p counters of all the other processors (0 to
 - 2 work requests (to all processors p  - 2 processors generates up to p  - 1 requests while each of the remaining p one processor must issue
). Note that the actual value 2 p  ( O ) = p  ( V  - 2); that is, p  - 2)( p  - 1) + ( p ) has an upper bound of ( p  ( V  - 1 and itself). Thus, p except processor
. 2 p  and p ) is between p  ( V of
 requests, each processor has received one request. p  In GRR, all processors receive requests in sequence. After Global Round Robin
. p ) is p  ( V Therefore,
). p  ( V ) is unbounded. Hence, we compute the average-case value of p  ( V  For RR, the worst-case value of Random Polling
 boxes. In each trial, a box is chosen at random and marked. We are interested in the mean number of trials p Consider a collection of
required to mark all the boxes. In our algorithm, each trial corresponds to a processor sending another randomly selected processor a
request for work.
 boxes have not been marked. Since the next box to bei  - p  boxes have been marked, and p  of thei ) represent a state in which p , i  ( F Let
 probability that it will be an unmarked box. Hence p )/ i  - p  probability that it will be a marked box and ( p/i marked is picked at random, there is
) denote the p , i  (f . Let p )/ i  - p ) with a probability of ( p  + 1,i  ( F  and transits to state p/i ) with a probability of p , i  ( F the system remains in state
). We have p  (0,f ) = p  ( V ). Then, p , p  ( F ) to p , i  ( F average number of trials needed to change from state
Hence,
H where
). p  denotes the natural logarithm of p  (where ln p  1.69 ln p H  becomes large, p  is a harmonic number. It can be shown that, as p
). p  log p  ( O ) = p  ( V Thus,
11.4.3 Analysis of Load-Balancing Schemes
. In each case, we assume that work is Section 11.4.1 This section analyzes the performance of the load-balancing schemes introduced in
transferred in fixed-size messages (the effect of relaxing this assumption is explored in Problem 11.3).
 is m . Since the message size m wt  + st  = commt -word message in the simplified cost model is m Recall that the cost of communicating an
o T  (1) if there is no congestion on the interconnection network. The communication overhead O  = commt assumed to be a constant,
) reduces to Equation 11.2 (
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
3  Equation 11.
 for each load-balancing scheme to derive the isoefficiency function due to communication. W We balance this overhead with problem size
, communication Equation 11.3 ). Substituting into 2 p  ( O ) for ARR is p  ( V , Section 11.4.2  As discussed in Asynchronous Round Robin
, we have W ). Balancing communication overhead against problem size W  log 2 p  ( O  is given by o T overhead
 into the right-hand side of the same equation and simplifying, W Substituting
, and can be ignored. W  grows no slower than log p ) is asymptotically smaller than the first term, provided W The double-log term (log log
). p  log 2 p  ( O The isoefficiency function for this scheme is therefore given by
o T , this yields a communication overhead Equation 11.3 ) for GRR. Substituting into p  ( O ) = p  ( V , Section 11.4.2  From Global Round Robin
). p  log p  ( O ). Simplifying as for ARR, the isoefficiency function for this scheme due to communication overhead is W  log p  ( O of
 is accessed repeatedly, possibly causing contention. The number of times this variable target In this scheme, however, the global variable
O ). If the processors are used efficiently, the total execution time is W  log p  ( O is accessed is equal to the total number of work requests,
 is larger than the total time p/ W  processors. Then, p  on W  while solving a problem of size target ). Assume that there is no contention for p/ W (
) decreases, but the number p/ W during which the shared variable is accessed. As the number of processors increases, the execution time (
of times the shared variable is accessed increases. Thus, there is a crossover point beyond which the shared variable becomes a
 at a rate such that the ratio between W bottleneck, prohibiting further reduction in run time. This bottleneck can be eliminated by increasing
 as follows: p  to grow with respect to W ) remains constant. This requires W  log p  ( O  and p/ W
4  Equation 11.
). p  log 2 p  ( O . This yields an isoefficiency term of p  in terms of W  to express Equation 11.4 We can simplify
Since the isoefficiency function due to contention asymptotically dominates the isoefficiency function due to communication, the overall
). Note that although it is difficult to estimate the actual overhead due to contention for the p  log 2 p  ( O isoefficiency function is given by
shared variable, we are able to determine the resulting isoefficiency function.
, the communication Equation 11.3 ) for RP. Substituting this value into p  log p  ( O ) = p  ( V  that Section 11.4.2  We saw in Random Polling
 and simplifying as before, we derive the isoefficiency function due to W  with the problem size o T ). Equating W  log p  log p  ( O  is o T overhead
). Since there is no contention in RP, this function also gives its overall isoefficiency function. p 2  log p  ( O communication overhead as
11.4.4 Termination Detection
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
One aspect of parallel DFS that has not been addressed thus far is termination detection. In this section, we present two schemes for
. Section 11.4.1 termination detection that can be used with the load-balancing algorithms discussed in
Dijkstra's Token Termination Detection Algorithm
 processors as being p Consider a simplified scenario in which once a processor goes idle, it never receives more work. Visualize the
 initiates a token 0 P connected in a logical ring (note that a logical ring can be easily mapped to underlying physical topologies). Processor
. At any stage in the computation, if a processor receives a 1 P when it becomes idle. This token is sent to the next processor in the ring,
token, the token is held at the processor until the computation assigned to the processor is complete. On completion, the token is passed
to the next processor in the ring. If the processor was already idle, the token is passed to the next processor. Note that if at any time the
 passes its token to -1 p P  have completed their computation. Processor -1 i P , ..., 0 P  , then all processorsi P token is passed to processor
 knows that all processors have completed their computation and the algorithm can 0 P ; when it receives the token, processor 0 P processor
terminate.
Such a simple scheme cannot be applied to the search algorithms described in this chapter, because after a processor goes idle, it may
receive more work from other processors. The token termination detection scheme thus must be modified.
. Initially, all white  or black In the modified scheme, the processors are also organized into a ring. A processor can be in one of two states:
. If the only work transfers allowed in the 0 P , -1 p P , ..., 1 P , 0 P . As before, the token travels in the sequence white processors are in state
 sends workj P , then the simple termination scheme is still adequate. However, if processor j  <i  such thatj P  toi P system are from processor
 since it causes the token to go around the black  is markedj P , the token must traverse the ring again. In this case processori P to processor
 must be able to tell by looking at the token it receives whether it should be propagated around the ring again. 0 P ring again. Processor
 (or black  implies termination; and a 0 P  (or valid) token, which when received by processor white Therefore the token itself is of two types: a
invalid) token, which implies that the token must traverse the ring again. The modified termination algorithm works as follows:
 token to processor white  and sending a white  initiates termination detection by making itself 0 P When it becomes idle, processor
. 1 P
. 1
. black  becomesj P  then processori  > j  andi P  sends work to processorj P If processor . 2
black , then the color of the token is set to black  isi P . If +1 i P  is idle, then it passes the token toi P  has the token andi P If processor
, the token is passed unchanged. white  isi P . If +1 i P before it is sent to
. 3
. white  becomesi P , +1i P  passes the token toi P After . 4
 token and is itself idle. The algorithm correctly detects termination by white  receives a 0 P The algorithm terminates when processor
accounting for the possibility of a processor receiving work after it has already been accounted for by the token.
) with a small constant. For a small number of processors, this scheme can be used without a P  ( O The run time of this algorithm is
significant impact on the overall performance. For a large number of processors, this algorithm can cause the overall isoefficiency function
) (Problem 11.4). 2 p  ( O of the load-balancing scheme to be at least
Tree-Based Termination Detection
 has all the work and a weight of one 0 P Tree-based termination detection associates weights with individual work pieces. Initially processor
 retains half of the weight and gives half of it 0 P is associated with it. When its work is partitioned and sent to another processor, processor
, then after the first work transfer, bothi P  is the weight at processori w  is the recipient processor andi P to the processor receiving the work. If
 are 0.5. Each time the work at a processor is partitioned, the weight is halved. When a processor completes its computation, iti w  and 0 w
 becomes one 0 P  at processor 0 w returns its weight to the processor from which it received work. Termination is signaled when the weight
 has finished its work. 0 P and processor
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Example 11.7 Tree-based termination detection
0 = w  has all the weight ( 0 P  illustrates tree-based termination detection for four processors. Initially, processor Figure 11.10
 partitions its work and gives 0 P  = 0). In step 1, processor 3 w  = 2 w  = 1 w 1), and the weight at the remaining processors is 0 (
 gives half of its 1 P  are 0. In step 2, processor 3 w  and 2 w  are 0.5 and 1 w  and 0 w . After this step, 1 P part of it to processor
 remain 3 w  and 0 w  after this work transfer are 0.25 and the weights 2 w  and 1 w . The weights 2 P work to processor
 and the weights of all processors become 0.25. In step 4, 1 P  gets work from processor 3 P unchanged. In step 3, processor
 becomes 0.5. As 1 P  of processor 1 w . The weight 1 P  completes its work and sends its weight to processor 2 P processor
 becomes 1. At this 0 P  at processor 0 w processors complete their work, weights are propagated up the tree until the weight
point, all work has been completed and termination can be signaled.
s Figure 11.10. Tree-based termination detection. Steps 1–6 illustrate the weights at various processor
. after each work transfer
This termination detection algorithm has a significant drawback. Due to the finite precision of computers, recursive halving of the weight
may make the weight so small that it becomes 0. In this case, weight will be lost and termination will never be signaled. This condition can
.i w , instead of manipulating the weight itself, it manipulates 1/i w  has weighti P be alleviated by using the inverse of the weights. If processor
The details of this algorithm are considered in Problem 11.5.
The tree-based termination detection algorithm does not change the overall isoefficiency function of any of the search schemes we have
considered. This follows from the fact that there are exactly two weight transfers associated with each work transfer. Therefore, the
algorithm has the effect of increasing the communication overhead by a constant factor. In asymptotic terms, this change does not alter the
isoefficiency function.
11.4.5 Experimental Results
In this section, we demonstrate the validity of scalability analysis for various parallel DFS algorithms. The satisfiability problem tests the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 can be satisfiability problem validity of boolean formulae. Such problems arise in areas such as VLSI design and theorem proving. The
stated as follows: given a boolean formula containing binary variables in conjunctive normal form, determine if it is unsatisfiable. A boolean
formula is unsatisfiable if there exists no assignment of truth values to variables for which the formula is true.
The Davis-Putnam algorithm is a fast and efficient way to solve this problem. The algorithm works by performing a depth-first search of the
 be the number of literals. Then the n binary tree formed by true or false assignments to the literals in the boolean expression. Let
. If, after a partial assignment of values to literals, the formula becomes false, then the n maximum depth of the tree cannot exceed
algorithm backtracks. The formula is unsatisfiable if depth-first search fails to find an assignment to variables for which the formula is true.
 possible combinations will actually be explored. For example, for a n Even if a formula is unsatisfiable, only a small subset of the 2
 nodes are actually 7 ), but only about 10 19  (approximately 3.7 x 10 65 65-variable problem, the total number of possible combinations is 2
expanded in a specific problem instance. The search tree for this problem is pruned in a highly nonuniform fashion and any attempt to
partition the tree statically results in an extremely poor load balance.
Table 11.1. Average speedups for various load-balancing schemes.
Number of processors
1024 512 256 128 64 32 16 8 Scheme
284.425 259.372 178.92 103.738 57.721 29.664 14.936 7.506 ARR
155.051 184.828 110.754 57.729 29.291 14.734 7.384 GRR
660.582 397.585 218.255 114.645 58.857 29.814 15.000 7.524 RP
The satisfiability problem is used to test the load-balancing schemes on a message passing parallel computer for up to 1024 processors.
. This program Section 11.4.1 We implemented the Davis-Putnam algorithm, and incorporated the load-balancing algorithms discussed in
was run on several unsatisfiable formulae. By choosing unsatisfiable instances, we ensured that the number of nodes expanded by the
parallel formulation is the same as the number expanded by the sequential one; any speedup loss was due only to the overhead of load
balancing.
 and 5 In the problem instances on which the program was tested, the total number of nodes in the tree varied between approximately 10
. The depth of the trees (which is equal to the number of variables in the formula) varied between 35 and 65. Speedup was calculated 7 10
with respect to the optimum sequential execution time for the same problem. Average speedup was calculated by taking the ratio of the
cumulative time to solve all the problems in parallel using a given number of processors to the corresponding cumulative sequential time.
On a given number of processors, the speedup and efficiency were largely determined by the tree size (which is roughly proportional to the
sequential run time). Thus, speedup on similar-sized problems was quite similar.
Table 11.2. Number of requests generated for GRR and RP.
Number of processors
1024 512 256 128 64 32 16 8 Scheme
72874 41382 17088 8557 3445 1572 661 260 GRR
885872 382695 136457 46056 15060 5106 2013 562 RP
 shows the average speedup obtained by parallel Table 11.1 All schemes were tested on a sample set of five problem instances.
 presents the total Table 11.2  is a graph of the speedups obtained. Figure 11.11 algorithms using different load-balancing techniques.
 shows the corresponding graph and compares the Figure 11.12 number of work requests made by RP and GRR for one problem instance.
) for RP and GRR, respectively. p  log p  ( O ) and p 2  log p  ( O number of messages generated with the expected values
Figure 11.11. Speedups of parallel DFS using ARR, GRR and RP load-balancing schemes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
) p  log p ( O ) and p 2  log p ( O Figure 11.12. Number of work requests generated for RP and GRR and their expected values (
respectively).
) which is much worse than the isoefficiency function of RP. This is reflected in the p  log 2 p  ( O The isoefficiency function of GRR is
, we see that the performance of GRR deteriorates very rapidly for more than 256 Figure 11.11 performance of our implementation. From
 > 256 only for very large problem instances. Experimental results also show that ARR is p processors. Good speedups can be obtained for
 log 2 p  ( O more scalable than GRR, but significantly less scalable than RP. Although the isoefficiency functions of ARR and GRR are both
) is p  ( V ). This value of 2 p  ( O ) = p  ( V  is an upper bound, derived using p  log 2 p ), ARR performs better than GRR. The reason for this is that p
)) is a tight bound. p  ( O ) used for GRR ( p  ( V only a loose upper bound for ARR. In contrast, the value of
To determine the accuracy of the isoefficiency functions of various schemes, we experimentally verified the isoefficiency curves for the RP
7  nodes to 10 5 technique (the selection of this technique was arbitrary). We ran 30 different problem instances varying in size from 10
nodes on a varying number of processors. Speedup and efficiency were computed for each of these. Data points with the same efficiency
for different problem sizes and number of processors were then grouped. Where identical efficiency points were not available, the problem
size was computed by averaging over points with efficiencies in the neighborhood of the required value. These data are presented in
 for values of efficiency equal to 0.9, 0.85, 0.74, and 0.64. We expect points p 2  log p  against W , which plots the problem size Figure 11.13
 that the points are reasonably collinear, which shows Figure 11.13 corresponding to the same efficiency to be collinear. We can see from
that the experimental isoefficiency function of RP is close to the theoretically derived isoefficiency function.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Figure 11.13. Experimental isoefficiency curves for RP for different efficiencies.
11.4.6 Parallel Formulations of Depth-First Branch-and-Bound Search
Parallel formulations of depth-first branch-and-bound search (DFBB) are similar to those of DFS. The preceding formulations of DFS can
be applied to DFBB with one minor modification: all processors are kept informed of the current best solution path. The current best
solution path for many problems can be represented by a small data structure. For shared address space computers, this data structure
can be stored in a globally accessible memory. Each time a processor finds a solution, its cost is compared to that of the current best
solution path. If the cost is lower, then the current best solution path is replaced. On a message-passing computer, each processor
maintains the current best solution path known to it. Whenever a processor finds a solution path better than the current best known, it
broadcasts its cost to all other processors, which update (if necessary) their current best solution cost. Since the cost of a solution is
captured by a single number and solutions are found infrequently, the overhead of communicating this value is fairly small. Note that, if a
processor's current best solution path is worse than the globally best solution path, the efficiency of the search is affected but not its
correctness. Because of DFBB's low communication overhead, the performance and scalability of parallel DFBB is similar to that of
parallel DFS discussed earlier.
11.4.7 Parallel Formulations of IDA*
Since IDA* explores the search tree iteratively with successively increasing cost bounds, it is natural to conceive a parallel formulation in
which separate processors explore separate parts of the search space independently. Processors may be exploring the tree using
different cost bounds. This approach suffers from two drawbacks.
It is unclear how to select a threshold for a particular processor. If the threshold chosen for a processor happens to be higher
than the global minimum threshold, then the processor will explore portions of the tree that are not explored by sequential IDA*.
. 1
This approach may not find an optimal solution. A solution found by one processor in a particular iteration is not provably optimal
until all the other processors have also exhausted the search space associated with thresholds lower than the cost of the
solution found.
. 2
). All processors use the same cost bound; Section 11.4 A more effective approach executes each iteration of IDA* by using parallel DFS (
each processor stores the bound locally and performs DFS on its own search space. After each iteration of parallel IDA*, a designated
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
processor determines the cost bound for the next iteration and restarts parallel DFS with the new bound. The search terminates when a
processor finds a goal node and informs all the other processors. The performance and scalability of this parallel formulation of IDA* are
similar to those of the parallel DFS algorithm.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
11.5 Parallel Best-First Search
 list. It maintains the unexpanded open  that an important component of best-first search (BFS) algorithms is the Section 11.2.2 Recall from
 list is open -value. In the sequential algorithm, the most promising node from thel nodes in the search graph, ordered according to their
 list. open removed and expanded, and newly generated nodes are added to the
 list. These formulations differ open In most parallel formulations of BFS, different processors concurrently expand different nodes from the
 processors, the simplest strategy assigns each processor to p  list. Given open according to the data structures they use to implement the
 because each processor gets work from a centralized strategy  list. This is called the open work on one of the current best nodes on the
 list. Since this formulation of parallel BFS expands more than one node at a time, it may expand nodes that would not open single global
 list is a solution. The parallel formulation still open be expanded by a sequential algorithm. Consider the case in which the first node on the
Figure  nodes, the amount of extra work is limited. p  list. However, since it always picks the best open  nodes on the p expands the first
 illustrates this strategy. There are two problems with this approach: 11.14
 list are being open  nodes from the p The termination criterion of sequential BFS fails for parallel BFS. Since at any moment,
expanded, it is possible that one of the nodes may be a solution that does not correspond to the best goal node (or the path
 - 1 nodes may lead to search spaces containing better goal p found is not the shortest path). This is because the remaining
, then this solution is not guaranteed to correspond to the c nodes. Therefore, if the cost of a solution found by a processor is
. The termination criterion c best goal node until the cost of nodes being searched at other processors is known to be at least
must be modified to ensure that termination occurs only after the best solution has been found.
. 1
 list is accessed for each node expansion, it must be easily accessible to all processors, which can severely limit open Since the
 be the average expt  list limits speedup. Let open performance. Even on shared-address-space architectures, contention for the
n  list for a single-node expansion. If there are open  be the average time to access the accesst time to expand a single node, and
nodes to be expanded by both the sequential and parallel formulations (assuming that they do an equal amount of work), then
). Assume that it is impossible to parallelize the expansion of individual expt  + accesst ( n the sequential run time is given by
 list must be accessed at least once for each node open , because the access nt nodes. Then the parallel run time will be at least
. accesst )/ expt  + accesst expanded. Hence, an upper bound on the speedup is (
. 2
Figure 11.14. A general schematic for parallel best-first search using a centralized strategy. The
locking operation is used here to serialize queue access by various processors.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 list. Initially, the search space is open  list is to let each processor have a local open One way to avoid the contention due to a centralized
 lists of various processors. All open statically divided among the processors by expanding some nodes and distributing them to the local
the processors then select and expand nodes simultaneously. Consider a scenario where processors do not communicate with each
other. In this case, some processors might explore parts of the search space that would not be explored by the sequential algorithm. This
leads to a high search overhead factor and poor speedup. Consequently, the processors must communicate among themselves to
 list trades-off communication and computation: decreasing communication open minimize unnecessary search. The use of a distributed
 lists increases search overhead factor, and decreasing search overhead factor with increased communication open between distributed
increases communication overhead.
The best choice of communication strategy for parallel BFS depends on whether the search space is a tree or a graph. Searching a graph
incurs the additional overhead of checking for duplicate nodes on the closed list. We discuss some communication strategies for tree and
graph search separately.
Communication Strategies for Parallel Best-First Tree Search
 lists on different processors. The objective of a open A communication strategy allows state-space nodes to be exchanged between
-values are distributed evenly among processors. In this section we discussl communication strategy is to ensure that nodes with good
three such strategies, as follows.
 list of a open , each processor periodically sends some of its best nodes to the random communication strategy In the
randomly selected processor. This strategy ensures that, if a processor stores a good part of the search space, the others get
part of it. If nodes are transferred frequently, the search overhead factor can be made very small; otherwise it can become
quite large. The communication cost determines the best node transfer frequency. If the communication cost is low, it is best to
communicate after every node expansion.
. 1
, the processors are mapped in a virtual ring. Each processor periodically exchanges ring communication strategy In the
 lists of its neighbors in the ring. This strategy can be implemented on message passing open some of its best nodes with the
as well as shared address space machines with the processors organized into a logical ring. As before, the cost of
 illustrates the ring communication strategy. Figure 11.15 communication determines the node transfer frequency.
Figure 11.15. A message-passing implementation of parallel best-first search using the
. 2
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
ring communication strategy.
Unless the search space is highly uniform, the search overhead factor of this scheme is very high. The reason is that this
scheme takes a long time to distribute good nodes from one processor to all other processors.
, there is a shared blackboard through which nodes are switched among blackboard communication strategy In the
-value isl  list, a processor expands the node only if its open processors as follows. After selecting the best node from its local
within a tolerable limit of the best node on the blackboard. If the selected node is much better than the best node on the
blackboard, the processor sends some of its best nodes to the blackboard before expanding the current node. If the selected
node is much worse than the best node on the blackboard, the processor retrieves some good nodes from the blackboard and
 illustrates the blackboard communication strategy. The blackboard strategy is Figure 11.16 reselects a node for expansion.
suited only to shared-address-space computers, because the value of the best node in the blackboard has to be checked after
each node expansion.
Figure 11.16. An implementation of parallel best-first search using the blackboard
communication strategy.
. 3
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Communication Strategies for Parallel Best-First Graph Search
While searching graphs, an algorithm must check for node replication. This task is distributed among processors. One way to check for
replication is to map each node to a specific processor. Subsequently, whenever a node is generated, it is mapped to the same processor,
which checks for replication locally. This technique can be implemented using a hash function that takes a node as input and returns a
processor label. When a node is generated, it is sent to the processor whose label is returned by the hash function for that node. Upon
 list. open  lists. If not, the node is inserted in the closed  or open receiving the node, a processor checks whether it already exists in the local
If the node already exists, and if the new node has a better cost associated with it, then the previous version of the node is replaced by the
 list. open new node on the
For a random hash function, the load-balancing property of this distribution strategy is similar to the random-distribution technique
discussed in the previous section. This result follows from the fact that each processor is equally likely to be assigned a part of the search
space that would also be explored by a sequential formulation. This method ensures an even distribution of nodes with good heuristic
values among all the processors (Problem 11.10). However, hashing techniques degrade performance because each node generation
results in communication (Problem 11.11).
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
11.6 Speedup Anomalies in Parallel Search Algorithms
In parallel search algorithms, speedup can vary greatly from one execution to another because the portions of the search space examined
by various processors are determined dynamically and can differ for each execution. Consider the case of sequential and parallel DFS
 illustrates sequential DFS search. The order of node expansions is Figure 11.17(a) . Figure 11.17 performed on the tree illustrated in
. G indicated by node labels. The sequential formulation generates 13 nodes before reaching the goal node
Figure 11.17. The difference in number of nodes searched by sequential and parallel formulations of DFS. For this
example, parallel DFS reaches a goal node after searching fewer nodes than sequential DFS.
 for two processors. The nodes expanded by Figure 11.17(b) Now consider the parallel formulation of DFS illustrated for the same tree in
. The parallel formulation reaches the goal node after generating only nine nodes. That is, the parallel L  and R the processors are labeled
formulation arrives at the goal node after searching fewer nodes than its sequential counterpart. In this case, the search overhead factor is
9/13 (less than one), and if communication overhead is not too large, the speedup will be superlinear.
) generates seven nodes before reaching the Figure 11.18(a) . The sequential formulation ( Figure 11.18 Finally, consider the situation in
goal node, but the parallel formulation generates 12 nodes. In this case, the search overhead factor is greater than one, resulting in
sublinear speedup.
Figure 11.18. A parallel DFS formulation that searches more nodes than its sequential counterpart.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In summary, for some executions, the parallel version finds a solution after generating fewer nodes than the sequential version, making it
possible to obtain superlinear speedup. For other executions, the parallel version finds a solution after generating more nodes, resulting in
. acceleration anomalies  processors are referred to as p  by using p sublinear speedup. Executions yielding speedups greater than
. deceleration anomalies  processors are called p  using p Speedups of less than
 list that open Speedup anomalies also manifest themselves in best-first search algorithms. Here, anomalies are caused by nodes on the
have identical heuristic values but require vastly different amounts of search to detect a solution. Assume that two such nodes exist; node
A leads rapidly to the goal node, and node B leads nowhere after extensive work. In parallel BFS, both nodes are chosen for expansion by
different processors. Consider the relative performance of parallel and sequential BFS. If the sequential algorithm picks node A to expand,
it arrives quickly at a goal. However, the parallel algorithm wastes time expanding node B, leading to a deceleration anomaly. In contrast,
if the sequential algorithm expands node B, it wastes substantial time before abandoning it in favor of node A. However, the parallel
algorithm does not waste as much time on node B, because node A yields a solution quickly, leading to an acceleration anomaly.
11.6.1 Analysis of Average Speedup in Parallel DFS
In isolated executions of parallel search algorithms, the search overhead factor may be equal to one, less than one, or greater than one. It
is interesting to know the average value of the search overhead factor. If it is less than one, this implies that the sequential search
algorithm is not optimal. In this case, the parallel search algorithm running on a sequential processor (by emulating a parallel processor by
using time-slicing) would expand fewer nodes than the sequential algorithm on the average. In this section, we show that for a certain type
of search space, the average value of the search overhead factor in parallel DFS is less than one. Hence, if the communication overhead
is not too large, then on the average, parallel DFS will provide superlinear speedup for this type of search space.
Assumptions
We make the following assumptions for analyzing speedup anomalies:
 leaf nodes. Solutions occur only at leaf nodes. The amount of computation needed to generate each M The state-space tree has
leaf node is the same. The number of nodes generated in the tree is proportional to the number of leaf nodes generated. This is
a reasonable assumption for search trees in which each node has more than one successor on the average.
. 1
Both sequential and parallel DFS stop after finding one solution. . 2
 leaf p/ M  processors; thus, each processor gets a subtree with p In parallel DFS, the state-space tree is equally partitioned among
nodes.
. 3
There is at least one solution in the entire tree. (Otherwise, both parallel search and sequential search generate the entire tree
without finding a solution, resulting in linear speedup.)
. 4
There is no information to order the search of the state-space tree; hence, the density of solutions across the unexplored nodes
. 5
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
is independent of the order of the search.
 is defined as the probability of the leaf node being a solution. We assume a Bernoulli distribution of r The solution density
solutions; that is, the event of a leaf node being a solution is independent of any other leaf node being a solution. We also
 1. r assume that
. 6
. The p W  processors before one of the processors finds a solution is denoted by p The total number of leaf nodes generated by
 are less p W  and W . Both W average number of leaf nodes generated by sequential DFS before a solution is found is given by
. M than or equal to
. 7
Analysis of the Search Overhead Factor
 leaves. Let the density of solutions p/ M  = K  regions, each with p  leaf nodes are statically divided into M Consider the scenario in which the
 independently until a processor finds ai  searches regioni P . In the parallel algorithm, each processori r  region be thi among the leaves in the
solution. In the sequential algorithm, the regions are searched in random order.
be the solution density in a region; and assume that the number of leaves K in the region is large. r Let Theorem 11.6.1
. r  1/ , the mean number of leaves generated by a single processor searching the region is  > 0 r Then, if
 Since we have a Bernoulli distribution, the mean number of trials is given by Proof:
5  Equation 11.
 becomes small; hence, the mean number of Equation 11.5 , the second term in K  and a large value of r For a fixed value of
. r trials is approximately equal to 1/
 and searches it to find a solution. Hence, the average number of leaf p  regions with probability 1/ p Sequential DFS selects any one of the
nodes expanded by sequential DFS is
This expression assumes that a solution is always found in the selected region; thus, only one region must be searched. However, the
. In this case, another region must be searched. Taking this into account makes the K )i r  not having any solutions is (1 -i probability of region
 somewhat. The overall results of the analysis will not change. W  more precise and increases the average value of W expression for
 regions is explored simultaneously. Hence the probability of success in a step of p In each step of parallel DFS, one node from each of the
 (neglecting the second-order terms, since each p r  + ··· + 2 r  + 1 r . This is approximately the parallel algorithm is
 are assumed to be small). Hence,i r
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, and AM is their p r , ..., 2 r , 1 r , where HM is the harmonic mean of AM  = 1/ p W  and HM  = 1/ W Inspecting the above equations, we see that
. In p W W , we have HM AM arithmetic mean. Since the arithmetic mean (AM) and the harmonic mean (HM) satisfy the relation
particular:
. When solutions are uniformly distributed, the average search overhead p W W , therefore HM  = AM , p r  = ··· = 2 r  = 1 r When
factor for parallel DFS is one.
. When solution densities in various regions are nonuniform, the average p W  > W , therefore HM  > AM  is different,i r When each
search overhead factor for parallel DFS is less than one, making it possible to obtain superlinear speedups.
The assumption that each node can be a solution independent of the other nodes being solutions is false for most practical problems. Still,
the preceding analysis suggests that parallel DFS obtains higher efficiency than sequential DFS provided that the solutions are not
distributed uniformly in the search space and that no information about solution density in various regions is available. This characteristic
applies to a variety of problem spaces searched by simple backtracking. The result that the search overhead factor for parallel DFS is at
least one on the average is important, since DFS is currently the best known and most practical sequential algorithm used to solve many
important problems.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
11.7 Bibliographic Remarks
Extensive literature is available on search algorithms for discrete optimization techniques such as branch-and-bound and heuristic
]. The relationship between branch-and-bound search, dynamic programming, and heuristic search Pea84 , LW66 , KK88a search [
]. The average time complexity of heuristic search KK88b , KK83 techniques in artificial intelligence is explored by Kumar and Kanal [
]. Extensive work has been done on parallel Wil86 ] and Wilf [ Smi84 algorithms for many problems is shown to be polynomial by Smith [
formulations of search algorithms. We briefly outline some of these contributions.
Parallel Depth-First Search Algorithms
, SK89 , SK90 , Rao90 , Ran91 , MV87 , KR87b , KGR94 , KK94 , FM87 , AJM88 Many parallel algorithms for DFS have been formulated [
]. Load balancing is the central issue in parallel DFS. In this chapter, distribution of work in parallel DFS was done using stack Vor87a
, FK88 ]. An alternative scheme for work-distribution is node splitting, in which only a single node is given out [ KR87b , KGR94 splitting [
] Ran91 , FTI90
This chapter discussed formulations of state-space search in which a processor requests work when it goes idle. Such load-balancing
 schemes. In other load-balancing schemes, a processor that has work gives away part of its work receiver-initiated schemes are called
 schemes. sender-initiated to another processor (with or without receiving a request). These schemes are called
] KGR94  [ et al. ]. Kumar KGR94 , KR87b , FM87 Several researchers have used receiver-initiated load-balancing schemes in parallel DFS [
analyze these load-balancing schemes including global round robin, random polling, asynchronous round robin, and nearest neighbor.
]. KR87b , KGR94  [ et al.  is based on the papers by Kumar Section 11.4 The description and analysis of these schemes in
]. SK89 , Ran91 , PFK90 , FTI90 , FK88 Parallel DFS using sender-initiated load balancing has been proposed by some researchers [
] presented the KN91 ]. Kimura and Nobuyuki [ FTI90  propose the single-level and multilevel sender-based schemes [ et al. Furuichi
distributed tree ] present a load-balancing scheme called PFK90 , FK88 scalability analysis of these schemes. Ferguson and Korf [
 (DTS). search
]. SK90 , SK89 , Ran91 , KP92 Other techniques using randomized allocation have been presented for parallel DFS of state-space trees [
]. SK89 , RK87 Issues relating to granularity control in parallel DFS have also been explored [
Saletore and Kale [SK90] present a formulation of parallel DFS in which nodes are assigned priorities and are expanded accordingly.
They show that the search overhead factor of this prioritized DFS formulation is very close to one, allowing it to yield consistently
increasing speedups with an increasing number of processors for sufficiently large problems.
In some parallel formulations of depth-first search, the state space is searched independently in a random order by different processors
] show that such methods are useful for solving robot motion planning and Ert92 ] and Ertel [ CGK93  [ et al. ]. Challou JAM88 , JAM87[
theorem proving problems, respectively.
Most generic DFS formulations apply to depth-first branch-and-bound and IDA*. Some researchers have specifically studied parallel
, RK87 ]. Many parallel formulations of IDA* have been proposed [ EDH80 , AKR90 , AKR89 formulations of depth-first branch-and-bound [
]. MD92 , PKF92 , KS91a , RKR87
Most of the parallel DFS formulations are suited only for MIMD computers. Due to the nature of the search problem, SIMD computers
], and PKF92  [ et al. ], Powley FM92 were considered inherently unsuitable for parallel search. However, work by Frye and Myczkowski [
] showed that parallel depth-first search techniques can be developed even for SIMD computers. Karypis and MD92 Mahanti and Daniels [
] presented parallel DFS schemes for SIMD computers that are as scalable as the schemes for MIMD computers. KK94 Kumar [
] present performance results for problems FM87 Several researchers have experimentally evaluated parallel DFS. Finkel and Manber [
such as the traveling salesman problem and the knight's tour for the Crystal multicomputer developed at the University of Wisconsin.
et al. ] show linear speedups on a network of transputers for a variety of combinatorial problems. Kumar MV87 Monien and Vornberger [
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
] show linear speedups for problems such as the 15-puzzle, tautology verification, and automatic test KGR94 , AKRS91 , AKR90 , AKR89[
pattern generation for various architectures such as a 128-processor BBN Butterfly, a 128-processor Intel iPSC, a 1024-processor
] have investigated the scalability RK87 , KR87b , KGR94 , GKR91 nCUBE 2, and a 128-processor Symult 2010. Kumar, Grama, and Rao [
Section and performance of many of these schemes for hypercubes, meshes, and networks of workstations. Experimental results in
]. KGR94  are taken from the paper by Kumar, Grama, and Rao [ 11.4.5
Parallel formulations of DFBB have also been investigated by several researchers. Many of these formulations are based on maintaining
a current best solution, which is used as a global bound. It has been shown that the overhead for maintaining the current best solution
tends to be a small fraction of the overhead for dynamic load balancing. Parallel formulations of DFBB have been shown to yield near
]. AKR89 , Eck94 , Eck97 , LM97 , ST95 linear speedups for many problems and architectures [
] proposed the ring DSG83 Many researchers have proposed termination detection algorithms for use in parallel search. Dijkstra [
, is similar to the one Section 11.4.4 termination detection algorithm. The termination detection algorithm based on weights, discussed in
] discuss the termination detection algorithm based on minimum DM93 ]. Dutt and Mahapatra [ RICN88  [ et al. proposed by Rokusawa
spanning trees.
Parallel Formulations of Alpha-Beta Search
Alpha-beta search is essentially a depth-first branch-and-bound search technique that finds an optimal solution tree of an AND/OR graph
, Lin83 , HB88 , FF82 , FK88 , Bau78 , ABJ82 ]. Many researchers have developed parallel formulations of alpha-beta search [ KK88b , KK83[
, MFMV90 , FK88 ]. Some of these methods have shown reasonable speedups on dozens of processors [ PFK90 , MP85 , MFMV90 , MC82
]. PFK90
The utility of parallel processing has been demonstrated in the context of a number of games, and in particular, chess. Work on large
] in 1990. This program was capable of playing chess at Hsu90  search led to the development of Deep Thought [ b  - a scale parallel
grandmaster level. Subsequent advances in the use of dedicated hardware, parallel processing, and algorithms resulted in the
] FMM94  [ et al. ] that beat the reigning world champion Gary Kasparov. Feldmann HG97 , HCH95 development of IBM's Deep Blue [
developed a distributed chess program that is acknowledged to be one of the best computer chess players based entirely on general
purpose hardware.
Parallel Best-First Search
, Qui89 , MV87 , LK85 , KRR88 , KK84 Many researchers have investigated parallel formulations of A* and branch-and-bound algorithms [
, Vor87b , MRSR92 , Ten90 , PRV88 , PR90 , PR89 , PC89 , Rou87 , LP92 , KB57 , CJP83 , AM88 , GKP92 , Rao90 , WM84 , Vor86 , HD89a
 list. Some formulations use the centralized open ]. All these formulations use different data structures to store the HD87 , MV85 , Moh83
], the ring KRR88 , Dal87 , Vor87b ]; some use distributed strategies such as the random communication strategy [ HD87 , Moh83 strategy [
] experimentally KRR88  [ et al. ]. Kumar KRR88 ]; and the blackboard communication strategy [ WM84 , Vor86 communication strategy [
evaluated the centralized strategy and some distributed strategies in the context of the traveling salesman problem, the vertex cover
] have proposed and evaluated a number of other communication MD93 , DM93 problem and the 15-puzzle. Dutt and Mahapatra [
strategies.
] proposed parallel EHMN90  [ et al. ]. Evett MS90 Manzini analyzed the hashing technique for distributing nodes in parallel graph search [
retracting A* (PRA*), which operates under limited-memory conditions. In this formulation, each node is hashed to a unique processor. If
a processor receives more nodes than it can store locally, it retracts nodes with poorer heuristic values. These retracted nodes are
reexpanded when more promising nodes fail to yield a solution.
] analyze the performance of parallel best-first branch-and-bound (that is, A*) by using a random distribution of KZ88 Karp and Zhang [
] use Monte Carlo simulations to model the performance of parallel RDK89  [ et al. nodes for a specific model of search trees. Renolet
] present stochastic models to analyze the performance of parallel formulations of depth-first WY85 best-first search. Wah and Yu [
branch-and-bound and best-first branch-and-bound search.
] presents a parallel branch-and-cut algorithm to solve the symmetric traveling salesman problem. He also presents solutions Bix91 Bixby [
] present parallel formulations of the best-first Mil91  [ et al. of the LP relaxations of airline crew-scheduling models. Miller
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
branch-and-bound technique for solving the asymmetric traveling salesman problem on heterogeneous network computer architectures.
] presents parallel best-first branch-and-bound formulations for shared-address-space computers and uses them to Rou91 Roucairol [
solve the multiknapsack and quadratic-assignment problems.
Speedup Anomalies in Parallel Formulations of Search Algorithms
]. Lai and RKR87 , MVS86 , LW86 , Kor81 , LS84 , IYF79 Many researchers have analyzed speedup anomalies in parallel search algorithms [
] present enhancements and LS86 ] present early work quantifying speedup anomalies in best-first search. Lai and Sprague [ LS84 Sahni [
] also present an analytical model and derive characteristics of the lower-bound function LS85 extensions to this work. Lai and Sprague [
et al. ] and Wah LW86 , LW84 for which anomalies are guaranteed not to occur as the number of processors is increased. Li and Wah [
 processors) and p ] investigate dominance relations and heuristic functions and their effect on detrimental (speedup of < 1using WLY84[
] derive an upper bound on the speedup attainable by any parallel formulation of the QD86 acceleration anomalies. Quinn and Deo [
] analyze the average speedup in RK93 , RK88b branch-and-bound algorithm using the best-bound search strategy. Rao and Kumar [
parallel DFS for two separate models with and without heuristic ordering information. They show that the search overhead factor in these
]. RK93  is based on the results of Rao and Kumar [ Section 11.6.1 cases is at most one.
], FM87 Finally, many programming environments have been developed for implementing parallel search. Some examples are DIB [
]. RDK89 ], and PICOS [ WM84 ], MANIP [ SK89 Chare-Kernel [
Role of Heuristics
Heuristics form the most important component of search techniques, and parallel formulations of search algorithms must be viewed in
the context of these heuristics. In BFS techniques, heuristics focus search by lowering the effective branching factor. In DFBB methods,
heuristics provide better bounds, and thus serve to prune the search space.
Often, there is a tradeoff between the strength of the heuristic and the effective size of search space. Better heuristics result in smaller
search spaces but are also more expensive to compute. For example, an important application of strong heuristics is in the computation
]. JNS97 of bounds for mixed integer programming (MIP). Mixed integer programming has seen significant advances over the years [
Whereas 15 years back, MIP problems with 100 integer variables were considered challenging, today, many problems with up to 1000
integer variables can be solved on workstation class machines using branch-and-cut methods. The largest known instances of TSPs and
]. The presence of effective heuristics may MP93 , BMCP98 QAPs have been solved using branch-and-bound with powerful heuristics [
prune the search space considerably. For example, when Padberg and Rinaldi introduced the branch-and-cut algorithm in 1987, they
used it to solve a 532 city TSP, which was the largest TSP solved optimally at that time. Subsequent improvements to the method led to
]. More recently, using cutting planes, problems with over 7000 cities have been solved PR91 the solution of a a 2392 city problem [
] on serial machines. However, for many problems of interest, the reduced search space still requires the use of parallelism JNS97[
]. Use of powerful heuristics combined with effective parallel processing has enabled the solution of MMR95 , Rou87 , MP93 , BMCP98[
 nodes 10 ]. For example, QAP problems from the Nugent and Eschermann test suites with up to 4.8 x 10 MP93 extremely large problems [
]. Another dazzling BMCP98 (Nugent22 with Gilmore-Lawler bound) were solved on a NEC Cenju-3 parallel computer in under nine days [
demonstration of this was presented by the IBM Deep Blue. Deep blue used a combination of dedicated hardware for generating and
evaluating board positions and parallel search of the game tree using an IBM SP2 to beat the current world chess champion, Gary
]. HG97 , HCH95 Kasparov [
Heuristics have several important implications for exploiting parallelism. Strong heuristics narrow the state space and thus reduce the
concurrency available in the search space. Use of powerful heuristics poses other computational challenges for parallel processing as
well. For example, in branch-and-cut methods, a cut generated at a certain state may be required by other states. Therefore, in addition
to balancing load, the parallel branch-and-cut formulation must also partition cuts among processors so that processors working in
]. Eck97 , LM97 , BCCL95 certain LP domains have access to the desired cuts [
In addition to inter-node parallelism that has been discussed up to this point, intra-node parallelism can become a viable option if the
 has been effectively parallelized for solving et al. heuristic is computationally expensive. For example, the assignment heuristic of Pekny
]. If the degree of inter-node parallelism is small, this form of parallelism provides a desirable alternative. MP93 large instances of TSPs [
Another example of this is in the solution of MIP problems using branch-and-cut methods. Branch-and-cut methods rely on LP relaxation
]. These LP relaxations JNS97 for generating lower bounds of partially instantiated solutions followed by generation of valid inequalities [
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
constitute a major part of the overall computation time. Many of the industrial codes rely on Simplex to solve the LP problem since it can
adapt the solution to added rows and columns. While interior point methods are better suited to parallelism, they tend to be less efficient
for reoptimizing LP solutions with added rows and columns (in branch-and-cut methods). LP relaxation using Simplex has been shown to
parallelize well on small numbers of processors but efforts to scale to larger numbers of processors have not been successful. LP based
branch and bound methods have also been used for solving quadratic assignment problems using iterative solvers such as
]. These methods have been used to PLRR94 preconditioned Conjugate Gradient to approximately compute the interior point directions [
compute lower bounds using linear programs with over 150,000 constraints and 300,000 variables for solving QAPs. These and other
iterative solvers parallelize very effectively to a large number of processors. A general parallel framework for computing heuristic
]. PK95 solutions to problems is presented by Pramanick and Kuhl [
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
, as a bottleneck in the GRR target , we identified access to the global pointer, Section 11.4.1  In ] KGR94 11.1 [
load-balancing scheme. Consider a modification of this scheme in which it is augmented with message combining. This
 at processor zero are combined target scheme works as follows. All the requests to read the value of the global pointer
at intermediate processors. Thus, the total number of requests handled by processor zero is greatly reduced. This
technique is essentially a software implementation of the fetch-and-add operation. This scheme is called GRR-M (GRR
with message combining).
. Each processor is at a leaf node in a complete (logical) Figure 11.19 An implementation of this scheme is illustrated in
binary tree. Note that such a logical tree can be easily mapped on to a physical topology. When a processor wants to
, it sends a request up the tree toward processor zero. An internal node of the tree target atomically read and increment
, then forwards the message to its parent. If a request comes d holds a request from one of its children for at most time
 is the totali , the two requests are combined and sent up as a single request. If d from the node's other child within time
. i  is target number of increment requests that have been combined, the resulting increment of
Figure 11.19. Message combining and a sample implementation on an eight-processor hypercube.
 had been target The returned value at each processor is equal to what it would have been if all the requests to
serialized. This is done as follows: each combined message is stored in a table at each processor until the request is
 is sent back to an internal node, two values are sent down to the left and right target granted. When the value of
. The two values are determined from the entries in the table corresponding target children if both requested a value of
, in which the original value of Figure 11.19 to increment requests by the two children. The scheme is illustrated by
 issue requests. The total requested increment is five. After the 7 P  and 6 P , 4 P , 2 P , 0 P , and processors x  is target
 + x  + 3 and x  + 2, x  + 1, x , x  received at these processors is target messages are combined and processed, the value of
4, respectively.
Analyze the performance and scalability of this scheme for a message passing architecture.
. counter  Consider another load-balancing strategy. Assume that each processor maintains a variable called ] Lin92 11.2 [
 to zero. Whenever a processor goes idle, it searches for two counter Initially, each processor initializes its local copy of
 is greateri P  at counter  in a logical ring embedded into any architecture, such that the value of +1 i P  andi P processors
. If no such pair of processors exists, +1 i P . The idle processor then sends a work request to processor +1 i P than that at
. counter the request is sent to processor zero. On receiving a work request, a processor increments its local value of
. Analyze the scalability of this load-balancing scheme based on your +1 i P  andi P Devise algorithms to detect the pairs
 for a message passing architecture. +1 i P  andi P algorithm to detect the pairs
 The upper bound on the number of work transfers for this scheme is similar to that for GRR. Hint:
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, we assumed that the cost of Section 11.4.2  In the analysis of various load-balancing schemes presented in 11.3
transferring work is independent of the amount of work transferred. However, there are problems for which the
work-transfer cost is a function of the amount of work transferred. Examples of such problems are found in tree-search
applications for domains in which strong heuristics are available. For such applications, the size of the stack used to
represent the search tree can vary significantly with the number of nodes in the search tree.
. Assume  nodes varies as w Consider a case in which the size of the stack for representing a search space of
that the load-balancing scheme used is GRR. Analyze the performance of this scheme for a message passing
architecture.
. Show that the contribution of Section 11.4.4  Consider Dijkstra's token termination detection scheme described in 11.4
). Comment on the value of the 2 p  ( O termination detection using this scheme to the overall isoefficiency function is
constants associated with this isoefficiency term.
. In this algorithm, the weights may become Section 11.4.4  Consider the tree-based termination detection scheme in 11.5
very small and may eventually become zero due to the finite precision of computers. In such cases, termination is
never signaled. The algorithm can be modified by manipulating the reciprocal of the weight instead of the weight itself.
Write the modified termination algorithm and show that it is capable of detecting termination correctly.
 Consider a termination detection algorithm in which a spanning tree of minimum diameter is mapped onto ] DM93 11.6 [
 of such a tree is a vertex with the minimum distance to the center the architecture of the given parallel computer. The
vertex farthest from it. The center of a spanning tree is considered to be its root.
. The termination detection algorithm requires all busy  or idle While executing parallel search, a processor can be either
 message. A processor is busy if it has work, or if it has sent ack work transfers in the system to be acknowledged by an
 message has not been received; otherwise the processor is idle. ack work to another processor and the corresponding
 messages to their parent when they become idle. Processors stop Processors at the leaves of the spanning tree send
 messages stop  message on to their parents when they have received stop at intermediate levels in the tree pass the
 messages from all its stop from all their children and they themselves become idle. When the root processor receives
children and becomes idle, termination is signaled.
 message to its parent, a processor signals stop Since it is possible for a processor to receive work after it has sent a
 message moves up the tree until it resume  message to its parent. The resume that it has received work by sending a
stop  message nullifies the resume  message, the stop  message. On meeting the stop meets the previously issued
 message is then sent to the processor that transferred part of its work. ack message. An
Show using examples that this termination detection technique correctly signals termination. Determine the
. p isoefficiency term due to this termination detection scheme for a spanning tree of depth log
 Consider the single-level load-balancing scheme which works as follows: a designated processor ] KN91 , FTI90 11.7 [
 generates many subtasks and gives them one-by-one to the requesting processors on demand. The manager called
 traverses the search tree depth-first to a predetermined cutoff depth and distributes nodes at that depth as manager
subtasks. Increasing the cutoff depth increases the number of subtasks, but makes them smaller. The processors
request another subtask from the manager only after finishing the previous one. Hence, if a processor gets subtasks
. If the cutoff depth is large enough, this manager corresponding to large subtrees, it will send fewer requests to the
scheme results in good load balance among the processors. However, if the cutoff depth is too large, the subtasks
. In this manager given out to the processors become small and the processors send more frequent requests to the
 illustrates the Figure 11.20  becomes a bottleneck. Hence, this scheme has a poor scalability. manager case, the
single-level work-distribution scheme.
Figure 11.20. The single-level work-distribution scheme for tree search.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Assume that the cost of communicating a piece of work between any two processors is negligible. Derive analytical
expressions for the scalability of the single-level load-balancing scheme.
 Consider the multilevel work-distribution scheme that circumvents the subtask generation ] KN91 , FTI90 11.8 [
bottleneck of the single-level scheme through multiple-level subtask generation. In this scheme, processors are
. The task of top-level subtask generation is given to the root processor. It divides d -ary tree of depth m arranged in an
the task into super-subtasks and distributes them to its successor processors on demand. These processors subdivide
the super-subtasks into subtasks and distribute them to successor processors on request. The leaf processors
repeatedly request work from their parents as soon as they have finished their previous work. A leaf processor is
 = 1, the multi- d allocated to another subtask generator when its designated subtask generator runs out of work. For
and single-level schemes are identical. Comment on the performance and scalability of this scheme.
 scheme in which processors are allocated to separate parts of the distributed tree search  Consider the ] FK88 11.9 [
search tree dynamically. Initially, all the processors are assigned to the root. When the root node is expanded (by one
of the processors assigned to it), disjoint subsets of processors at the root are assigned to each successor, in
accordance with a selected processor-allocation strategy. One possible processor-allocation strategy is to divide the
processors equally among ancestor nodes. This process continues until there is only one processor assigned to a
node. At this time, the processor searches the tree rooted at the node sequentially. If a processor finishes searching
the search tree rooted at the node, it is reassigned to its parent node. If the parent node has other successor nodes still
being explored, then this processor is allocated to one of them. Otherwise, the processor is assigned to its parent. This
process continues until the entire tree is searched. Comment on the performance and scalability of this scheme.
 Consider a parallel formulation of best-first search of a graph that uses a hash function to distribute nodes to 11.10
). The performance of this scheme is influenced by two factors: the communication cost and Section 11.5 processors (
the number of "good" nodes expanded (a "good" node is one that would also be expanded by the sequential
algorithm). These two factors can be analyzed independently of each other.
Assuming a completely random hash function (one in which each node has a probability of being hashed to a
), show that the expected number of nodes expanded by this parallel formulation differs from the p processor equal to 1/
). Assuming that the cost of communicating a node from p optimal number by a constant factor (that is, independent of
 (1), derive the isoefficiency function of this scheme. O one processor to another is
 For the parallel formulation in Problem 11.10, assume that the number of nodes expanded by the sequential and 11.11
parallel formulations are the same. Analyze the communication overhead of this formulation for a message passing
architecture. Is the formulation scalable? If so, what is the isoefficiency function? If not, for what interconnection
network would the formulation be scalable?
 Note that a fully random hash function corresponds to an all-to-all personalized communication operation, which Hint:
is bandwidth sensitive.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 12. Dynamic Programming
 (DP) is a commonly used technique for solving a wide variety of discrete optimization problems such as Dynamic programming
scheduling, string-editing, packaging, and inventory management. More recently, it has found applications in bioinformatics in matching
sequences of amino-acids and nucleotides (the Smith-Waterman algorithm). DP views a problem as a set of interdependent
subproblems. It solves subproblems and uses the results to solve larger subproblems until the entire problem is solved. In contrast to
divide-and-conquer, where the solution to a problem depends only on the solution to its subproblems, in DP there may be
interrelationships across subproblems. In DP, the solution to a subproblem is expressed as a function of solutions to one or more
subproblems at the preceding levels.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
12.1 Overview of Dynamic Programming
We start our discussion with a simple DP algorithm for computing shortest paths in a graph.
Example 12.1 The shortest-path problem
Consider a DP formulation for the problem of finding a shortest (least-cost) path between a pair of vertices in an acyclic
, i ( c  has cost j  to nodei  for an introduction to graph terminology.) An edge connecting node Section 10.1 graph. (Refer to
 - 1, and n  nodes numbered 0, 1, ..., n . The graph contains ) = j , i ( c  are not connected then j  andi ). If two vertices j
. The shortest-path problem is to find a least-cost path between nodes 0 and j  <i  only if j  to nodei has an edge from node
 - 1) solves n  (f  (0) is zero, and findingf . Thus, x ) denote the cost of the least-cost path from node 0 to node x  (f  - 1. Let n
): x  (f the problem. The DP formulation for this problem yields the following recursive equations for
1  Equation 12.
 (4). Itf . The problem is to find Figure 12.1 As an instance of this algorithm, consider the five-node acyclic graph shown in
 (2). More precisely,f  (3) andf can be computed given
Figure 12.1. A graph for which the shortest path between nodes 0 and 4 is to be
computed.
 (1)f  (3) depends onf  (4) depends. Similarly,f  (3) are elements of the set of subproblems on whichf  (2) andf Therefore,
f  (2), which are used to solvef  (1) andf  (0) is known, it is used to solvef  (0). Sincef  (2) depend onf  (1) andf  (2),andf and
(3).
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
In general, the solution to a DP problem is expressed as a minimum (or maximum) of possible alternate solutions. Each of these alternate
, ..., 2 x , 1 x  represents the cost of a solution composed of subproblems r solutions is constructed by composing one or more subproblems. If
 can be written as r , then l x
, and its nature depends on the problem. If the optimal solution to each problem is composition function  is called the g The function
determined by composing optimal solutions to the subproblems and selecting the minimum (or maximum), the formulation is said to be a
 is the minimum 8 x  illustrates an instance of composition and minimization of solutions. The solution to problem Figure 12.2 DP formulation.
. The cost of the first solution is determined by composing solutions to 3 r , and 2 r , 1 r of the three possible solutions having costs
, and the third solution by composing 5 x  and 4 x , the second solution by composing solutions to subproblems 3 x  and 1 x subproblems
. 7 x , and 6 x , 2 x solutions to subproblems
). 8 x  (f Figure 12.2. The computation and composition of subproblem solutions to solve problem
DP represents the solution to an optimization problem as a recursive equation whose left side is an unknown quantity and whose right side
. In optimization equation  or an functional equation is a minimization (or maximization) expression. Such an equation is called a
). This function is additive, since it is the sum of two terms. In a general DP x , j( c ) + j  (f  is given by g , the composition function Equation 12.1
)) yields a j  (f formulation, the cost function need not be additive. A functional equation that contains a single recursive term (for example,
 DP formulation. For an arbitrary DP formulation, the cost function may contain multiple recursive terms. DP formulations whose monadic
 formulations. polyadic cost function contains multiple recursive terms are called
The dependencies between subproblems in a DP formulation can be represented by a directed graph. Each node in the graph represents
 is used to computei  indicates that the solution to the subproblem represented by node j  to nodei a subproblem. A directed edge from node
. If the graph is acyclic, then the nodes of the graph can be organized into levels such j the solution to the subproblem represented by node
that subproblems at a particular level depend only on subproblems at previous levels. In this case, the DP formulation can be categorized
 DP serial as follows. If subproblems at all levels depend only on the results at the immediately preceding levels, the formulation is called a
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 DP formulation. nonserial formulation; otherwise, it is called a
nonserial , serial polyadic , serial monadic Based on the preceding classification criteria, we define four classes of DP formulations:
. These classes, however, are not exhaustive; some DP formulations cannot be classified into any of nonserial polyadic , and monadic
these categories.
Due to the wide variety of problems solved using DP, it is difficult to develop generic parallel algorithms for them. However, parallel
formulations of the problems in each of the four DP categories have certain similarities. In this chapter, we discuss parallel DP
formulations for sample problems in each class. These samples suggest parallel algorithms for other problems in the same class. Note,
however, that not all DP problems can be parallelized as illustrated in these examples.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
12.2 Serial Monadic DP Formulations
We can solve many problems by using serial monadic DP formulations. This section discusses the shortest-path problem for a multistage
graph and the 0/1 knapsack problem. We present parallel algorithms for both and point out the specific properties that influence these
parallel formulations.
12.2.1 The Shortest-Path Problem
 + 1.i  is connected to every node at leveli . Each node at level Figure 12.3  + 1 levels, as shown in r Consider a weighted multistage graph of
S  nodes. We refer to the node at level zero as the starting node n  contain only one node, and every other level contains r Levels zero and
 node at level thi . The R  to S . The objective of this problem is to find the shortest path from R  as the terminating node r and the node at level
R . The cost of reaching the goal node  is labeled  to node . The cost of an edge connecting  in the graph is labeledl
. l  is referred to as C , the vector l  nodes at level n . If there are  is represented by from any node
. The structure of the . Since the graph has only one starting node, 0 The shortest-path problem reduces to computing C
 - 1). The cost of any such path is the sum of the cost of n j  (0  includes a node R  to graph is such that any path from
, the ). Thus,  (which is given by R  and  and the cost of the shortest path between  and the edge between
 + 1. Therefore,l , is equal to the minimum cost over all paths through each node in level R  and cost of the shortest path between
2  Equation 12.
Figure 12.3. An example of a serial monadic DP formulation for finding the shortest path in a
graph whose nodes can be organized into levels.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Hence,  is equal to , the cost r  at level R  have only one edge connecting them to the goal node Since all nodes
3  Equation 12.
 contains only one recursive term in its right-hand side, it is a monadic formulation. Note that the solution to a Equation 12.2 Because
subproblem requires solutions to subproblems only at the immediately preceding level. Consequently, this is a serial monadic formulation.
 - 1) is r  <l  (0 <l  from any node at level R Using this recursive formulation of the shortest-path problem, the cost of reaching the goal node
Now consider the operation of multiplying a matrix with a vector. In the matrix-vector product, if the addition operation is replaced by
minimization and the multiplication operation is replaced by addition, the preceding set of equations is equivalent to
4  Equation 12.
n  is an +1 l, l M  + 1, andl  andl  x 1 vectors representing the cost of reaching the goal node from each node at levels n  are +1 l  and Cl where C
 + 1. This matrix isl  at level j  to nodel  at leveli ) stores the cost of the edge connecting node j , i  matrix in which entry ( n x
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The shortest-path problem has thus been reformulated as a sequence of matrix-vector multiplications. On a sequential computer, the DP
 is 0 . Finally, C Equation 12.4  -2 using r  = 1, 2, ..., k  for -1 k - r , and then computes C Equation 12.3  from -1 r formulation starts by computing C
. Equation 12.2 computed using
). The parallel algorithm for this problem can be derived 2 n ( Q  isl  nodes at each level, the cost of computing each vector C n Since there are
) processing elements can compute n ( Q . For example, Section 8.1 using the parallel algorithms for the matrix-vector product discussed in
 is the number of levels in the graph. r  Recall that. ) rn ( Q ) and solve the entire problem in time n ( Q  in timel each vector C
Many serial monadic DP formulations with dependency graphs identical to the one considered here can be parallelized using a similar
parallel algorithm. For certain dependency graphs, however, this formulation is unsuitable. Consider a graph in which each node at a level
. In  contains many elements with value +1 l, l M can be reached from only a small fraction of nodes at the previous level. Then matrix
 =  + x , x  is considered to be a sparse matrix for the minimization and addition operations. This is because, for all M this case, matrix
. If . Therefore, the addition and minimization operations need not be performed for entries whose value is x } = , x , and min{
we use a regular dense matrix-vector multiplication algorithm, the computational complexity of each matrix-vector multiplication becomes
significantly higher than that of the corresponding sparse matrix-vector multiplication. Consequently, we must use a sparse matrix-vector
multiplication algorithm to compute each vector.
12.2.2 The 0/1 Knapsack Problem
 objects numbered 1, n  and a set of c A one-dimensional 0/1 knapsack problem is defined as follows. We are given a knapsack of capacity
] be a solution vector in which n v , ..., 2 v , 1 v  = [ v . Object profits and weights are integers. Let i p  and profiti w  has weighti . Each object n 2, ...,
 = 1 if it is in the knapsack. The goal is to find a subset of objects to put into the knapsack soi v  is not in the knapsack, andi  = 0 if objecti v
that
(that is, the objects fit into the knapsack) and
is maximized (that is, the profit is maximized).
 objects and choose the one that fits into the n  possible subsets of the n A straightforward method to solve this problem is to consider all 2
] x , i  [ F ). Let n  / n  (2 O  = c knapsack and maximizes the profit. Here we provide a DP formulation that is faster than the simple method when
] is the solution to the problem. The DP c , n  [ F }. Theni  using only objects {1, 2, ..., x be the maximum profit for a knapsack of capacity
formulation for this problem is as follows:
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, the decision to include x This recursive equation yields a knapsack of maximum profit. When the current capacity of the knapsack is
 , and profit is unchanged; (ii) the object x  can lead to one of two situations: (i) the object is not included, knapsack capacity remainsi object
. The DP algorithm decides whether or not to include an object i p , and profit increases by i w  - x is included, knapsack capacity becomes
based on which choice leads to maximum profit.
. The table is constructed in row-major order. The c  x n  of size F The sequential algorithm for this DP formulation maintains a table
algorithm first determines the maximum profit by using only the first object with knapsacks of different capacities. This corresponds to filling
the first row of the table. Filling entries in subsequent rows requires two entries from the previous row: one from the same column and one
].i w  - j  - 1,i  [ F ] and j  - 1,i  [ F ] requires j , i  [ F from the column offset by the weight of the object. Thus, the computation of an arbitrary entry
). nc ( Q . Computing each entry takes constant time; the sequential run time of this algorithm is Figure 12.4 This is illustrated in
 for the 0/1 knapsack problem. The computation of F Figure 12.4. Computing entries of table
i[ F ] and j  - 1,i[ F ] requires communication with processing elements containing entries j , i[ F entry
].i w  - j - 1,
. Computation of n  = 1, 2, ...,i  levels for n ] are organized into x , i  [ F This formulation is a serial monadic formulation. The subproblems
 - 1. Hence the formulation is serial. The formulation is monadic becausei  depends only on the subproblems at leveli problems in level
] depends on only one subproblem. Furthermore, dependencies between levels are sparse x , i  [ F each of the two alternate solutions of
because a problem at one level depends only on two subproblems from previous level.
r P . Processing element -1 c P  to 0 P  processing elements labeled c Consider a parallel formulation of this algorithm on a CREW PRAM with
] r  - 1, j  [ F  requires the values -1 r P , processing element j ] during iteration r , j  [ F . When computing F  column of matrix th r  computes the -1
] also requires constant r , j  [ F  in constant time, so computing F  can read any element of matrix -1 r P ]. Processing element wj  - r  - 1, j  [ F and
c ). The formulation uses n ( Q  iterations, the parallel run time is n time. Therefore, each iteration takes constant time. Since there are
). Therefore, the algorithm is cost-optimal. nc ( Q processing elements, hence its processor-time product is
 is distributed among the F -processing elements. Table c Let us now consider its formulation on a distributed memory machine with
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Each processing Figure 12.4 processing elements so that each processing element is responsible for one column. This is illustrated in
] is r  - 1, j  [ F , -1 r P ] at processing element r , j  [ F  iteration, for computing th j element locally stores the weights and profits of all objects. In the
 -shift operation j w ] must be fetched from another processing element. This corresponds to the circular j w  - r  - 1, j  [ F available locally but
 for a p ) log m wt  + st  processing elements is bounded by ( p . The time taken by this circular shift operation on Section 4.6 described in
, this time is given c  = p  on a network with adequate bandwidth. Since the size of the message is one word and we have m message of size
n . Since there are c ) log wt  + st  + ( ct , then each iteration takes time ct . If the sum and maximization operations take time c ) log wt  + st by (
); therefore, the algorithm c  log nc  ( O ). The processor-time product for this formulation is c  log n  ( O such iterations, the total time is given by
is not cost-optimal.
-processing elements, each p Let us see what happens to this formulation as we increase the number of elements per processor. Using
 computes the values of 0 P  iteration, processing element th j  elements of the table in each iteration. In the p/ c processing element computes
], and so on. Computing the p/ c , 2 j  [ F  + 1], ..., p/ c , j  [ F  computes values of elements 1 P ], processing element p/ c , j  [ F , 1], ..., j  [ F elements
 table can be fetched from remote F ]. Required values of the wj  - k  - 1, j  [ F ] and k  - 1, j  [ F , requires values k ], for any k , j  [ F value of
, the required nonlocal values may be available p  and j w processing elements by performing a circular shift. Depending on the values of
 irrespective of whether p/ c from one or two processing elements. Note that the total number of words communicated via these messages is
 is large and the p/ c ) assuming that p/ c wt  + st they come from one or two processing elements. The time for this operation is at most (2
 such elements, the total time for each iteration p/ c ). Since each processing element computes Section 4.6 network has enough bandwidth (
). In asymptotic terms, this p/ c wt  + st  + 2 p/ c ct ( n  iterations is n . Therefore, the parallel run time of the algorithm for p/ c wt  + st  + 2 p/ c ct is
), which is cost-optimal. nc  ( O ). Its processor-time product is p/ nc  ( O algorithm's parallel run time is
There is an upper bound on the efficiency of this formulation because the amount of data that needs to be communicated is of the same
 (Problem 12.1). ct  and wt order as the amount of computation. This upper bound is determined by the values of
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
12.3 Nonserial Monadic DP Formulations
The DP algorithm for determining the longest common subsequence of two given sequences can be formulated as a nonserial monadic
DP formulation.
12.3.1 The Longest-Common-Subsequence Problem
> is a z , b , a . For example, < A >, a subsequence of A can be formed by deleting some entries from n a  ..., 2, a 1, a  = < A Given a sequence
 (LCS) problem can be stated as longest-common-subsequence > are not. The l , d , a > and < z , c , a >, but < z , r , b , d , a , c subsequence of <
. B  and A , find the longest sequence that is a subsequence of both > m b , ..., 2 b , 1 b  < B =  and > n a  ..., 2, a 1, a  = < A follows. Given two sequences
>. z , b , a  is < B  and A >, the longest common subsequence of z , b , s , a  = < B > and z , r , b , d , a , c  = < A For example, if
. The objective of the B  elements of j  and the first A  elements ofi ] denote the length of the longest common subsequence of the first j , i  [ F Let
] j  - 1,i  [ F  - 1], and j , i  [ F  - 1], j  - 1,i  [ F ] in terms of j , i  [ F ]. The DP formulation for this problem expresses m , n  [ F LCS problem is to determine
as follows:
 , consider two pointers pointing to the start of the sequences. If the entries pointed to by the two pointers are B  and A Given sequences
identical, then they form components of the longest common subsequence. Therefore, both pointers can be advanced to the next entry of
the respective sequences and the length of the longest common subsequence can be incremented by one. If the entries are not identical
 and the sequence A then two situations arise: the longest common subsequence may be obtained from the longest subsequence of
B ; or the longest subsequence may be obtained from the longest subsequence of B obtained by advancing the pointer to the next entry of
. Since we want to determine the longest subsequence, the A and the sequence obtained by advancing the pointer to the next entry of
maximum of these two must be selected.
 in row-major order. Since there is a constant amount F The sequential implementation of this DP formulation computes the values in table
). This DP formulation is nonserial monadic, as nm ( Q of computation at each entry in the table, the overall complexity of this algorithm is
. Treating nodes along a diagonal as belonging to one level, each node depends on two subproblems at the Figure 12.5(a) illustrated in
preceding level and one subproblem two levels earlier. The formulation is monadic because a solution to any subproblem at a level is a
] are j  - 1,i  [ F  - 1] and j , i  [ F , both Equation 12.5 function of only one of the solutions at preceding levels. (Note that, for the third case in
 shows that this problem has a very Figure 12.5 ] is the maximum of the two.) j , i  [ F ], and the optimal solution to j , i  [ F possible solutions to
regular structure.
 for the longest-common-subsequence problem. Computation proceeds F Figure 12.5. (a) Computing entries of table
along the dotted diagonal lines. (b) Mapping elements of the table to processing elements.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Example 12.2 Computing LCS of two amino-acid sequences
. For the interested P A W H E A E  and H E A G A W G H E E Let us consider the LCS of two amino-acid sequences
reader, the names of the corresponding amino-acids are A: Alanine, E: Glutamic acid, G: Glycine, H: Histidine, P: Proline,
. The LCS of the two Figure 12.6  entries for these two sequences is shown in F and W: Tryptophan. The table of
. A W H E E sequences, as determined by tracing back from the maximum score and enumerating all the matches, is
. P A W H E A E  and H E A G A W G H E E  table for computing the LCS of sequences F Figure 12.6. The
. Consider a parallel formulation of this m  = n To simplify the discussion, we discuss parallel formulation only for the case in which
. Table entries are F  column of table thi  computes thei P  processing elements. Each processing element n algorithm on a CREW PRAM with
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 processing elements, and each processing n computed in a diagonal sweep from the top-left to the bottom-right corner. Since there are
n , the elements of each diagonal are computed in constant time (the diagonal can contain at most F element can access any entry in table
 The algorithm is. ) n ( Q ) iterations. Thus, the parallel run time is n ( Q  - 1 such diagonals, the algorithm requires n elements). Since there are 2
) processor-time product equals the sequential complexity. 2 n ( Q cost-optimal, since its
 among different processing F  processing elements by distributing table n This algorithm can be adapted to run on a logical linear array of
Section elements. Note that this logical topology can be mapped to a variety of physical architectures using embedding techniques in
 are assigned to processing elements as illustrated in F  + 1)th column of the table. Entries in tablei  stores the (i P . Processing element 2.7.1
j , i  [ F  - 1] or the value of j  - 1,i  [ F  may need either the value of -1 j P ], processing element j , i  [ F . When computing the value of Figure 12.5(b)
 to communicate a single word from a neighboring processing element. To wt  + st - 1] from the processing element to its left. It takes time
compute each entry in the table, a processing element needs a single value from its immediate neighbor, followed by the actual
wt  + st . Since each processing element computes a single entry on the diagonal, each iteration takes time ( ct computation, which takes time
 - 1) diagonal sweeps (iterations) across the table; thus, the total parallel run time is n ). The algorithm makes (2 ct +
, the efficiency of this algorithm is ct 2 n Since the sequential run time is
A careful examination of this expression reveals that it is not possible to obtain efficiencies above a certain threshold. To compute this
 = 0. In this case, the wt  = st threshold, assume it is possible to communicate values between processing elements instantaneously; that is,
efficiency of the parallel algorithm is
5  Equation 12.
Thus, the efficiency is bounded above by 0.5. This upper bound holds even if multiple columns are mapped to a processing element.
Higher efficiencies are possible using alternate mappings (Problem 12.3).
 can be partitioned so computing F Note that the basic characteristic that allows efficient parallel formulations of this algorithm is that table
each element requires data only from neighboring processing elements. In other words, the algorithm exhibits locality of data access.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
12.4 Serial Polyadic DP Formulations
Floyd's algorithm for determining the shortest paths between all pairs of nodes in a graph can be reformulated as a serial polyadic DP
formulation.
12.4.1 Floyd's All-Pairs Shortest-Paths Algorithm
, i c  has a weight E  in j  to nodei . An edge from node E  and a set of edges V , which consists of a set of nodes G Consider a weighted graph
). The cost of a path Section 10.4.2  ( V ) in j , i  of the shortest path between each pair of nodes ( j , i d . Floyd's algorithm determines the cost j
is the sum of the weights of the edges in the path.
. The functional equation of the DP -1 k v , ..., 1 v , 0 v , using only nodes j  to nodei  be the minimum cost of a path from node Let
formulation for this problem is
6  Equation 12.
 andi  nodes, it is also the cost of the overall shortest path between nodes n  using all j  to nodei  is the shortest path from node Since
). Thus, the overall run time of the 2 n ( Q  iterations, and each iteration requires time n  . The sequential formulation of this algorithm requires j
). 3 n ( Q sequential algorithm is
 + 1 k . Elements at level k  levels, one for each value of n  can be partitioned into  is a serial polyadic formulation. Nodes Equation 12.6
. Hence, the formulation is serial. The formulation is polyadic since one of the solutions to k depend only on elements at level
 from the previous level. Furthermore, the dependencies  and requires a composition of solutions to two subproblems
 requires only three results from the preceding level (out between levels are sparse because the computation of each element in
). 2 n of
 processing elements. Processing elements are organized into a logical 2 n A simple CREW PRAM formulation of this algorithm uses
, processing k . In each iteration n  = 1, 2, ..., k  for  computes the value of j, i P two-dimensional array in which processing element
 in constant time. . Given these values, it computes the value of , and ,  requires the values j, i P element
). This formulation is cost-optimal because its processor-time product is n ( Q Therefore, the PRAM formulation has a parallel run time of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). This algorithm can be adapted to various practical architectures to yield efficient parallel 3 n ( Q the same as the sequential run time of
). Section 10.4.2 formulations (
As with serial monadic formulations, data locality is of prime importance in serial polyadic formulations since many such formulations
have sparse connectivity between levels.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
12.5 Nonserial Polyadic DP Formulations
In nonserial polyadic DP formulations, in addition to processing subproblems at a level in parallel, computation can also be pipelined to
increase efficiency. We illustrate this with the optimal matrix-parenthesization problem.
12.5.1 The Optimal Matrix-Parenthesization Problem
 columns. The order in whichi r  rows and -1 i r  is a matrix withi A , where each n A , ..., 2 A , 1 A  matrices, n Consider the problem of multiplying
the matrices are multiplied has a significant impact on the total number of operations required to evaluate the product.
Example 12.3 Optimal matrix parenthesization
 of dimensions 10 x 20, 20 x 30, and 30 x 40, respectively. The product of these 3 A , and 2 A , 1 A Consider three matrices
) requires 10 x 20 2 A  x 1 A , computing ( 3 A ) x 2 A  x 1 A ). In ( 3 A  x 2 A  x ( 1 A  or as 3 A ) x 2 A  x 1 A matrices can be computed as (
 requires 10 x 30 x 40 additional 3 A x 30 operations and yields a matrix of dimensions 10 x 30. Multiplying this by
 x 1 A operations. Therefore the total number of operations is 10 x 20 x 30 + 10 x 30 x 40 = 18000. Similarly, computing
) requires 20 x 30 x 40 + 10 x 20 x 40 = 32000 operations. Clearly, the first parenthesization is desirable. 3 A  x 2 A (
The objective of the parenthesization problem is to determine a parenthesization that minimizes the number of operations. Enumerating all
possible parenthesizations is not feasible since there are exponentially many of them.
. This chain of matrices can be expressed as a product of two smaller j A  ,...,i A ] be the optimal cost of multiplying the matrices j , i  [ C Let
j A  ,..., +1 k A , and the chain k r  x -1 i r  results in a matrix of dimensions k A  ,..., +1 i A , i A . The chain j A  ,..., +1 k A  and k A  +1,...,i A , i A chains,
, i A . Hence, the cost of the parenthesization ( jr k r -1 i r . The cost of multiplying these two matrices is jr  x k r results in a matrix of dimensions
. This gives rise to the following recurrence relation for the jr k r -1 i r ] + j  + 1, k  [ C ] + k , i  [ C ) is given by j A  ,..., +1 k A )( k A  +1 ,...,i A
parenthesization problem:
7  Equation 12.
. Figure 12.7 ]. The composition of costs of matrix chains is shown in n  [1, C , the problem reduces to finding the value of Equation 12.7 Given
]. The algorithm fills j , i  [ C  that stores the values C  can be solved if we use a bottom-up approach for constructing the table Equation 12.7
 in an order corresponding to solving the parenthesization problem on matrix chains of increasing length. Visualize this by thinking C table
 + 1. Froml  corresponds to the cost of multiplying matrix chains of lengthl ). Entries in diagonal Figure 12.8 of filling in the table diagonally (
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 -1. j  toi  can take values from k }, where jr k r -1 i r ]+ j  +1, k  [ C ]+ k , i  [ C ] is computed as min{ j , i  [ C , we can see that the value of Equation 12.7
, ct ) terms and select their minimum. The computation of each term takes time i  - j ] requires that we evaluate ( j , i  [ C Therefore, computing
. c lt  can be computed in timel . Thus, each entry in diagonal ct ) i  - j ] takes time ( j , i  [ C and the computation of
Figure 12.7. A nonserial polyadic DP formulation for finding an optimal matrix parenthesization
for a chain of four matrices. A square node represents the optimal cost of multiplying a matrix
chain. A circle node represents a possible parenthesization.
Figure 12.8. The diagonal order of computation for the optimal matrix-parenthesization problem.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 - n  - 1) chains of length two. This takes time ( n In computing the cost of the optimal parenthesization sequence, the algorithm computes (
. n . In the final step, the algorithm computes one chain of length ct  - 2)2 n  - 2) chains of length three takes time ( n . Similarly, computing ( ct 1)
. Thus, the sequential run time of this algorithm is ct  - 1) n This takes time (
8  Equation 12.
). 3 n ( Q The sequential complexity of the algorithm is
, each processing element computes l  processing elements. In step n Consider the parallel formulation of this algorithm on a logical ring of
 illustrates the Figure 12.8 . C  + 1)th column of Tablei  computes the (i P  diagonal. Processing element thl a single element belonging to the
, each C partitioning of the table among different processing elements. After computing the assigned value of the element in table
). Therefore, the assigned Section 4.2 processing element sends its value to all other processing elements using an all-to-all broadcast (
 because it corresponds to c lt  takes timel  during iteration C value in the next iteration can be computed locally. Computing an entry in table
 - 1) n ( wt  + n  log st  processing elements takes time n  + 1. An all-to-all broadcast of a single word onl the cost of multiplying a chain of length
 - 1). The parallel run time is the sum of n ( wt  + n  log st  + c lt  isl ). The total time required to compute the entries along diagonal Section 4.2 (
 - 1 diagonals. n the time taken over computation of
), which is the same as the sequential complexity, 3 n ( Q ). Since the processor-time product is 2 n ( Q The parallel run time of this algorithm is
this algorithm is cost-optimal.
 nodes in a diagonal, each processing n ) organized in a logical ring, if there are n p  processing elements (1 p When using
] of the entries assigned to it. After computation, an all-to-all j , i  [ C  nodes. Each processing element computes the cost p/ n element stores
broadcast sends the solution costs of the subproblems for the most recently computed diagonal to all the other processing elements.
Because each processing element has complete information about subproblem costs at preceding diagonals, no other communication is
 entries of p/ n . The time to compute n wt  + p  log st p  - 1)/ p ( n wt  + p  log st  words is p/ n required. The time taken for all-to-all broadcast of
. The parallel run time is p/ n c lt  diagonal is thl the table in the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is sufficiently large with n ) the communication time. If 2 n ( Q ) is the computation time, and p/ 3 n ( Q ). Here, 2 n ( Q ) + p/ 3 n ( Q  = P T In order terms,
, communication time can be made an arbitrarily small fraction of computation time, yielding linear speedup. p respect to
). This time can be improved by pipelining 2 n ( Q ) processing elements to accomplish the task in time n ( Q This formulation can use at most
. C ) of matrix j , i ( c  + 1)/2 processing elements. Each processing element computes a single entry n ( n ] on j , i  [ C the computation of the cost
 does not depend only on the entriest Pipelining works due to the nonserial nature of the problem. Computation of an entry on a diagonal
 - 1 is completed.t  can start even before work on diagonalt  - 1 but also on all the earlier diagonals. Hence work on diagonalt on diagonal
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
12.6 Summary and Discussion
This chapter provides a framework for deriving parallel algorithms that use dynamic programming. It identifies possible sources of
parallelism, and indicates under what conditions they can be utilized effectively.
By representing computation as a graph, we identify three sources of parallelism. First, the computation of the cost of a single
Figure subproblem (a node in a level) can be parallelized. For example, for computing the shortest path in the multistage graph shown in
). For many problems, however, node n ( Q , node computation can be parallelized because the complexity of node computation is itself 12.3
computation complexity is lower, limiting available parallelism.
Second, subproblems at each level can be solved in parallel. This provides a viable method for extracting parallelism from a large class
of problems (including all the problems in this chapter).
The first two sources of parallelism are available to both serial and nonserial formulations. Nonserial formulations allow a third source of
parallelism: pipelining of computations among different levels. Pipelining makes it possible to start solving a problem as soon as the
subproblems it depends on are solved. This form of parallelism is used in the parenthesization problem.
. As Section 10.4.2 Note that pipelining was also applied to the parallel formulation of Floyd's all-pairs shortest-paths algorithm in
, this algorithm corresponds to a serial DP formulation. The nature of pipelining in this algorithm is different from Section 12.4 discussed in
the one in nonserial DP formulation. In the pipelined version of Floyd's algorithm, computation in a stage is pipelined with the
communication among earlier stages. If communication cost is zero (as in a PRAM), then Floyd's algorithm does not benefit from
pipelining.
Throughout the chapter, we have seen the importance of data locality. If the solution to a problem requires results from other
subproblems, the cost of communicating those results must be less than the cost of solving the problem. In some problems (the 0/1
knapsack problem, for example) the degree of locality is much smaller than in other problems such as the longest-common-subsequence
problem and Floyd's all-pairs shortest-paths algorithm.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
12.7 Bibliographic Remarks
] for solving multistage decision problems. Various formal models Bel57 Dynamic programming was originally presented by Bellman [
]. Several textbooks and articles present sequential DP formulations of the KK88b , MM73 , KH67 have since been developed for DP [
longest-common-subsequence problem, the matrix chain multiplication problem, the 0/1 knapsack problem, and the shortest-path
]. Bro79 , PS82 , HS78 , CLR90 problem [
] show that monadic serial DP formulations can be solved in parallel on systolic arrays as matrix-vector WL88 , LW85 Li and Wah [
products. They further present a more concurrent but non-cost-optimal formulation by formulating the problem as a matrix-matrix product.
] present a polyadic serial formulation for the string editing problem and derive a parallel formulation based on a RS90b Ranka and Sahni [
checkerboard partitioning.
The DP formulation of a large class of optimization problems is similar to that of the optimal matrix-parenthesization problem. Some
]. The AU72 ], and CYK parsing [ CLR90 examples of these problems are optimal triangularization of polygons, optimal binary search trees [
). Several parallel formulations have been proposed by 3 n ( Q serial complexity of the standard DP formulation for all these problems is
). Guibas, Kung, and 2 n ( Q ) processing elements on a hypercube and that solve the problem in time n ( Q ] that use IPS91 . [ et al Ibarra
). Karypis and Kumar n ( Q ) processing cells and solves the problem in time 2 n ( Q ] present a systolic algorithm that uses GKT79 Thompson [
] and experimentally evaluate them by GKT79  [ et al. ] analyze three distinct mappings of the systolic algorithm presented by Guibas KK93[
using the matrix-multiplication parenthesization problem. They show that a straightforward mapping of this algorithm to a mesh
architecture has an upper bound on efficiency of 1/12. They also present a better mapping without this drawback, and show near-linear
speedup on a mesh embedded into a 256-processor hypercube for the optimal matrix-parenthesization problem.
Many faster parallel algorithms for solving the parenthesization problem have been proposed, but they are not cost-optimal and are
applicable only to theoretical models such as the PRAM. For example, a generalized method for parallelizing such programs is described
] uses Ryt88 ) processing elements. Rytter [ 9 n  ( O ) on n 2  (log O ] that leads directly to formulations that run in time VSBR83  [ et al. by Valiant
) for a 6 n  ( O ) for a CREW PRAM and n /log 6 n  ( O the parallel pebble game on trees to reduce the number of processing elements to
] present a similar algorithm for CREW PRAM models that run HLV90  [ et al. ). Huang n 2  (log O hypercube, yet solves this problem in time
] use vectorized formulations of DP for the Cray to DCG90  [ et al. ) processing elements. DeMello n  log 3.5  (n O ) on n  log  ( O in time
solve optimal control problems.
As we have seen, the serial polyadic formulation of the 0/1 knapsack problem is difficult to parallelize due to lack of communication
] use specific characteristics of the knapsack problem and derive a divide-and-conquer strategy for parallelizing LSS88  [ et al. locality. Lee
 demonstrate et al. the DP algorithm for the 0/1 knapsack problem on a MIMD message-passing computer (Problem 12.2). Lee
experimentally that it is possible to obtain linear speedup for large instances of the problem on a hypercube.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
. Derive the speedup and Section 12.2.2  Consider the parallel algorithm for solving the 0/1 knapsack problem in 12.1
efficiency for this algorithm. Show that the efficiency of this algorithm cannot be increased beyond a certain value by
increasing the problem size for a fixed number of processing elements. What is the upper bound on efficiency for this
? ct  and wt formulation as a function of
, the degree of Section 12.2.2  In the parallel formulation of the 0/1 knapsack problem presented in ] LSS88 12.2 [
, the knapsack capacity. Also this algorithm has limited data locality, as the amount of c concurrency is proportional to
et data to be communicated is of the same order of magnitude as the computation at each processing element. Lee
, the number of weights. This n  present another formulation in which the degree of concurrency is proportional to al.
formulation also has much more data locality. In this formulation, the set of weights is partitioned among processing
elements. Each processing element computes the maximum profit it can achieve from its local weights for knapsacks
. This information is expressed as lists that are merged to yield the global solution. Compute c of various sizes up to
the parallel run time, speedup, and efficiency of this formulation. Compare the performance of this algorithm with that
. Section 12.2.2 in
 We noticed that the parallel formulation of the longest-common-subsequence problem has an upper bound of 12.3
0.5 on its efficiency. It is possible to use an alternate mapping to achieve higher efficiency for this problem. Derive a
formulation that does not suffer from this upper bound, and give the run time of this formulation.
. Section 3.4.1  Consider the blocked-cyclic mapping discussed in Hint:
 The traveling salesman problem (TSP) is defined as follows: Given a set of cities and the distance ] HS78 12.4 [
between each pair of cities, determine a tour through all cities of minimum length. A tour of all cities is a trip visiting
each city once and returning to the starting point. Its length is the sum of distances traveled. This problem can be
 be represented by V ). Let the set of cities E , V ( G solved using a DP formulation. View the cities as vertices in a graph
) k , S  (f . If j  andi  be the distance between cities j, i c }. Furthermore, let n v , ..., 3 v , 2 v  { S } and let n v , ..., 2 v , 1 v {
, then the k , and terminating in city S , passing through all the cities in set 1 v represents the cost of starting at city
): k , S  (f following recursive equations can be used to compute
9  Equation 12.
, derive a parallel formulation. Compute the parallel run time and the speedup. Is this parallel Equation 12.9 Based on
formulation cost-optimal?
) records. These files can be m  ( O ) and n  ( O  Consider the problem of merging two sorted files containing ] HS78 12.5 [
 such files, the problem of merging them into a single file can be r ). Given n  + m  ( O merged into a sorted file in time
formulated as a sequence of merge operations performed on pairs of files. The overall cost of the merge operation is
a function of the sequence in which they are merged. The optimal merge order can be formulated as a greedy
problem and its parallel formulations derived using principles illustrated in this chapter.
 processing p Write down the recursive equations for this problem. Derive a parallel formulation for merging files using
elements. Compute the parallel run time and speedup, and determine whether your parallel formulation is
cost-optimal.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 devices connected in series, as n  Consider the problem of designing a fault-tolerant circuit containing ] HS78 12.6 [
, the overall probability of if . If the probability of failure of each of these devices is given by Figure 12.9(a) shown in
 represents a product of specified terms. The . Here, failure of the circuit is given by
reliability of this circuit can be improved by connecting multiple functional devices in parallel at each stage, as shown
 ,if  duplicate functional units, each with a probability of failure given byi r  in the circuit hasi . If stage Figure 12.9(b) in
 and the overall probability of failure of the circuit then the overall probability of failure of this stage is reduced to
. In general, for physical reasons, the probability of failure at a particular level is given by
). The objective of the problem is to minimize the overall probability of i m , i r  (i , but some function may not be
. failure of the circuit,
 devices connected in a series within a circuit. (b) Each stage in the circuit n Figure 12.9. (a)
 such stages connected in the series. n  functional units. There arei m now has
 theni c  costsi Construction cost adds a new dimension to this problem. If each of the functional units used at stage
. c  should be less than a fixed quantity due to cost constraints, the overall cost
The problem can be formally defined as
. n i  > 0 and 0 <i m where
). The c  ( nf . The optimal solution is given by x  stages of costi ) represent the reliability of a system with x  (if Let
) is as follows: x  (if recursive equation for
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
0  Equation 12.1
Classify this formulation into one of the four DP categories, and derive a parallel formulation for this algorithm.
Determine its parallel run time, speedup, and isoefficiency function.
 Consider the simplified optimal polygon-triangulation problem. This problem can be defined as follows. ] CLR90 12.7 [
Given a simple polygon, break the polygon into a set of triangles by connecting nodes of the polygon with chords.
 is defined by a k v , and j v , i v . The cost of constructing a triangle with nodes Figure 12.10 This process is illustrated in
). For this problem, let the cost be the total length of the edges of the triangle (using Euclidean k v , j v , i v  (f function
distance). The optimal polygon-triangulation problem breaks up a polygon into a set of triangles such that the total
length of each triangle (the sum of the individual lengths) is minimized. Give a DP formulation for this problem.
 processing elements. Determine its p Classify it into one of the four categories and derive a parallel formulation for
parallel run time, speedup, and isoefficiency function.
Figure 12.10. Two possible triangulations of a regular polygon.
 This problem is similar to the optimal matrix-parenthesization problem. Hint:
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Chapter 13. Fast Fourier Transform
The discrete Fourier transform (DFT) plays an important role in many scientific and technical applications, including time series and
waveform analysis, solutions to linear partial differential equations, convolution, digital signal processing, and image filtering. The DFT is
 regularly sampled points from a cycle of a periodic signal, like a sine wave, onto an equal number of n a linear transformation that maps
points representing the frequency spectrum of the signal. In 1965, Cooley and Tukey devised an algorithm to compute the DFT of an
) operations. Their new algorithm was a significant improvement over previously known methods for computing n  log n ( Q -point series in n
fast ) operations. The revolutionary algorithm by Cooley and Tukey and its variations are referred to as the 2 n ( Q the DFT, which required
 (FFT). Due to its wide application in scientific and engineering fields, there has been a lot of interest in implementing Fourier transform
FFT on parallel computers.
Several different forms of the FFT algorithm exist. This chapter discusses its simplest form, the one-dimensional, unordered, radix-2
FFT. Parallel formulations of higher-radix and multidimensional FFTs are similar to the simple algorithm discussed in this chapter
because the underlying ideas behind all sequential FFT algorithms are the same. An ordered FFT is obtained by performing bit reversal
) on the output sequence of an unordered FFT. Bit reversal does not affect the overall complexity of a parallel Section 13.4 (
implementation of FFT.
transpose  and the binary-exchange algorithm In this chapter we discuss two parallel formulations of the basic algorithm: the
, and the memory or network bandwidth, one of these may run p , the number of processes n . Depending on the size of the input algorithm
faster than the other.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
13.1 The Serial Algorithm
Y  = < Y  is the sequence X . The discrete Fourier transform of the sequence n  - 1]> of length n  [ X  [1], ..., X  [0], X  = < X Consider a sequence
 1]>, where n -  [ Y  [1], ..., Y [0],
1  Equation 13.
 is the base of natural e , where th root of unity in the complex plane; that is, n  is the primitive w , Equation 13.1 In
 in the equation can be thought of as elements of the finite commutative ring of integers modulo w logarithms. More generally, the powers of
. twiddle factors  used in an FFT computation are also known as w . The powers of n
 complex multiplications. Therefore, the sequential complexity of n  requires Equation 13.1 ] according toi  [ Y The computation of each
n ( Q ). The fast Fourier transform algorithm described below reduces this complexity to 2 n ( Q  is n  of length Y computing the entire sequence
). n log
-point DFT computation to be split into n  is a power of two. The FFT algorithm is based on the following step that permits an n Assume that
/2)-point DFT computations: n two (
2  Equation 13.
 as follows: Equation 13.2 /2)th root of unity. Then, we can rewrite n  is the primitive ( ; that is, Let
3  Equation 13.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is a power of two, each of these n /2)-point DFT computation. If n , each of the two summations on the right-hand side is an ( Equation 13.3 In
DFT computations can be divided similarly into smaller computations in a recursive manner. This leads to the recursive FFT algorithm
. This FFT algorithm is called the radix-2 algorithm because at each level of recursion, the input sequence is split Algorithm 13.1 given in
into two equal halves.
Algorithm 13.1 The recursive, one-dimensional, unordered, radix-2 FFT algorithm. Here
.
) w , n , Y , X  R_FFT( procedure 1.
else [0] X [0] := Y then  = 1) n  ( if 2.
begin 3.
); 2 w /2, n /2]>, n[ Q [1], ..., Q [0], Q  - 2]>, < n[ X [2], ..., X [0], X 4. R_FFT(<
); 2 w /2, n /2]>, n[ T [1], ..., T [0], T  - 1]>, < n[ X [3], ..., X [1], X 5. R_FFT(<
do  - 1 n to  := 0i for 6.
/2)]; n  mod (i[ T i w /2)] + n  mod (i[ Q ] :=i[ Y 7.
 R_FFT end 8.
 illustrates how the recursive algorithm works on an 8-point sequence. As the figure shows, the first set of computations Figure 13.1
 takes place at the deepest level of recursion. At this level, the elements of the sequence whose Algorithm 13.1 corresponding to line 7 of
/2 are used in the computation. In each subsequent level, the difference between the indices of the elements used n indices differ by
 used in each computation. w together in a computation decreases by a factor of two. The figure also shows the powers of
Figure 13.1. A recursive 8-point unordered FFT computation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
The size of the input sequence over which an FFT is computed recursively decreases by a factor of two at each level of recursion (lines 4
th level of m . At the n  for an initial sequence of length n ). Hence, the maximum number of levels of recursion is log Algorithm 13.1 and 5 of
) and the n ( Q  each are computed. Thus, the total number of arithmetic operations (line 7) at each level is m /2 n  FFTs of size m recursion, 2
). n  log n ( Q  is Algorithm 13.1 overall sequential complexity of
The serial FFT algorithm can also be cast in an iterative form. The parallel implementations of the iterative form are easier to illustrate.
Therefore, before describing parallel FFT algorithms, we give the iterative form of the serial algorithm. An iterative FFT algorithm is derived
 gives the classic iterative Cooley-Tukey Algorithm 13.2 by casting each level of recursion, starting with the deepest level, as an iteration.
 iterations of the outer loop starting on line n -point, one-dimensional, unordered, radix-2 FFT. The program performs log n algorithm for an
)th level of recursion in the recursive m  - n  in the iterative version of the algorithm corresponds to the (log m 5. The value of the loop index
 complex multiplications and additions. n ). Just as in each level of recursion, each iteration performs Figure 13.1 version (
-point FFT, and the inner loop starting at n  times for an n  has two main loops. The outer loop starting at line 5 is executed log Algorithm 13.2
 times during each iteration of the outer loop. All operations of the inner loop are constant-time arithmetic operations. n line 8 is executed
 is updated using the R ). In every iteration of the outer loop, the sequence n og n l ( Q Thus, the sequential time complexity of the algorithm is
 serves as the initial X  during the previous iteration. For the first iteration, the input sequence S elements that were stored in the sequence
. Y  from the final iteration is the desired Fourier transform and is copied to the output sequence X . The updated sequence R sequence
Algorithm 13.2 The Cooley-Tukey algorithm for one-dimensional, unordered, radix-2 FFT. Here
.
) n , Y , X  ITERATIVE_FFT( procedure 1.
begin 2.
; n  := log r 3.
];i[ X ] :=i[ R do  - 1 n to  := 0i for 4.
 /* Outer loop */ do  - 1 r to  := 0 m for 5.
begin 6.
];i[ R ]:=i[ S do  - 1 n to  := 0i for 7.
 /* Inner loop */ do  - 1 n to  := 0i for 8.
begin 9.
 */i ) be the binary representation of -1 r b  ··· 1 b 0 b  /* Let (
); -1 r b · ·· +1 m b 0 -1 m b ... 0 b  := ( j 10.
); -1 r b · ·· +1 m b 1 -1 m b ... 0 b  := ( k 11.
12.
; /* Inner loop */ endfor 13.
; /* Outer loop */ endfor 14.
];i[ R ] :=i[ Y do  - 1 n to  := 0i for 15.
 ITERATIVE_FFT end 16.
 are k  and j ]. The indices k[ S ] and j[ S ] by usingi[ R  performs a crucial step in the FFT algorithm. This step updates Algorithm 13.2 Line 12 in
) be -1 r b·  ·· 1 b 0 b  bits. Let ( r  containsi , the binary representation of n  <i . Since0 r  = 2 n  as follows. Assume thati derived from the index
th most m  is derived by forcing the j ), index r  < m th iteration of the outer loop (0 m . In the i the binary representation of index
 differ only in their k  and j  to 1. Thus, the binary representations of m b  is derived by forcing k ) to zero. Index m b  (that is,i significant bit of
, i , one is the same as index k  and j  is either 0 or 1. Hence, of the two indices m b , i th most significant bits. In the binary representation of m
] is generated by executingi[ R  - 1, n  between 0 andi th iteration of the outer loop, for each m  = 1. In the m b  = 0 or m b depending on whether
Figure 13.2 th most significant bit. m  only in thei  whose index differs from S ] and on another element ofi[ S  on Algorithm 13.2 line 12 of
 = 16. n shows the pattern in which these elements are paired for the case in which
Figure 13.2. The pattern of combination of elements of the input and the intermediate
sequences during a 16-point unordered FFT computation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
13.2 The Binary-Exchange Algorithm
 for performing FFT on a parallel computer. First, a decomposition is induced by binary-exchange algorithm This section discusses the
partitioning the input or the output vector. Therefore, each task starts with one element of the input vector and computes the
corresponding element of the output. If each task is assigned the same label as the index of its input or output element, then in each of the
 iterations of the algorithm, exchange of data takes place between pairs of tasks with labels differing in one bit position. n log
13.2.1 A Full Bandwidth Network
In this subsection, we describe the implementation of the binary-exchange algorithm on a parallel computer on which a bisection width
 parallel processes. Since the pattern of interaction among the tasks of parallel FFT matches that of p ) is available to p ( Q ) of Section 2.4.4 (
a hypercube network, we describe the algorithm assuming such an interconnection network. However, the performance and scalability
). p  ( O analysis would be valid for any parallel computer with an overall simultaneous data-transfer capacity of
One Task Per Process
 illustrates the interaction pattern induced by Figure 13.3 We first consider a simple mapping in which one task is assigned to each process.
] and finallyi  [ X ) initially stores n  <i  (0i  = 16. As the figure shows, process n this mapping of the binary-exchange algorithm for
. All Algorithm 13.2 ] by executing line 12 ofi[ R  updates the value ofi  iterations of the outer loop, process P n ]. In each of the logi  [ Y generates
 updates are performed in parallel. n
.i  denotes the process labeledi Figure 13.3. A 16-point unordered FFT on 16 processes. P
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 at only one bit. Recall that in ai  from a process whose label differs from S  requires an element ofi To perform the updates, process P
hypercube, a node is connected to all those nodes whose labels differ from its own at only one bit position. Thus, the parallel FFT
computation maps naturally onto a hypercube with a one-to-one mapping of processes to nodes. In the first iteration of the outer loop, the
 communicate 7  to P 0 labels of each pair of communicating processes differ only at their most significant bits. For instance, processes P
, respectively. Similarly, in the second iteration, the labels of processes communicating with each other differ at the second 15  to P 8 with P
most significant bit, and so on.
 iterations of this algorithm, every process performs one complex multiplication and addition, and exchanges one n In each of the log
) to execute the n (log Q complex number with another process. Thus, there is a constant amount of work per iteration. Hence, it takes time
 nodes. This hypercube formulation of FFT is cost-optimal because its process-time n algorithm in parallel by using a hypercube with
-point FFT. n ), the same as the complexity of a serial n  log n ( Q product is
Multiple Tasks Per Process
. For the sake of simplicity, p  > n  processes, where p -point FFT are mapped onto n  tasks of an n We now consider a mapping in which the
 shows, we partition the sequences into blocks of Figure 13.4 . As d  = 2 p  andr  = 2 n  are powers of two, i.e., p  and n let us assume that both
 contiguous elements and assign one block to each process. p/ n
p . In general, the number of processes isi  denotes the process labeledi Figure 13.4. A 16-point FFT on four processes. P
. r  = 2 n  and the length of the input sequence is d = 2
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 <i , such that 0 i ) is the binary representation of any -1 r b  ··· 1 b 0 b  is that, if ( Figure 13.4 An interesting property of the mapping shown in
 most significant bits of the index of any element of the d ). That is, the -1 d b · ·· 0 b ] are mapped onto the process labeled (i[ S ] andi[ R , then n
sequence are the binary representation of the label of the process that the element belongs to. This property of the mapping plays an
important role in determining the amount of communication performed during the parallel execution of the FFT algorithm.
 (= 2) most significant bits are mapped onto different processes. However, d  shows that elements with indices differing at their Figure 13.4
 most significant bits are mapped onto the same process. Recall from the previous section that d all elements with indices having the same
th most m th iteration of the loop, elements with indices differing in the m  iterations of the outer loop. In the n  = log r -point FFT requires n an
 iterations belong to different processes, and pairs of d significant bit are combined. As a result, elements combined during the first
) iterations belong to the same processes. Hence, this parallel FFT algorithm requires interprocess d  - r elements combined during the last (
 thi  iterations. Furthermore, in the d  - r  iterations. There is no interaction during the last n  of the log p  = log d interaction only during the first
e  iterations, all the elements that a process requires come from exactly one other process – the one whose label is different at th d of the first
 th most significant bit.i
 + p  log st  words of data. Therefore, the time spent in communication in the entire algorithm is p/ n Each interaction operation exchanges
 iterations. If a complex multiplication and addition pair takes time n  during each of the log R  elements of p/ n . A process updates p ) log p/ n ( wt
-node hypercube network is p -point FFT on a n , then the parallel run time of the binary-exchange algorithm for ct
4  Equation 13.
– ) n  log n  ( O . For the parallel system to be cost-optimal, this product should be p  log n wt  + p  log p st  + n  log n ct The process-time product is
. n p the sequential time complexity of the FFT algorithm. This is true for
The expressions for speedup and efficiency are given by the following equations:
5  Equation 13.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Scalability Analysis
-point FFT is n  for an W , we know that the problem size Section 13.1 From
6  Equation 13.
p  to keep p  log p n  log n  or p n , Figure 13.3  processes with the mapping of n -point FFT can utilize a maximum of n Since an
) due to concurrency. We now derive the p  log p ( W processes busy. Thus, the isoefficiency function of this parallel FFT algorithm is
 as Equation 13.5 isoefficiency function for the binary exchange algorithm due to the different communication-related terms. We can rewrite
K , where K ) should be equal to a constant 1/ n  log ct )/( p log wt ) + ( n  log n ct )/( p  log p st  , the expression ( E In order to maintain a fixed efficiency
, Section 5.4.2 . As proposed in Chapter 5  in this manner to keep the terminology consistent with K ). We have defined the constant E /(1 - E =
we use an approximation to obtain closed expressions for the isoefficiency function. We first determine the rate of growth of the problem
 = 0. Now the condition for maintaining constant wt  constant. To do this, we assume st  that would keep the terms due to p size with respect to
 is as follows: E efficiency
7  Equation 13.
 gives the isoefficiency function due to the overhead resulting from interaction latency or the message startup time. Equation 13.7
E  = 0; hence, a fixed efficiency st . We assume that wt Similarly, we derive the isoefficiency function due to the overhead resulting from
requires that the following relation be maintained:
8  Equation 13.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). In this case, p  log p ( Q  is less than Equation 13.8  is less than one, then the rate of growth of the problem size required by ct / w Kt If the term
Equation 13.8  exceeds one, then ct / w Kt  determines the overall isoefficiency function of this parallel system. However, if Equation 13.7
. Equation 13.7 ) given by p  log p ( Q determines the overall isoefficiency function, which is now greater than the isoefficiency function of
 is an increasing function of K . Here, ct , and wt , K For this algorithm, the asymptotic isoefficiency function depends on the relative values of
 depends on the speed of the ct  depends on the communication bandwidth of the parallel computer, and wt  to be maintained, E the efficiency
computation speed of the processors. The FFT algorithm is unique in that the order of the isoefficiency function depends on the desired
)) acts wt  + ct /( ct  = E , or wt / ct ) = E  = 1(i.e.,1/(1 - ct / w Kt efficiency and hardware-dependent parameters. In fact, the efficiency corresponding to
), the asymptotic wt  + ct /( ct E , efficiencies up to the threshold can be obtained easily. For wt  and ct as a kind of threshold. For a fixed
) can be obtained only if the problem size is w tc + t /( tc  log p). Efficiencies much higher than the threshold p ( Q isoefficiency function is
. The following Q extremely large. The reason is that for these efficiencies, the asymptotic isoefficiency function is
 on the isoefficiency function. ct / w Kt examples illustrate the effect of the value of
Example 13.1 Threshold effect in the binary-exchange algorithm
 = 4, and wt  = 2, ct Consider a hypothetical hypercube for which the relative values of the hardware parameters are given by
) is 0.33. wt  + ct /( ct  = 25. With these values, the threshold efficiency st
Now we study the isoefficiency functions of the binary-exchange algorithm on a hypercube for maintaining efficiencies
. From Equations p  log p below and above the threshold. The isoefficiency function of this algorithm due to concurrency is
 and p  log p ) ct / st  ( K  terms in the overhead function are wt  and st , the isoefficiency functions due to the 13.8  and 13.7
), the overall isoefficiency K  (that is, for a given E , respectively. To maintain a given efficiency p  log
function is given by:
 = 0.20, 0.25, 0.30, 0.35, 0.40, and 0.45. Notice that E  shows the isoefficiency curves given by this function for Figure 13.5
the various isoefficiency curves are regularly spaced for efficiencies up to the threshold. However, the problem sizes
 = 0.20, E required to maintain efficiencies above the threshold are much larger. The asymptotic isoefficiency functions for
. ) p  log 64 . 1 p ( Q  0.45 is E = ), and that for p  log 1.33 p ( Q  = 0.40 is E ). The isoefficiency function for p  log p ( Q 0.25, and 0.30 are
 = 4, w t  = 2, c t Figure 13.5. Isoefficiency functions of the binary-exchange algorithm on a hypercube with
t and
s
. E  = 25 for various values of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
-point FFTs on a 256-node hypercube with the same hardware parameters. The n  shows the efficiency curve of Figure 13.6
 is equal to 256. The figure shows that the p , when n  for various values of Equation 13.5  is computed by using E efficiency
efficiency initially increases rapidly with the problem size, but the efficiency curve flattens out beyond the threshold.
 on a 256-node hypercube n Figure 13.6. The efficiency of the binary-exchange algorithm as a function of
t with
c
 = 25. s t  = 4, and w t  = 2,
 shows that there is a limit on the efficiency that can be obtained for reasonable problem sizes, and that the limit is Example 13.1
determined by the ratio between the CPU speed and the communication bandwidth of the parallel computer being used. This limit can be
raised by increasing the bandwidth of the communication channels. However, making the CPUs faster without increasing the
communication bandwidth lowers the limit. Hence, the binary-exchange algorithm performs poorly on a parallel computer whose
communication and computation speeds are not balanced. If the hardware is balanced with respect to its communication and computation
speeds, then the binary-exchange algorithm is fairly scalable, and reasonable efficiencies can be maintained while increasing the problem
). p  log p ( Q size at the rate of
13.2.2 Limited Bandwidth Network
Now we consider the implementation of the binary-exchange algorithm on a parallel computer whose cross-section bandwidth is less than
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 tasks are n ). We choose a mesh interconnection network to illustrate the algorithm and its performance characteristics. Assume that p ( Q
. d  = 2 p  andr  = 2 n  is a power of two. Let  columns, and that  rows and  processes running on a mesh with p mapped onto
Also assume that the processes are labeled in a row-major fashion and that the data are distributed in the same manner as for the
). -1 d b  ··· 0 b ) is mapped onto the process labeled ( -1 r b · ·· 1 b 0 b hypercube; that is, an element with index (
 iterations between processes whose labels differ at one p As in case of the hypercube, communication takes place only during the first log
bit. However, unlike the hypercube, the communicating processes are not directly linked in a mesh. Consequently, messages travel over
 shows the messages sent and received by Figure 13.7 multiple links and there is overlap among messages sharing the same links.
processes 0 and 37 during an FFT computation on a 64-node mesh. As the figure shows, process 0 communicates with processes 1, 2, 4,
8, 16, and 32. Note that all these processes lie in the same row or column of the mesh as that of process 0. Processes 1, 2, and 4 lie in the
same row as process 0 at distances of 1, 2, and 4 links, respectively. Processes 8, 16, and 32 lie in the same column, again at distances
 steps that require communication, the communicating processes are in the p  of the log of 1, 2, and 4 links. More precisely, in log
 steps, they are in the same column. The number of messages that share at least one link is same row, and in the remaining log
equal to the number of links that each message traverses (Problem 13.9) because, during a given FFT iteration, all pairs of nodes
exchange messages that traverse the same number of links.
Figure 13.7. Data communication during an FFT computation on a logical square mesh of 64 processes. The figure
shows all the processes with which the processes labeled 0 and 37 exchange data.
 links, doubling in each of the The distance between the communicating processes in a row or a column grows from one link to
. Thus, the total time spent in Figure 13.7  iterations. This is true for any process in the mesh, such as process 37 shown in log
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. An equal amount of time is spent in columnwise performing rowwise communication is
 such p/ n . Since a process performs ct communication. Recall that we assumed that a complex multiplication and addition pair takes time
 iterations, the overall parallel run time is given by the following equation: n calculations in each of the log
9  Equation 13.
The speedup and efficiency are given by the following equations:
0  Equation 13.1
O . The process-time product should be The process-time product of this parallel system is
 in st ). Since the communication term due to n 2  (log O  = p , or ) for cost-optimality, which is obtained when n  log n (
. By Equation 13.7 ) as given by p og p l ( Q  is the same as for the hypercube, the corresponding isoefficiency function is again Equation 13.9
 term is wt , we can show that the isoefficiency function due to the Section 13.2.1 performing isoefficiency analysis along the same lines as in
 (Problem 13.4). Given this isoefficiency function, the problem size must grow exponentially with
the number of processes to maintain constant efficiency. Hence, the binary-exchange FFT algorithm is not very scalable on a mesh.
The communication overhead of the binary-exchange algorithm on a mesh cannot be reduced by using a different mapping of the
sequences onto the processes. In any mapping, there is at least one iteration in which pairs of processes that communicate with each
-node ensemble, p ) bisection bandwidth on a p ( Q  links apart (Problem 13.2). The algorithm inherently requires other are at least
 bisection bandwidth, the communication time cannot be asymptotically better than Q and on an architecture like a 2-D mesh with
 as discussed above.
13.2.3 Extra Computations in Parallel FFT
So far, we have described a parallel formulation of the FFT algorithm on a hypercube and a mesh, and have discussed its performance
and scalability in the presence of communication overhead on both architectures. In this section, we discuss another source of overhead
that can be present in a parallel FFT implementation.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
-point n . For an S  (a twiddle factor) with an element of w  that the computation step of line 12 multiplies a power of Algorithm 13.2 Recall from
) are used in -1 n w , ..., 2 w , 1 w , 0 w  (that is, w  distinct powers of n  times in the sequential algorithm. However, only n  log n FFT, line 12 executes
the entire algorithm. So some of the twiddle factors are used repeatedly. In a serial implementation, it is useful to precompute and store all
) complex operations n ( Q  twiddle factors before starting the main algorithm. That way, the computation of twiddle factors requires only n
) operations needed to compute all twiddle factors in each iteration of line 12. n  log n ( Q rather than the
). The reason is that, even if a n ( Q In a parallel implementation, the total work required to compute the twiddle factors cannot be reduced to
certain twiddle factor is used more than once, it might be used on different processes at different times. If FFTs of the same size are
computed on the same number of processes, every process needs the same set of twiddle factors for each computation. In this case, the
twiddle factors can be precomputed and stored, and the cost of their computation can be amortized over the execution of all instances of
FFTs of the same size. However, if we consider only one instance of FFT, then twiddle factor computation gives rise to additional overhead
in a parallel implementation, because it performs more overall operations than the sequential implementation.
th iteration of the loop starting on m  used in the three iterations of an 8-point FFT. In the w As an example, consider the various powers of
 + 1 most m  is the integer obtained by reversing the order of thel ), such that n  <i  (0i  is computed for alll w line 5 of the algorithm,
 isl  to see how Algorithm 13.2  and Figure 13.1  + 1) zeros to the right (refer to m  - ( n  and then padding them by logi significant bits of
 for an 8-point FFT. m  andi  required for all values of w  shows the binary representation of the powers of Table 13.1 derived).
r . Process 0 computes just one twiddle facto Table 13.1 If eight processes are used, then each process computes and uses one column of
. for all its iterations, but some processes (in this case, all other processes 2–7) compute a new twiddle factor in each of the three iterations
e /2 = 4, then each process computes two consecutive columns of the table. In this case, the last process computes the twiddl n  = p If
 = 0 (100) m factors in the last two columns of the table. Hence, the last process computes a total of four different powers – one each for
 = 2 (011 and 111). Although different processes may compute a different number of twiddle factors, the total m  = 1 (110), and two for m and
 times the maximum number of twiddle factors that any single process computes. Let p overhead due to the extra work is proportional to
 shows the Table 13.2 -point FFT. n  processes computes during an p ) be the maximum number of twiddle factors that any of the p , n ( h
 = 1, 2, 4, and 8. The table also shows the maximum number of new twiddle factors that any single process computes p ) for p (8, h values of
in each iteration.
 calculated in different iterations of an 8-point FFT w Table 13.1. The binary representation of the various powers of
 is the index of the inneri  refers to the iteration number of the outer loop, and m ). The value of Figure 13.1 (also see
. Algorithm 13.2 loop of
i
7 6 5 4 3 2 1 0
100 100 100 100 000 000 000 000  = 0 m
110 110 010 010 100 100 000 000  = 1 m
111 011 101 001 110 010 100 000  = 2 m
 used by any process in each iteration of an 8-point FFT w Table 13.2. The maximum number of new powers of
computation.
 = 8 p  = 4 p  = 2 p  = 1 p
1 1 1 2  = 0 m
1 1 2 2  = 1 m
1 2 4 4  = 2 m
3 4 7 8 ) p (8, h Total =
 is defined by the following recurrence relation (Problem 13.5): h The function
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 is p n  > 1 and p The solution to this recurrence relation for
p  log  to compute one twiddle factor, then at least one process spends time Thus, if it takes time
computing twiddle factors. The total cost of twiddle factor computation, summed over all processes, is
 in computing twiddle factors, the total parallel overhead due to extra work ( . Since even a serial implementation incurs a cost of p log
) is given by the following equation:
This overhead is independent of the architecture of the parallel computer used for the FFT computation. The isoefficiency function due to
). Since this term is of the same order as the isoefficiency terms due to message startup time and p  log p ( Q  is
concurrency, the extra computations do not affect the overall scalability of parallel FFT.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
13.3 The Transpose Algorithm
The binary-exchange algorithm yields good performance on parallel computers with sufficiently high communication bandwidth with
respect to the processing speed of the CPUs. Efficiencies below a certain threshold can be maintained while increasing the problem size
at a moderate rate with an increasing number of processes. However, this threshold is very low if the communication bandwidth of the
parallel computer is low compared to the speed of its processors. In this section, we describe a different parallel formulation of FFT that
trades off some efficiency for a more consistent level of parallel performance. This parallel algorithm involves matrix transposition, and
. transpose algorithm hence is called the
The performance of the transpose algorithm is worse than that of the binary-exchange algorithm for efficiencies below the threshold.
However, it is much easier to obtain efficiencies above the binary-exchange algorithm's threshold using the transpose algorithm. Thus, the
transpose algorithm is particularly useful when the ratio of communication bandwidth to CPU speed is low and high efficiencies are
) bisection width, the transpose algorithm has a fixed asymptotic isoefficiency p ( Q -node network with p desired. On a hypercube or a
). That is, the order of this isoefficiency function is independent of the ratio of the speed of point-to-point p  log 2 p ( Q function of
communication and the computation.
13.3.1 Two-Dimensional Transpose Algorithm
The simplest transpose algorithm requires a single transpose operation over a two-dimensional array; hence, we call this algorithm the
. two-dimensional transpose algorithm
 are arranged in a Algorithm 13.2  used in n  is a power of 2, and that the sequences of size Assume that
n  points requires log n  = 16. Recall that computing the FFT of a sequence of n  for Figure 13.8 two-dimensional square array, as shown in
, then the FFT computation in each column Figure 13.8 . If the data are arranged as shown in Algorithm 13.2 iterations of the outer loop of
 iterations without any column requiring data from any other column. Similarly, in the remaining log can proceed independently for log
 shows the Figure 13.8  iterations, computation proceeds independently in each row without any row requiring data from any other row.
 are arranged in a n pattern of combination of the elements for a 16-point FFT. The figure illustrates that if data of size
point FFT computations in the columns of the array, followed by - -point FFT computation is equivalent to independent n array, then an
point FFT computations in the rows. - independent
Figure 13.8. The pattern of combination of elements in a 16-point FFT when the data are arranged in a 4 x 4
two-dimensional square array.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
-point column FFTs, then the remaining part of the problem is to  array of data is transposed after computing the If the
point columnwise FFTs of the transposed matrix. The transpose algorithm uses this property to compute the FFT in - compute the
 array of data among the processes. For instance, parallel by using a columnwise striped partitioning to distribute the
, where the 4 x 4 array of data is distributed among four processes such Figure 13.9 consider the computation of the 16-point FFT shown in
that each process stores one column of the array. In general, the two-dimensional transpose algorithm works in three phases. In the first
point FFT is computed for each column. In the second phase, the array of data is transposed. The third and final phase is - phase, a
 shows Figure 13.9 point FFTs for each column of the transposed array. - identical to the first phase, and involves the computation of
 points for that the first and third phases of the algorithm do not require any interprocess communication. In both these phases, all
each columnwise FFT computation are available on the same process. Only the second phase requires communication for transposing the
 matrix.
Figure 13.9. The two-dimensional transpose algorithm for a 16-point FFT on four processes.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, one column of the data array is assigned to one process. Before analyzing the transpose Figure 13.9 In the transpose algorithm shown in
 array of data . The p  processes are used and 1 p algorithm further, consider the more general case in which
 rows is assigned to each process. In the first and third phases of the algorithm, each is striped into blocks, and one block of
 matrix, which is distributed  each. The second phase transposes the  FFTs of size process computes
 that such a transpose requires an all-to-all personalized Section 4.5  processes with a one-dimensional partitioning. Recall from p among
communication.
Now we derive an expression for the parallel run time of the two-dimensional transpose algorithm. The only inter-process interaction in this
 processes is transposed. p  array of data partitioned along columns or rows and mapped onto algorithm occurs when the
d  – the amount of data owned by each process – in the expression for all-to-all personalize p/ n  by m Replacing the message size
 as the time spent in the second phase of the algorithm. The first and third phases each p/ n wt  - 1) + p  ( st  yields Table 4.1 communication in
) bisection p ( Q . Thus, the parallel run time of the transpose algorithm on a hypercube or any take time
width network is given by the following equation:
1  Equation 13.1
The expressions for speedup and efficiency are as follows:
2  Equation 13.1
). p  log 2 p ( W  = n  log n . This parallel system is cost-optimal if n wt  + 2 p st  + n  log n ct The process-time product of this parallel system is
 is independent of the number of processes. The Equation 13.12  in the expression for efficiency in wt Note that the term associated with
 processes can be used to partition the  because at most degree of concurrency of this algorithm requires that
). Thus, the problem size must increase at p  log 2 p ( W  = n  log n ), or 2 p ( W  = n  array of data in a striped manner. As a result,
) with respect to the number of processes to use all of them efficiently. The overall isoefficiency function of the p  log 2 p ( Q least as fast as
). This p ( Q ) on a hypercube or another interconnection network with bisection width p  log 2 p ( Q two-dimensional transpose algorithm is
. On a network whose cross-section bandwidth ct  for point-to-point communication and wt isoefficiency function is independent of the ratio of
, and the E , S , P T ) in order to derive b / p ( Q  term must be multiplied by an appropriate expression of wt  nodes, the p ) for p ( Q  is less than b
isoefficiency function (Problem 13.6).
Comparison with the Binary-Exchange Algorithm
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 shows that the transpose algorithm has a much higher overhead than the binary-exchange 13.11  and 13.4 A comparison of Equations
. As a result, either of the two wt , but has a lower overhead due to per-word transfer time st algorithm due to the message startup time
 is very low, then the transpose algorithm may be the st . If the latency wt  and st algorithms may be faster depending on the relative values of
algorithm of choice. On the other hand, the binary-exchange algorithm may perform better on a parallel computer with a high
communication bandwidth but a significant startup time.
) can be realized by using the binary-exchange algorithm if the p  log p ( Q  that an overall isoefficiency function of Section 13.2.1 Recall from
 = 2, then the overall isoefficiency ct / w Kt ). If the desired efficiency is such that E  /(1 - E  = K  1, where ct / w Kt efficiency is such that
 > 2, the two-dimensional c /t w Kt ). When p  log 2 p ( Q function of both the binary-exchange and the two-dimensional transpose algorithms is
transpose algorithm is more scalable than the binary-exchange algorithm; hence, the former should be the algorithm of choice, provided
. Note, however, that the transpose algorithm yields a performance benefit over the binary-exchange algorithm only if the 2 p n that
 nodes (Problem 13.6). p ) for p ( Q target architecture has a cross-section bandwidth of
13.3.2 The Generalized Transpose Algorithm
 two-dimensional array that is partitioned  is arranged in a n In the two-dimensional transpose algorithm, the input of size
 processes. These processes, irrespective of the underlying architecture of the parallel computer, can be p along one dimension on
 data points to be arranged n regarded as arranged in a logical one-dimensional linear array. As an extension of this scheme, consider the
Figure 13.10  two-dimensional mesh of processes.  three-dimensional array mapped onto a logical 1/3 n  x 1/3 n  x 1/3 n in an
. z , and y , x illustrates this mapping. To simplify the algorithm description, we label the three axes of the three-dimensional array of data as
 parts. As the figure shows, each process stores  plane of the array is checkerboarded into y - x In this mapping, the
. Thus, each process has 1/3 n axis) is -  columns of data, and the length of each column (along the z
 elements of data.
 processes p -point FFT on n Figure 13.10. Data distribution in the three-dimensional transpose algorithm for an
.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 can be computed by first computing  that the FFT of a two-dimensionally arranged input of size Section 13.3.1 Recall from
point one-dimensional FFTs of all the - -point one-dimensional FFTs of all the columns of the data and then computing the the
-point FFT can be computed similarly. In this case, n  three-dimensional array, the entire 1/3 n  x 1/3 n  x 1/3 n rows. If the data are arranged in an
n
-point FFTs are computed over the elements of the columns of the array in all three dimensions, choosing one dimension at a time. We 1/3
. This algorithm can be divided into the following five phases: three-dimensional transpose algorithm call this algorithm the
-axis. z -point FFTs are computed on all the rows along the 1/3 n In the first phase, . 1
 plane are transposed. z - y  along the 1/3 n  x 1/3 n  cross-sections of size 1/3 n In the second phase, all the . 2
-axis. z -point FFTs are computed on all the rows of the modified array along the 1/3 n In the third phase, . 3
 plane is transposed. z - x  cross-sections along the 1/3 n  x 1/3 n In the fourth phase, each of the . 4
-axis are computed again. z -point FFTs of all the rows along the 1/3 n In the fifth and final phase, . 5
, in the first, third, and fifth phases of the algorithm, all processes perform Figure 13.10 For the data distribution shown in
. Since all the data for performing these computations are locally 1/3 n  FFT computations, each of size
available on each process, no interprocess communication is involved in these three odd-numbered phases. The time spent by a process
. Thus, the total time that a process spends in in each of these phases is
n. ) log p/ n ( ct computation is
 shows, the second Figure 13.11(a)  illustrates the second and fourth phases of the three-dimensional transpose algorithm. As Figure 13.11
 processes  plane. Each column of z - y  along the 1/3 n  x 1/3 n phase of the algorithm requires transposing square cross-sections of size
 such cross-sections. This transposition involves all-to-all personalized communications performs the transposition of
) is used, this phase p ( Q -node network with bisection width p . If a 3/2 p/ n  processes with individual messages of size among groups of
 processes , is similar. Here each row of Figure 13.11(b) . The fourth phase, shown in takes time
 data 1/3 n  x 1/3 n  plane. Again, each cross-section consists of z - x  cross-sections along the performs the transpose of
elements. The communication time of this phase is the same as that of the second phase. The total parallel run time of the
-point FFT is n three-dimensional transpose algorithm for an
3  Equation 13.1
-point n Figure 13.11. The communication (transposition) phases in the three-dimensional transpose algorithm for an
 processes. p FFT on
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
-dimensional transpose algorithm q Having studied the two- and three-dimensional transpose algorithms, we can derive a more general
 terms). Now the entire q  (a total of q 1/ n x  x··· q 1/ n  x q 1/ n -dimensional array of size q -point input be arranged in a logical n similarly. Let the
 subcomputations along a different dimension consists of q  subcomputations. Each of the q -point FFT computation can be viewed as n
n
, q -1)/ q ( n p  processes, where p  - 1)-dimensional array of q  data points. We map the array of data onto a logical ( q 1/ n  FFTs over q -1)/ q (
 - 1) phases (recall that there are three phases in the q . The FFT of the entire data is now computed in (2 s  for some integer s -1) q (  = 2 p and
 odd-numbered phases, each q two-dimensional transpose algorithm and five phases in the three-dimensional transpose algorithm). In the
 computation phases is the q -point FFTs. The total computation time for each process over all q 1/ n  of the required p/ q -1)/ q ( n process performs
-point FFTs computed by each process in each computation q 1/ n  (the number of p/ q -1)/ q ( n  (the number of computation phases), q product of
) log p/ n ( ct -point FFT). Multiplying these terms gives a total computation time of q 1/ n ) (the time to compute a single q 1/ n  log( q 1/ n ct phase), and
. n
-dimensional logical array of q  are transposed on rows of the q 1/ n  x q 1/ n  - 1) even-numbered phases, sub-arrays of size q In each of the (
 - 1)-dimensional q  processes. One such transpose is performed along every dimension of the ( -1) q 1/( p processes. Each such row contains
. p/ n wt  - 1) + -1) q 1/( p ( st  - 1) communication phases. The time spent in communication in each transposition is q process array in each of the (
) is p ( Q -node network with bisection width p -point FFT on a n -dimensional transpose algorithm for an q Thus, the total parallel run time of the
4  Equation 13.1
, respectively. 13.13  and 13.11  with 2 and 3, and comparing the result with Equations q  can be verified by replacing Equation 13.14
 of the transpose algorithm q  shows an interesting trend. As the dimension 13.4 , and 13.14 , 13.13 , 13.11 A comparison of Equations
 decreases. The binary-exchange algorithm and the st  increases, but that due to wt increases, the communication overhead due to
 but has the largest st two-dimensional transpose algorithms can be regarded as two extremes. The former minimizes the overhead due to
 . The variations of the transpose st  but has the largest overhead due to wt . The latter minimizes the overhead due to wt overhead due to
 determine wt  , and st , ct  lie between these two extremes. For a given parallel computer, the specific values of p  < log q algorithm for 2 <
which of these algorithms has the optimal parallel run time (Problem 13.8).
Note that, from a practical point of view, only the binary-exchange algorithm and the two- and three-dimensional transpose algorithms are
 limit their applicability. p  and n feasible. Higher-dimensional transpose algorithms are very complicated to code. Moreover, restrictions on
 must be a p , and that q  must be a power of two that is a multiple of n -dimensional transpose algorithm are that q These restrictions for a
 are integers. s , and r , q , where s -1) q (  = 2 p , and qr  = 2 n  - 1). In other words, q power of 2 that is a multiple of (
Example 13.2 A comparison of binary-exchange, 2-D transpose, and 3-D transpose algorithms
This example shows that either the binary-exchange algorithm or any of the transpose algorithms may be the algorithm of
choice for a given parallel computer, depending on the size of the FFT. Consider a 64-node version of the hypercube
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
 shows speedups attained by the binary-exchange Figure 13.12  = 4. wt  = 25, and st  = 2, ct  with Example 13.1 described in
algorithm, the 2-D transpose algorithm, and the 3-D transpose algorithm for different problem sizes. The speedups are
, respectively. The figure shows that for different 13.13 , and 13.11 , 13.4 based on the parallel run times given by Equations
-point FFT. For the given values of the hardware n , a different algorithm provides the highest speedup for an n ranges of
parameters, the binary-exchange algorithm is best suited for very low granularity FFT computations, the 2-D transpose
algorithm is best for very high granularity computations, and the 3-D transpose algorithm's speedup is the maximum for
intermediate granularities.
Figure 13.12. A comparison of the speedups obtained by the binary-exchange, 2-D transpose, and 3-D
 = 25. s t  = 4, and w t  = 2, c t transpose algorithms on a 64-node hypercube with
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
13.4 Bibliographic Remarks
Due to the important role of Fourier transform in scientific and technical computations, there has been great interest in implementing FFT
] describes many implementations of the FFT algorithm on Swa87 on parallel computers and on studying its performance. Swarztrauber [
] give a comprehensive performance analysis of the NS87 ] and Norton and Silberger [ Cve87 vector and parallel computers. Cvetanovic [
FFT algorithm on pseudo-shared-memory architectures such as the IBM RP-3. They consider various partitionings of data among
memory blocks and, in each case, obtain expressions for communication overhead and speedup in terms of problem size, number of
e ] analyze the performanc ACS89c processes, memory latency, CPU speed, and speed of communication. Aggarwal, Chandra, and Snir [
n of FFT and other algorithms on LPRAM – a new model for parallel computation. This model differs from the standard PRAM model i
d that remote accesses are more expensive than local accesses in an LPRAM. Parallel FFT algorithms and their implementation an
, DT89 , BKH89 , BCJ90 , Bai90 , AGGM90[ experimental evaluation on various architectures have been pursued by many other researchers
]. Loa92 , KA88 , JKFM89 , GK93b
The basic FFT algorithm whose parallel formulations are discussed in this chapter is called the unordered FFT because the elements of
the output sequence are stored in bit-reversed index order. In other words, the frequency spectrum of the input signal is obtained by
 is j ], where j  [ Y ] is replaced byi  [ Y , i  in such a way that for all Algorithm 13.2  produced by Y reordering the elements of the output sequence
. bit reversal ) and is known as Section 4.6 . This is a permutation operation ( i obtained by reversing the bits in the binary representation of
. p  = log d  + 1 communication steps, where d ] show that an ordered transform can be obtained with at most 2 NS87 Norton and Silberger [
 communication steps, the total communication overhead in the case of ordered d Since the unordered FFT computation requires only
FFT is roughly double of that for unordered FFT. Clearly, an unordered transform is preferred where applicable. The output sequence
]. In Swa87 need not be ordered when the transform is used as a part of a larger computation and as such remains invisible to the user [
many practical applications of FFT, such as convolution and solution of the discrete Poisson equation, bit reversal can be avoided
] for a distributed-memory parallel Loa92 ]. If required, bit reversal can be performed by using an algorithm described by Van Loan [ Loa92[
computer. The asymptotic communication complexity of this algorithm is the same as that of the binary-exchange algorithm on a
hypercube.
] show GK93b Several variations of the simple FFT algorithm presented here have been suggested in the literature. Gupta and Kumar [
that the total communication overhead for mesh and hypercube architectures is the same for the one- and two-dimensional FFTs.
Certain schemes for computing the DFT have been suggested that involve fewer arithmetic operations on a serial computer than the
]. Notable among these are computing one-dimensional FFTs with Win77 , RB76 , Nus82 simple Cooley-Tukey FFT algorithm requires [
radix greater than two and computing multidimensional FFTs by transforming them into a set of one-dimensional FFTs by using the
 each, q / n  sequences of size q  into n  FFT is computed by splitting the input sequence of size q polynomial transform method. A radix-
 smaller FFTs, and then combining the result. For example, in a radix-4 FFT, each step computes four outputs from four q computing the
. The input length should, of course, be a power of four. Despite the n 2  rather than log n 4 inputs, and the total number of iterations is log
 FFT remains the same as that for radix-2. For q reduction in the number of iterations, the aggregate communication time for a radixexample, for a radix-4 algorithm on a hypercube, each communication step now involves four processes distributed in two dimensions
rather than two processes in one dimension. In contrast, the number of multiplications in a radix-4 FFT is 25% fewer than in a radix-2
]. This number can be marginally improved by using higher radices, but the amount of communication remains unchanged. Nus82 FFT [
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Problems
. Consider its implementation on an n  log n ct -point FFT computation be n  Let the serial run time of an 13.1
 = 0.2. wt  = 1and ct . Assume that p )/ p  log n wt  + ( p )/ n  log n ct architecture on which the parallel run time is (
Write expressions for the speedup and efficiency. . 1
What is the isoefficiency function if an efficiency of 0.6 is desired? . 2
How will the isoefficiency function change (if at all) if an efficiency of 0.4 is desired? . 3
 = 1 and everything else is the same. wt Repeat parts 1 and 2 for the case in which . 4
 processes by using any mapping of data p  Show that, while performing FFT on a square mesh of ] Tho83 13.2 [
onto the processes, there is at least one iteration in which the pairs of processes that need to communicate are at
 links apart. least
 processes. What p  Describe the communication pattern of the binary-exchange algorithm on a linear array of 13.3
are the parallel run time, speedup, efficiency, and isoefficiency function of the binary-exchange algorithm on a
linear array?
 = 0, the isoefficiency function of the binary-exchange algorithm on a mesh is given by st  Show that, if 13.4
. Equation 13.10  Use Hint:
 Prove that the maximum number of twiddle factors computed by any process in the parallel implementation of 13.5
. Section 13.2.3  processes is given by the recurrence relation given in p -point FFT using n an
 Derive expressions for the parallel run time, speedup, and efficiency of the two-dimensional transpose 13.6
p -node two-dimensional mesh and a linear array of p -point FFT on a n  for an Section 13.3.1 algorithm described in
nodes.
-node mesh be increased so that it p , by what factor should the communication bandwidth of a st  Ignoring 13.7
-node p -point FFT on a n yields the same performance on the two-dimensional transpose algorithm for an
hypercube?
 = wt  = 250, st  You are given the following sets of communication-related constants for a hypercube network: (i) 13.8
 = 1. wt  = 0, st  =1, and (v) wt  = 2, st  =1, (iv) wt  = 10, st  = 1, (iii) wt  = 50, st 1, (ii)
Given a choice among the binary-exchange algorithm and the two-, three-, four-, and five-dimensional
 for each of the preceding sets 12  = 2 p  and 15  = 2 n transpose algorithms, which one would you use for
? wt  and st of values of
. 1
. 12  = 2 p , 20  = 2 n , and (b) 6  = 2 p , 12  = 2 n Repeat part 1 for (a) . 2
 mesh. If the channel bandwidth grows -point FFT on a n  Consider computing an ] GK93b 13.9 [
 in the mesh, show that the isoefficiency function due to p  > 0) with the number of nodes x ) ( x p ( Q at a rate of
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
). Also show that p  log 1+x p ( Q ) and that due to concurrency is x 0.5- p ) tw/tc 2( 2 x 0.5- p ( Q communication overhead is
), even if the channel bandwidth increases p  log 1.5 p ( Q the best possible isoefficiency for FFT on a mesh is
arbitrarily with the number of nodes in the network.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Appendix A. Complexity of Functions and Order
Analysis
Order analysis and the asymptotic complexity of functions are used extensively in this book to analyze the performance of algorithms.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
A.1 Complexity of Functions
When analyzing parallel algorithms in this book, we use the following three types of functions:
 if it can be expressed in the form x  function in exponential  from reals to reals is called anf  A function Exponential functions:
. x 1.5 , and 3 +2 x , 1.5 x  > 1. Examples of exponential functions are 2 a  (the set of real numbers) and a , x  for x a ) = x  (f
. 1
 if it can be expressed in x  in b degree  function of polynomial  from reals to reals is called af  A function Polynomial functions:
 function quadratic  function is a polynomial function of degree one and a linear  > 0. A b  and b , x  for b x ) = x  (f the form
. 2.3 x , and 5.5 x is a polynomial function of degree two. Examples of polynomial functions are 2, 5
 is also a polynomial function whose degree is equal to the h  and g  that is a sum of two polynomial functionsf A function
 is a polynomial function of degree two. 2 x  + x . For example, 2 h  and g maximum of the degrees of
. 2
 and b > 1 b  for x b ) = log x  (f  from reals to reals that can be expressed in the formf  A function Logarithmic functions:
 and x 5. 1  of the logarithm. Examples of logarithmic functions are log base  is called the b  In this expression, x.  in logarithmic is
 to denote x 2 , and log x 2  to denote log x . Unless stated otherwise, all logarithms in this book are of base two. We use log x 2 log
. 2 ) x 2 (log
. 3
) grows x  (f  if g  a function dominate  is said tof Most functions in this book can be expressed as sums of two or more functions. A function
. In other x ) is a monotonically increasing function in x ( g )/ x  (f  if and only if g  dominates functionf ). Thus, function x ( g at a faster rate than
. An exponential function 0 x  > x ) for x ( cg ) > x  (f  such that 0 x  > 0, there exists a value c  if and only if for any constant g  dominatesf words,
 is transitive. If dominates dominates a polynomial function and a polynomial function dominates a logarithmic function. The relation
. Thus, an exponential h  also dominates functionf , then function h  dominates function g , and function g  dominates functionf function
function also dominates a logarithmic function.
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
A.2 Order Analysis of Functions
In the analysis of algorithms, it is often cumbersome or impossible to derive exact expressions for parameters such as run time,
speedup, and efficiency. In many cases, an approximation of the exact expression is adequate. The approximation may indeed be more
illustrative of the behavior of the function because it focuses on the critical factors influencing the parameter.
Example A.1 Distances traveled by three cars
 is moving at a A  = 0, cart  = 0. Att . Assume that we start monitoring the cars at time C , and B , A Consider three cars
 's velocity is 100 feet per second and B  = 0, cart velocity of 1000 feet per second and maintains a constant velocity. At
 = 0 and accelerates at at  starts from a standstill at C it is accelerating at a rate of 20 feet per second per second. Car
 seconds byt ) represent the distances traveled int  ( C D ), andt  ( B D ),t ( A D rate of 25 feet per second per second. Let
. From elementary physics, we know that C , and B , A cars
 outperforms B  > 45 seconds, cart Now, we compare the cars according to the distance they travel in a given time. For
. A  outperforms car C  > 40 seconds, cart , and for B  outperforms car C  > 20 seconds, cart . Similarly, for A car
 > 20, which implies that after a certain time, the difference int ) fort  ( C D ) <t  ( B D ) andt  ( B D ) < 1.25t  ( C D Furthermore,
 is bounded by the other scaled by a constant multiplicative factor. All these facts can C  and B the performance of cars
be captured by the order analysis of the expressions.
B  > 20; that is, the difference in the performance of carst ) fort  ( C D ) <t ( B D ) andt ( B D ) < 1.25t  ( C D , Example A.1  From  Notation: Q The
 = 0 is bounded by the other scaled by a constant multiplicative factor. Such an equivalence in performance is oftent  after C and
) andt ( C D  notation captures the relationship between these two functions. The functions Q significant when analyzing performance. The
). 2t ( Q )). Furthermore, both functions are equal tot ( C D ( Q ) =t ( B D )) andt ( B D ( Q ) =t ( C D  notation as Q ) can be expressed by using thet ( B D
 > 0, there exists an 2 c , 1 c )) if and only if for any constants x ( g ( Q ) = x (f ), x ( g  notation is defined as follows: given a function Q Formally, the
. 0 x x ) for all x ( g 2 c ) x  (f ) x ( g 1 c  0 such that 0 x
 we have Example A.1  Often, we would like to bound the growth of a particular parameter by a simpler function. From  Notation: O The
 (big-oh) notation O ) is expressed using thet ( B D ) andt ( A D ). This relation betweent  ( A D ) is always greater thant ( B D  > 45,t seen that for
)).t ( B D  ( O ) =t ( A D as
0 x  > 0, their exists an c )) if and only if for any constant x ( g  ( O ) = x  (f ), x ( g  notation is defined as follows: given a function O Formally, the
)t ( A D ). Furthermore, 2t ( O ) =t ( B D ) and 2t ( O ) =t ( A D . From this definition we deduce that 0 x x ) for all x ( cg ) x  (f  0 such that
 notation. O ) also satisfies the conditions of thet  ( O =
 notation; O  notation is the converse of the W  notation sets an upper bound on the rate of growth of a function. The O  The  Notation: W The
 > 40. This relationship can bet ) fort  ( C D ) <t ( A D , Example A.1 that is, it sets a lower bound on the rate of growth of a function. From
)).t ( A D ( W ) =t  ( C D  notation as W expressed using the
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
) for x ( cg ) x  (f  0 such that 0 x  0, there exists an > )) if and only if for any constant c x ( g ( W ) = x  (f ), x ( g Formally, given a function
. 0 x x all
Properties of Functions Expressed in Order Notation
The order notations for expressions have a number of properties that are useful when analyzing the performance of algorithms. Some of
the important properties are as follows:
x
. b a ) if and only if b x ( O  = a . 1
. b  and a )) for all x ( b (log Q ) = x  ( a log . 2
a
. b a ) if and only if x b  ( O  = x . 3
 (1). O  = c , c For any constant . 4
). g  ( O  = g  +f ) then g  ( O  =f If . 5
).f ( Q ) = g ( Q  = g ) then f + g ( Q  =f If . 6
).f ( W  = g ) if and only if g  ( O  =f . 7
. ) g  O( f = ) and g ( W f = ) if and only if g ( Q  =f . 8
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
[ Team LiB ]
Bibliography
IEEE Transactions on . Design and implementation of a parallel tree search algorithm [ABJ82] S. G. Akl, D. T. Bernard, and R. J. Jordan.
. , PAMI-4:192–203, 1982 Pattern Analysis and Machine Intelligence
. . ACM Press, New York, NY, 1991 Resources in Parallel and Concurrent Systems [ACM91] ACM.
. . Technical Report RC 15118 (No. 67337), IBM T. J A model for hierarchical memory [ACS89a] A. Aggarwal, A. K. Chandra, and M. Snir.
Watson Research Center, Yorktown Heights, NY, 1989.
. Technical Report RC 14973 (No. On communication latency in PRAM computations [ACS89b] A. Aggarwal, A. K. Chandra, and M. Snir.
. 66882), IBM T. J. Watson Research Center, Yorktown Heights, NY, 1989
. . Technical Report RC 14998 (64644), IBM T. J Communication complexity of PRAMs [ACS89c] A. Aggarwal, A. K. Chandra, and M. Snir.
Watson Research Center, Yorktown Heights, NY, 1989.
d 91] A. Agarwal, G. D'Souza, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B.-H. Lim, G. Maa, D. Nussbaum, M. Parkin, an + [ADJ
Proceedings of Workshop on Scalable Shared . In The MIT alewife machine : A large-scale distributed-memory multiprocessor D. Yeung.
. . Kluwer Academic, 1991 Memory Multiprocessors
Solving Problems on Concurrent Processors: Software for Concurrent [AFKW90] I. Angus, G. C. Fox, J. Kim, and D. W. Walker.
. . Prentice-Hall, Englewood Cliffs, NJ, 1990 Processors: Volume II
. Benjamin/Cummings, Redwood City, CA, 1994. (Second Edition). Highly Parallel Computing [AG94] G. S. Almasi and A. Gottlieb.
f . Technical Report 89-566, Massachusetts Institute o Performance tradeoffs in multithreaded processors [Aga89] A. Agarwal.
Technology, Microsystems Program Office, Cambridge, MA, 1989.
. Technical report MIT/LCS/TR 501; VLSI memo no. 89-566, Performance tradeoffs in multithreaded processors [Aga91] A. Agarwal.
Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA, 1991.
, , 15:61–74 Parallel Computing . A parallel FFT on an MIMD machine [AGGM90] A. Averbuch, E. Gabber, B. Gordissky, and Y. Medan.
. 1990
. MIT Press, Cambridge, MA, 1986. Actors: A Model of Concurrent Computation in Distributed Systems [Agh86] G. Agha.
Deterministic simulation of idealized parallel computers on more realistic [AHMP87] H. Alt, T. Hagerup, K. Mehlhorn, and F. P. Preparata.
. , 16(5):808–835, October 1987 SIAM Journal of Computing . ones
, . Addison-Wesley, Reading, MA The Design and Analysis of Computer Algorithms [AHU74] A. V. Aho, J. E. Hopcroft, and J. D. Ullman.
1974.
Proceedings of the . In A randomized parallel branch-and-bound algorithm [AJM88] D. P. Agrawal, V. K. Janakiram, and R. Mehrotra.
, 1988. 1988 International Conference on Parallel Processing
y , 31(3):649–667, Jul Journal of ACM . Graph problems on a mesh-connected processor array [AK84] M. J. Atallah and S. R. Kosaraju.
. 1984
. . Academic Press, San Diego, CA, 1985 Parallel Sorting Algorithms [Akl85] S. G. Akl.
. . Prentice-Hall, Englewood Cliffs, NJ, 1989 The Design and Analysis of Parallel Algorithms [Akl89] S. G. Akl.
. . Prentice-Hall, Englewood Cliffs, NJ, 1997 Parallel Computation Models and Methods [Akl97] S. G. Akl.
Proceedings of the 1989 International . In Floorplan optimization on multiprocessors [AKR89] S. Arvindam, V. Kumar, and V. N. Rao.
, 1989. Also published as Technical Report ACT-OODS-241-89, Microelectronics and Computer Conference on Computer Design
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Corporation, Austin, TX
. In Efficient parallel algorithms for search problems: Applications in VLSI CAD [AKR90] S. Arvindam, V. Kumar, and V. N. Rao.
, 1990. Proceedings of the Third Symposium on the Frontiers of Massively Parallel Computation
, Parallel Computing . Automatic test pattern generation on multiprocessors [AKRS91] S. Arvindam, V. Kumar, V. N. Rao, and V. Singh.
. 17, number 12:1323–1342, December 1991
Proceedings of the 15th Annual ACM Symposium on . In ) sorting network n  log n  ( O An [AKS83] M. Ajtai, J. Komlos, and E. Szemeredi.
. , 1–9, 1983 Theory of Computing
. . Prentice-Hall, Englewood Cliffs, NJ, 1993 Parallel Computational Geometry [AL93] S. G. Akl and K. A. Lyons.
Proceedings of the . In Parallel branch-and-bound algorithms on hypercube multiprocessors [AM88] T. S. Abdelrahman and T. N. Mudge.
. , 1492–1499, New York, NY, 1988. ACM Press Third Conference on Hypercubes, Concurrent Computers, and Applications
AFIPS Conference . In Validity of the single processor approach to achieving large scale computing capabilities [Amd67] G. M. Amdahl.
. , 483–485, 1967 Proceedings
. . Benjamin/Cummings, Redwood City, CA, 1991 Concurrent Programming: Principles and Practice [And91] G. R. Andrews.
IEEE Transactions on Parallel and . Balanced parallel sort on hypercube multiprocessors [AOB93] B. Abali, F. Ozguner, and A. Bataineh.
. , 4(5):572–581, May 1993 Distributed Systems
IEEE Transactions . New connectivity and MSF algorithms for shuffle-exchange network and PRAM [AS87] B. Awerbuch and Y. Shiloach.
. , C– 36(10):1258–1263, October 1987 on Computers
d . Prentice-Hall, Englewoo The Theory of Parsing, Translation and Compiling: Volume 1, Parsing [AU72] A. V. Aho and J. D. Ullman.
. Cliffs, NJ, 1972
. . SIAM, 1997 ScaLAPACK Users' Guide 97] L. S. Blackford et al. + [B
. . Prentice-Hall, Englewood Cliffs, NJ, 1982 Principles of Concurrent Programming [BA82] M. Ben-Ari.
. . Addison-Wesley, Reading, MA, 1988 Programming Parallel Processors [Bab88] R. G. Babb.
. , 4:23–35, 1990 Journal of Supercomputing . FFTs in external or hierarchical memory [Bai90] D. H. Bailey.
. , C-17(8):746–757, 1968 IEEE Transactions on Computers . The ILLIAC IV computer [Bar68] G. H. Barnes.
,  , 307–314 Proceedings of the 1968 Spring Joint Computer Conference . In Sorting networks and their applications [Bat68] K. E. Batcher.
. 1968
. , 65–71, 1976 Proceedings of International Conference on Parallel Processing . In The Flip network in STARAN [Bat76] K. E. Batcher.
. , 836–840, September 1980 IEEE Transactions on Computers . Design of a massively parallel processor [Bat80] K. E. Batcher.
n . Ph.D. Thesis, Carnegie-Mello The Design and Analysis of Algorithms for Asynchronous Multiprocessors [Bau78] G. M. Baudet.
. University, Pittsburgh, PA, 1978
. In Approximate algorithms for the partitionable independent task scheduling problem [BB90] K. P. Belkhale and P. Banerjee.
. , I72–I75, 1990 Proceedings of the 1990 International Conference on Parallel Processing
. . Cambridge, MA. 1989 TC-2000 Technical Product Summary [BBN89] BBN Advanced Computers Inc.
. Technical Report CRPC TR 95554, Center Parallel mixed integer programming [BCCL95] R. E. Bixby, W. Cook, A. Cox, and E. K. Lee.
for Research on Parallel Computation, Research Monograph, 1995.
Experimental application-driven architecture analysis of an SIMD/MIMD [BCJ90] E. C. Bronson, T. L. Casavant, and L. H. Jamieson.
. , 1(2):195–205, 1990 IEEE Transactions on Parallel and Distributed Systems . parallel processing system
, , 16(3):287–318 Computing Surveys . A taxonomy of parallel sorting [BDHM84] D. Bitton, D. J. DeWitt, D. K. Hsiao, and M. J. Menon.
. September 1984
. . Princeton University Press, Princeton, NJ, 1957 Dynamic Programming [Bel57] R. Bellman.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. , 16(1):87–90, 1958 Quarterly of Applied Mathematics . On a routing problem [Bel58] R. Bellman.
. , 27(1):51–59, March 1980 Journal of the ACM . A parallel algorithm for constructing minimum spanning trees [Ben80] J. L. Bentley.
Information Processing . On computing the determinant in small parallel time using a small number of processors [Ber84] S. Berkowitz.
. , 18(3):147–150, March 1984 Letters
. , 12:335–342, 1989 Parallel Computing . Communication efficient matrix multiplication on hypercubes [Ber89] J. Berntsen.
Proceedings of the 14th Annual . In Routing merging and sorting on parallel models of computation [BH82] A. Borodin and J. E. Hopcroft.
. , 338–344, May 1982 ACM Symposium on Theory of Computing
Proceedings of the Workshop on Parallel Computing of Discrete . In Two applications of linear programming [Bix91] R. E. Bixby.
, 1991. Optimization Problems
. In Cilk: An efficient multithreaded runtime system 95] R. Blumofe, C. Joerg, B. Kuszmaul, C. Leiserson, K. Randall, and Y. Zhou. + [BJK
, 1995. Proceedings of the 5th Symposium on Principles and Practice of Parallel Programming
Proceedings of the Fourth Conference on Hypercubes, . In The giant-Fourier-transform [BKH89] S. Bershader, T. Kraay, and J. Holland.
. , 387–389, 1989 Concurrent Computers, and Applications: Volume I
. . MIT Press, Cambridge, MA, 1990 Vector Models for Data-Parallel Computing [Ble90] G. E. Blelloch.
Solving large-scale qap problems in parallel with the search library [BMCP98] A. Brungger, A. Marzetta, J. Clausen, and M. Perregaard.
. , 50:157–169, 1998 Journal of Parallel and Distributed Computing . zram
Proceedings . In Designing broadcasting algorithms in the postal model for message-passing systems [BNK92] A. Bar-Noy and S. Kipnis.
. , 13–22, 1992 of 4th ACM Symposium on Parallel Algorithms and Architectures
. Optimal communication algorithms for hypercubes 91] D. P. Bertsekas, C. Ozveren, G. D. Stamoulis, P. Tseng, and J. N. Tsitsiklis. + [BOS
. , 11:263–275, 1991 Journal of Parallel and Distributed Computing
On optimal and practical routing methods for a massive data movement operation on [BR90] R. Boppana and C. S. Raghavendra.
. . Technical report, University of Southern California, Los Angeles, CA, 1990 hypercubes
Technology news & reviews: Chemkin software; OpenMP Fortran Standard; ODE toolbox for Matlab; Java products; [Bra97] R. Bramley.
. , 4(4):75–78, October/December 1997 IEEE Computational Science and Engineering . Scientific WorkPlace 3.0
, . Technical Report CMU-CS-79-106, Carnegie Mellon University Dynamic programming in computer science [Bro79] K. Brown.
Pittsburgh, PA, 1979.
, IEEE Transactions on Computers . Optimal sorting algorithms for parallel computers [BS78] G. M. Baudet and D. Stevenson.
. C–27(1):84–87, January 1978
. . Prentice-Hall, NJ, 1989 Parallel and Distributed Computation: Numerical Methods [BT89] D. P. Bertsekas and J. N. Tsitsiklis.
. . Athena Scientific, 1997 Parallel and Distributed Computation: Numerical Methods [BT97] D. P. Bertsekas and J. N. Tsitsiklis.
. . Addison-Wesley, Reading, MA, 1997 Programming with POSIX Threads [But97] D. R. Butenhof.
. . Prentice Hall, 1999 High Performance Cluster Computing: Architectures and Systems [Buy99] R. Buyya, editor.
. In Computing performance as a function of the speed, quantity, and the cost of processors [BW89] M. L. Barton and G. R. Withers.
. , 759–764, 1989 Supercomputing '89 Proceedings
s . Addison-Wesley Developer Multithreading Applications in Win32: the Complete Guide to Threads [BW97] J. Beveridge and R. Wiener.
. Press, Reading, MA, 1997
e . Technical Report CS-95-292, Computer Scienc A proposal for a set of Parallel Basic Linear Algebra Subprograms 95] J. Choi et al. + [C
Department, University of Tennessee, 1995.
The parallelization of some level 2 and 3 BLAS [CAHH91] N. P. Chrisopchoides, M. Aboelaze, E. N. Houstis, and C. E. Houstis.
Proceedings of the First International Conference of the Austrian Center of Parallel . In operations on distributed-memory machines
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Springer-Verlag Series Lecture Notes in Computer Science, 1991. Computation
, . Ph.D. Thesis, Montana State University, Bozman A cellular computer to implement the Kalman Filter Algorithm [Can69] L. E. Cannon.
. MT, 1969
. . Wiley, New York, NY, 1989 Parallel Supercomputing: Methods, Algorithms and Applications [Car89] G. F. Carey, editor.
, . SIAM Parallel Processing . In R. Porth, editor, An approach to parallel vision algorithms [CD87] S. Chandran and L. S. Davis.
. Philadelphia, PA, 1987
n . Morga Parallel Programming in OpenMP 00] R. Chandra, L. Dagum, D. Kohr, D. Maydan, J. McDonald, and R. M. (editors). + [CDK
. Kaufmann Publishers, 2000
, Parallel Computing . Gaussian elimination with partial pivoting and load balancing on a multiprocessor [CG87] E. Chu and A. George.
. 5:65–74, 1987
Proceedings of the IEEE Conference . In Parallel search algorithms for robot motion planning [CGK93] D. Challou, M. Gini, and V. Kumar.
. , 46–51, 1993 on Robotics and Automation
. Report UCB/CSD Analysis of multithreaded microprocessors under multiprogramming [CGL92] D. E. Culler, M. Gunter, and J. C. Lee.
. 92/687, University of California, Berkeley, Computer Science Division, Berkeley, CA, May 1992
, . Technical Report RC-6193, IBM T. J. Watson Research Center Maximal parallelism in matrix multiplication [Cha79] A. K. Chandra.
Yorktown Heights, NY, 1979.
Hypercube . In M. T. Heath, editor, An alternate view of LU factorization on a hypercube multiprocessor [Cha87] R. Chamberlain.
. , 569–575. SIAM, Philadelphia, PA, 1987 Multiprocessors 1987
, Operations Research . Solving large-scale zero-one linear programming problem [CJP83] H. Crowder, E. L. Johnson, and M. Padberg.
. 2:803–834, 1983
LogP: Towards a 93a] D. Culler, R. Karp, D. Patterson, A. Sahay, K. Schauser, E. Santos, R. Subramonian, and T. von Eicken. + [CKP
Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel . In realistic model of parallel computation
. , 1–12, 1993 Programming
Principles and . In Logp: Towards a realistic model of parallel computation 93b] D. E. Culler, R. Karp, D. A. Patterson, et al. + [CKP
. , May 1993 Practices of Parallel Programming
. . Addison-Wesley, 1993 Introduction to Parallel Processing [CL93] B. Codenotti and M. Leoncini.
Proceedings of the 1981 . In Optimal parallel algorithms for the connected component problem [CLC81] F. Y. Chin, J. Lam, and I. Chen.
. , 170–175, 1981 International Conference on Parallel Processing
, Communications of the ACM . Efficient parallel algorithms for some graph problems [CLC82] F. Y. Chin, J. Lam, and I. Chen.
. 25(9):659–665, September 1982
. . MIT Press, McGraw-Hill, New York, NY, 1990 Introduction to Algorithms [CLR90] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.
, Communications of the ACM . Distributed computation on graphs: Shortest path algorithms [CM82] K. M. Chandy and J. Misra.
. 25(11):833–837, November 1982
. , 1470, 1998 Lecture Notes in Computer Science . OpenMP and HPF: Integrating two paradigms [CM98] B. Chapman and P. Mehrotra.
. , 17(4):770–785, August 1988 SIAM Journal on Computing . Parallel merge sort [Col88] R. Cole.
. . MIT Press, Cambridge, MA, 1989 Algorithmic Skeletons: Structured Management of Parallel Computation [Col89] M. Cole.
. . Addison-Wesley, Reading, MA, 1989 Programming in PARLOG [Con89] T. Conlon.
s . Technical Report AFWL-TR-89-01, Air Force Weapon A model of parallel performance [CR89] E. A. Carmona and M. D. Rice.
Laboratory, 1989.
Journal of Parallel and Distributed . Modeling the serial and parallel fractions of a parallel algorithm [CR91] E. A. Carmona and M. D. Rice.
, 1991. Computing
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Journal of . Efficient mapping and implementations of matrix algorithms on a hypercube [CS88] B. V. Cherkassky and R. Smith.
. , 2:7–27, 1988 Supercomputing
, . Morgan Kaufmann Parallel Computer Architecture: A Hardware/Software Approach [CSG98] D. E. Culler, J. P. Singh, and A. Gupta.
1998.
. . Jones and Bartlett, Austin, TX, 1992 An Introduction to Parallel Programming [CT92] K. M. Chandy and S. Taylor.
, 1991. Journal of Parallel and Distributed Processing . Parallel quicksort [CV91] B. S. Chlebus and I. Vrto.
IBM Journal of Research . Performance analysis of the FFT algorithm on a shared-memory parallel architecture [Cve87] Z. Cvetanovic.
. , 31(4):435–451, 1987 and Development
. . O'Reilly & Associates, 1998 Win32 Multithreaded Programming [CWP98] A. Cohen, M. Woodring, and R. Petrusha.
. , 12(2):23–39, 1992 IEEE Micro . The message-driven processor 92] W. J. Dally et al. + [D
. . Kluwer Academic Publishers, Boston, MA, 1987 A VLSI Architecture for Concurrent Data Structures [Dal87] W. J. Dally.
, 39(6), June 1990. IEEE Transactions on Computers . Analysis of k-ary n-cube interconnection networks [Dal90a] W. J. Dally.
VLSI and . In R. Sauya and G. Birtwistle, editors, Network and processor architecture for message-driven computers [Dal90b] W. J. Dally.
. . Morgan Kaufmann, San Mateo, CA, 1990 Parallel Computation
SIAM Journal on Algebraic and Discrete . Column LU factorization with pivoting on a hypercube multiprocessor [Dav86] G. J. Davis.
. , 7:538–550, 1986. Also available as Technical Report ORNL-6219, Oak Ridge National Laboratory, Oak Ridge, TN, 1985 Methods
Vectorization and multitasking of dynamic programming in control: experiments on [DCG90] J. D. DeMello, J. L. Calvet, and J. M. Garcia.
. , 13:261–269, 1990 Parallel Computing . a CRAY-2
Numerical Linear Algebra for High Performance Computers (Software, [DDSV99] J. Dongarra, I. S. Duff, D. Sorensen, and H. V. Vorst.
. . SIAM, 1999 Environments, Tools)
. The Technology of Parallel Processing: Parallel Processing Architectures and VLSI Hardware: Volume 1 [DeC89] A. L. DeCegama.
. Prentice-Hall, Englewood Cliffs, NJ, 1989
, . Addison-Wesley Parallel Processing for Computer Vision and Display [DEH89] P. M. Dew, R. A. Earnshaw, and T. R. Heywood.
. Reading, MA, 1989
. , C-31(4):278–288, 1982 IEEE Transactions on Computers . Experiences with multiprocessor algorithms [Dem82] J. Deminet.
. Technical Report A taxonomy of parallel sorting algorithms [DFHM82] D. J. DeWitt, D. B. Friedland, D. K. Hsiao, and M. J. Menon.
. TR-482, Computer Sciences Department, University of Wisconsin, Madison, WI, 1982
. Scalable parallel computational geometry for coarse grained multicomputers [DFRC96] F. Dehne, A. Fabri, and A. Rau-Chaplin.
. , 6(3):379–400, 1996 International Journal on Computational Geometry
. , 111–197, 1993 Acta Numerica . Parallel numerical linear algebra [DHvdV93] J. W. Demmel, M. T. Heath, and H. A. van der Vorst.
. , 1:269–271, 1959 Numerische Mathematik . A note on two problems in connection with graphs [Dij59] E. W. Dijkstra.
Proceedings of the . In Parallel A* algorithms and their performance on hypercube multiprocessors [DM93] S. Dutt and N. R. Mahapatra.
. , 797–803, 1993 Seventh International Parallel Processing Symposium
IEEE Computational Science . OpenMP: An industry-standard API for shared-memory programming [DM98] L. Dagum and R. Menon.
. , 5(1):46–55, January/March 1998 and Engineering
. , 10:657–673, 1981 SIAM Journal on Computing . Parallel matrix and graph algorithms [DNS81] E. Dekel, D. Nassimi, and S. Sahni.
. , 1(2), April 1996 JavaWorld: IDG's magazine for the Java community . Introduction to Java threads [Dra96] D. G. Drake.
Proceedings of the IBM Kingston Parallel . In VM parallel environment [DRGNP] F. Darema-Rogers, D. George, V. Norton, and G. Pfister.
. Processing Symposium
. , 1(3):187–196, 1986 Journal of Distributed Computing . The torus routing chip [DS86] W. J. Dally and C. L. Seitz.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
IEEE Transactions on . Deadlock-free message routing in multiprocessor interconnection networks [DS87] W. J. Dally and C. L. Seitz.
. , C-36(5):547– 553, 1987 Computers
Derivation of a termination detection algorithm for a distributed [DSG83] E. W. Dijkstra, W. H. Seijen, and A. J. M. V. Gasteren.
. , 16(5):217–219, 1983 Information Processing Letters . computation
Proceedings . In Implementing the discrete Fourier transform on a hypercube vector-parallel computer [DT89] L. Desbat and D. Trystram.
. , 407– 410, 1989 of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications: Volume I
, IEEE Transactions on Computers . Optimal graph algorithms on a fixed-size linear array [DV87] K. A. Doshi and P. J. Varman.
. C–36(4):460–470, April 1987
Proceedings of the Fourth Conference on . In Multicomputer matrix computations: Theory and practice [dV89] E. F. V.de Velde.
. , 1303–1308, 1989 Hypercubes, Concurrent Computers, and Applications
Proceedings of the 1981 International . In Parallel algorithms for the minimum spanning tree problem [DY81] N. Deo and Y. B. Yoo.
. , 188–189, 1981 Conference on Parallel Processing
, SIAM Journal on Optimization . Parallel branch-and-bound methods for mixed-integer programming on the cm-5 [Eck94] J. Eckstein.
. 4(4):794–814, 1994
Distributed versus centralized storage and control for parallel branch and bound: Mixed integer programming on the [Eck97] J. Eckstein.
. , 7(2):199–220, 1997 Computational Optimization and Applications . cm-5
. s Optimal matrix transposition and bit-reversal on hypercubes: Node address–memory address exchange [Ede89] A. Edelman.
. Technical report, Thinking Machines Corporation, Cambridge, MA, 1989
, IEEE Transactions on Computers . Distributed enumeration on network computers [EDH80] O. I. El-Dessouki and W. H. Huen.
. C-29:818–825, September 1980
Modified cyclic algorithms for solving triangular systems on [EHHR88] S. C. Eisenstat, M. T. Heath, C. S. Henkel, and C. H. Romine.
. , 9(3):589–600, 1988 SIAM Journal on Scientific and Statistical Computing . distributed-memory multiprocessors
. PRA*: A memory-limited heuristic search procedure for the connection machine [EHMN90] M. Evett, J. Hendler, A. Mahanti, and D. Nau.
. , 145– 149, 1990 Proceedings of the Third Symposium on the Frontiers of Massively Parallel Computation In
. , 21(7):801–803, 1972 IEEE Transactions on Computers . A fast computer method for matrix transposing [Ekl72] J. O. Eklundh.
LPAR '92: Logic Programming and . In A. Voronokov, editor, n OR—parallel theorem proving with random competitio [Ert92] W. Ertel.
. , 226–237. Springer-Verlag, New York, NY, 1992 Automated Reasoning
, IEEE Transactions on Computers . Speedup versus efficiency in parallel systems [EZL89] D. L. Eager, J. Zahorjan, and E. D. Lazowska.
. 38(3):408–423, 1989
. , 12–27, December 1981 IEEE Computer . A survey of interconnection networks [Fen81] T. Y. Feng.
. , 19:89–106, 1982 Artificial Intelligence . Parallelism in alpha-beta search [FF82] R. A. Finkel and J. P. Fishburn.
e . Technical Report CCCP-314, California Institut Optimal communication algorithms on hypercube [FF86] G. C. Fox and W. Furmanski.
of Technology, Pasadena, CA, 1986.
. . MIT Press, 1996 Introduction to High-Performance Scientific Computing [FJDS96] L. Fosdick, E. Jessup, G. Domik, and C. Schauble.
Solving Problems on Concurrent Processors: Volume 88] G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. + [FJL
. . Prentice-Hall, Englewood Cliffs, NJ, 1988 1
Proceedings of the 1988 National . In Distributed tree search and its application to alpha-beta pruning [FK88] C. Ferguson and R. Korf.
, 1988. Conference on Artificial Intelligence
. , 12:1–20, 1989 Parallel Computing . Performance of parallel processors [FK89] H. P. Flatt and K. Kennedy.
. , 1986. Hm 244 Caltech/JPL . Sorting on a hypercube [FKO86] E. Felten, S. Karlin, and S. W.Otto.
o . Technical Report G320-3540, IBM Corporation, Pal Further applications of the overhead model for parallel systems [Fla90] H. P. Flatt.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Alto Scientific Center, Palo Alto, CA, 1990.
, 5(6):345, June 1962. Communications of the ACM . Algorithm 97: Shortest path [Flo62] R. W. Floyd.
. , C-21(9):948–960, 1972 IEEE Transactions on Computers . Some computer organizations and their effectiveness [Fly72] M. J. Flynn.
. . Jones and Bartlett, 1995 Computer Architecture: Pipelined and Parallel Processor Design [Fly95] M. J. Flynn.
, Journal of the ACM . Samplesort: A sampling approach to minimal storage tree sorting [FM70] W. D. Frazer and A. C. McKellar.
. 17(3):496–507, July 1970
ACM Transactions on Programming Languages . g DIB—a distributed implementation of backtrackin [FM87] R. A. Finkel and U. Manber.
. , 9(2):235–256, April 1987 and Systems
. In Load balancing algorithms on the connection machine and their use in Monte-Carlo methods [FM92] R. Frye and J. Myczkowski.
, 1992. Proceedings of the Unstructured Scientific Computation on Multiprocessors Conference
Proc. of the . In Studying overheads in massively parallel min/max-tree evaluation [FMM94] R. Feldmann, P. Mysliwietz, and B. Monien.
. , 94–103, 1994 6th ACM Symposium on Parallel Algorithms and Architectures
, , 4:17–31 Parallel Computing . Matrix algorithms on a hypercube I: Matrix multiplication [FOH87] G. C. Fox, S. W. Otto, and A. J. G. Hey.
. 1987
, . Addison-Wesley Designing and Building Parallel Programs: Concepts and Tools for Parallel Software Engineering [Fos95] I. Foster.
1995.
. . Cambridge University Press, 1994 Parallel Computing: Principles and Practice [Fou94] T. J. Fountain.
. . Princeton University Press, Princeton, NJ, 1962 Flows in Networks [FR62] L. R. Ford and R. L. Rivest.
. . Technical Report CS-TR-1993-1196, University of Wisconsin, 1993 The multiscalar architecture [Fra93] M. Franklin.
A multi-level load balancing scheme for OR-parallel exhaustive search programs on the [FTI90] M. Furuichi, K. Taki, and N. Ichiyoshi.
. , 50–59, 1990 Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming . In Multi-PSI
, Proceedings of ACM Symposium on Theory of Computing . In Parallelism in random access machines [FW78] S. Fortune and J. Wyllie.
. 114–118, 1978
. . O'Reilly & Associates, 1995 Posix. 4 : Programming for the Real World [Gal95] B. O. Gallmeister.
, . MIT Press PVM: Parallel Virtual Machine 94] G. A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. + [GBD
. Cambridge, MA, 1994
k . Technical Report ORNL-6211, Oa Efficient parallel LU factorization with pivoting on a hypercube multiprocessor [Gei85] G. A. Geist.
Ridge National Laboratory, Oak Ridge, TN, 1985.
, The NYU Ultracomputer—designing a MIMD 83] A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. + [GGK
. , C–32(2):175–189, February 1983 IEEE Transactions on Computers . r shared memory parallel compute
IEEE . Isoefficiency: Measuring the scalability of parallel algorithms and architectures [GGK93] A. Y. Grama, A. Gupta, and V. Kumar.
. , 1(3):12–21, August 1993 Parallel and Distributed Technology
k . Technical Report ORNL-6190, Oa Parallel Cholesky factorization on a hypercube multiprocessor [GH85] G. A. Geist and M. T. Heath.
Ridge National Laboratory, Oak Ridge, TN, 1985.
Hypercube . In M. T. Heath, editor, Matrix factorization on a hypercube multiprocessor [GH86] G. A. Geist and M. T. Heath.
. , 161–180. SIAM, Philadelphia, PA, 1986 Multiprocessors 1986
. . SIAM, 2001 Performance Optimization of Numerically Intensive Codes [GH01] S. Goedecker and A. Hoisie.
. . Cambridge University Press, Cambridge, 1985 Algorithmic Graph Theory [Gib85] A. Gibbons.
Proceedings of the 1989 ACM Symposium on Parallel Algorithms and . In A more practical PRAM model [Gib89] P. B. Gibbons.
. , 158–168, 1989 Architectures
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, . Technical Report TR 91-54 The scalability of matrix multiplication algorithms on parallel computers [GK91] A. Gupta and V. Kumar.
Proceedings of 1993 Department of Computer Science, University of Minnesota, Minneapolis, MN, 1991. A short version appears in
. , pages III-115–III-119, 1993 International Conference on Parallel Processing
, Journal of Parallel and Distributed Computing . Performance properties of large scale parallel systems [GK93a] A. Gupta and V. Kumar.
, 19:234–244, 1993. Also available as Technical Report TR 92-32, Department of Computer Science, University of Minnesota
. Minneapolis, MN
, IEEE Transactions on Parallel and Distributed Systems . The scalability of FFT on parallel computers [GK93b] A. Gupta and V. Kumar.
y 4(8):922–932, August 1993. A detailed version is available as Technical Report TR 90-53, Department of Computer Science, Universit
. of Minnesota, Minneapolis, MN
Encyclopaedia of . In Parallel processing of discrete optimization problems [GKP92] A. Grama, V. Kumar, and P. M. Pardalos.
. . Marcel Dekker Inc., New York, 1992 Microcomputers
Proceedings . In Experimental evaluation of load balancing techniques for the hypercube [GKR91] A. Y. Grama, V. Kumar, and V. N. Rao.
. , 497–514, 1991 of the Parallel Computing '91 Conference
. In : A simple and asymptotically accurate model for parallel computation 3 A [GKRS96] A. Grama, V. Kumar, S. Ranka, and V. Singh.
. , Annapolis, MD, 1996 Proceedings of the Sixth Symposium on Frontiers of Massively Parallel Computing
Performance and scalability of preconditioned conjugate gradient methods on parallel [GKS92] A. Gupta, V. Kumar, and A. H. Sameh.
t . Technical Report TR 92-64, Department of Computer Science, University of Minnesota, Minneapolis, MN, 1992. A shor computers
. , pages 664–674, 1993 Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing version appears in
Proceedings of . In Direct VLSI Implementation of Combinatorial Algorithms [GKT79] L. J. Guibas, H. T. Kung, and C. D. Thompson.
. , 509–525, 1979 Conference on Very Large Scale Integration, California Institute of Technology
. . The Johns Hopkins University Press, Baltimore, MD, 1996 Matrix Computations [GL96a] G. H. Golub and C. V. Loan.
e . Mathematics and Computer Scienc , a Portable Implementation of MPI mpich User's Guide for [GL96b] W. D. Gropp and E. Lusk.
. Division, Argonne National Laboratory. ANL-96/6. 1996
A high-performance, portable implementation of the MPI message passing [GLDS96] W. Gropp, E. Lusk, N. Doss, and A. Skjellum.
. , 22(6):789–828, September 1996 Parallel Computing . interface standard
. . MIT Press, 1999. 2nd Edition Using MPI [GLS99] W. Gropp, E. Lusk, and A. Skjellum.
SIAM . Development of parallel methods for a 1024-processor hypercube [GMB88] J. L. Gustafson, G. R. Montry, and R. E. Benner.
. , 9(4):609–638, 1988 Journal on Scientific and Statistical Computing
. . Academic Press, 1993 Scientific Computing: An Introduction with Parallel Computing [GO93] G. H. Golub and J. M. Ortega.
, SIAM Review  . Parallel algorithms for dense linear algebra computations [GPS90] K. A. Gallivan, R. J. Plemmons, and A. H. Sameh.
, . SIAM, Philadelphia, PA Parallel Algorithms for Matrix Computations 32(1):54–135, March 1990. Also appears in K. A. Gallivan et al.
1990.
SIAM Journal on . LU factorization algorithms on distributed-memory multiprocessor architectures [GR88] G. A. Geist and C. H. Romine.
l , 9(4):639–649, 1988. Also available as Technical Report ORNL/TM-10383, Oak Ridge Nationa Scientific and Statistical Computing
. Laboratory, Oak Ridge, TN, 1987
. . Cambridge University Press, Cambridge, UK, 1990 Efficient Parallel Algorithms [GR90] A. Gibbons and W. Rytter.
. . MIT Press, Cambridge, MA, 1991 Parallel Processing for Computer Graphics [Gre91] S. Green.
. . MIT Press, 1998 MPI: The Complete Reference [GSNL98] W. Gropp, M. Snir, W. Nitzberg, and E. Lusk.
r , 35(4):921–940, Octobe Journal of the ACM . A new approach to the maximum-flow problem [GT88] A. V. Goldberg and R. E. Tarjan.
. 1988
. . Morgan Kaufmann, Los Altos, CA, 1987 Parallelism in Production Systems [Gup87] A. Gupta.
. , 31(5):532–533, 1988 Communications of the ACM . Reevaluating Amdahl's law [Gus88] J. L. Gustafson.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Proceedings of the 25th Hawaii International . In The consequences of fixed time performance measurement [Gus92] J. L. Gustafson.
. , 113–124, 1992 Conference on System Sciences: Volume III
. . McGraw-Hill, New York, NY, 1984 Computer Architecture and Parallel Processing [HB84] K. Hwang and F. A. Briggs.
. , 44:3–17, 1988 Information Science . Alpha-beta search on virtual tree machines [HB88] M. M. Huntbach and F. W. Burton.
Proceedings of the 1995 International Conference . In Deep Blue system overview [HCH95] F.-H. Hsu, M. S. Campbell, and A. J. Hoane.
. , 240–244, 1995 on Supercomputing, Barcelona, Spain
Communications . Computing connected components on parallel computers [HCS79] D. S. Hirschberg, A. K. Chandra, and D. V. Sarwate.
. , 22(8):461– 464, August 1979 of the ACM
. Technical A tight upper bound for the speedup of parallel best-first branch-and-bound algorithms [HD87] S.-R. Huang and L. S. Davis.
. report, Center for Automation Research, University of Maryland, College Park, MD, 1987
Proceedings of . In Parallel iterative a* search: An admissible distributed heuristic search algorithm [HD89a] S. R. Huang and L. S. Davis.
. , 23–29, 1989 the Eleventh International Joint Conference on Artificial Intelligence
. . McGraw-Hill, New York, NY, 1989 Parallel Processing for Supercomputers and Artificial Intelligence [HD89b] K. Hwang and D. DeGroot.
Installation and user guide for the oxford bsp toolset: User guide for the oxford bsp toolset [HDM97] J. Hill, S. Donaldson, and A. McEwan.
. . Technical report, Oxford University Computing Laboratory, 1997 (v1.3) implementation of bsplib
, . Technical Report ORNL-6150 Parallel Cholesky factorization in message-passing multiprocessor environments [Hea85] M. T. Heath.
Oak Ridge National Laboratory, Oak Ridge, TN, 1985.
. , 30(10):29–35, October 1997 IEEE Computer . Deep Blue's hardware-software synergy [HG97] S. Hamilton and L. Garber.
. . MIT Press, Cambridge, MA, 1985 The Connection Machine [Hil85] W. D. Hillis.
. , 18(4), 1990 Computer Architecture News What is scalability? [Hil90] M. D. Hill.
. Technical Report C3P 746, Concurrent Computation Matrix multiplication on the JPL/Caltech Mark IIIfp hypercube [Hip89] P. G. Hipes.
. Program, California Institute of Technology, Pasadena, CA, 1989
Proceedings of the 8th . In Parallel algorithms for the transitive closure and connected component problem [Hir76] D. S. Hirschberg.
. , 55–57, 1976 Annual ACM Symposium on the Theory of Computing
. , 21(8):657–666, August 1978 Communications of the ACM . Fast parallel sorting algorithms [Hir78] D. S. Hirschberg.
f . Technical Report YALEU/DCS/RR-508, Department o Spanning balanced trees in Boolean cubes [HJ87] C.-T. Ho and S. L. Johnsson.
Computer Science, Yale University, New Haven, CT, 1987.
. In Matrix multiplication on hypercubes using full bandwidth and constant storage [HJE91] C.-T. Ho, S. L. Johnsson, and A. Edelman.
. , 447–451, 1991 Proceedings of the 1991 International Conference on Parallel Processing
, Journal of Parallel and Distributed Computing . : A parallel model for coarse-grained machines 3 C [HK96] S. Hambrusch and A. Khokhar.
. 32(2):139–154, February 1996
. , 1(3–4):197–206, 1984 Parallel Computing . Experiences with the Denelcor HEP [HLM84] R. E. Hiromoto, O. M. Lubeck, and J. Moore.
. In A sub-linear parallel algorithm for some dynamic programming problems [HLV90] S. H. S. Huang, H. Liu, and V. Vishwanathan.
. , III–261–III–264, 1990 Proceedings of the 1990 International Conference on Parallel Processing
e . Osu-cisrc-tr-80-7, Computer Scienc Parallel record-sorting methods for hardware realization [HM80] D. K. Hsiao and M. J. Menon.
Information Department, Ohio State University, Columbus, OH, 1980.
Intl. J. of Par. . A study of the earth-manna multithreaded system 96] H. Hum, O. Maquelin, K. Theobald, X. Tian, and G. Gao. + [HMT
. , 24:319–347, 1996 Prog.
, IEEE Transactions on Computers . Parallel quicksort using fetch-and-add [HNR90] P. Heidelberger, A. Norton, and J. T. Robinson.
. C-39(1):133–138, January 1990
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. , 5:10–15, 1962 Computer Journal . Quicksort [Hoa62] C. A. R. Hoare.
Proceedings of the 1989 ACM . In Deterministic PRAM simulation with constant redundancy [HP89] S. W. Hornick and F. P. Preparata.
. , 103–109, 1989 Symposium on Parallel Algorithms and Architectures
. . MIT Press, Cambridge, MA, 1991 Data Parallel Programming [HQ91] P. J. Hatcher and M. J. Quinn.
SIAM Journal on . Parallel solution of triangular systems on distributed-memory multiprocessors [HR88] M. T. Heath and C. H. Romine.
. , 9(3):558–588, 1988 Scientific and Statistical Computing
Sixth Distributed Memory . In Efficient communication primitives on circuit-switched hypercubes [HR91] C.-T. Ho and M. T. Raghunath.
. , 390–397, 1991 Computing Conference Proceedings
. . Computer Science Press, Rockville, MD, 1978 Fundamentals of Computer Algorithms [HS78] E. Horowitz and S. Sahni.
. , 29(12):1170–1183, 1986 Communications of the ACM . Data parallel algorithms [HS86] W. D. Hillis and G. L. Steele.
. Large scale parallelization of alpha-beta search: An algorithmic and architectural study with computer chess [Hsu90] F.-H. Hsu.
. Technical report, Carnegie Mellon University, Pittsburgh, PA, 1990. Ph.D. Thesis
Proceedings of . In Solving some graph problems with optimal or near-optimal speedup on mesh-of-trees networks [Hua85] M. A. Huang.
. , 232–340, 1985 the 26th Annual IEEE Symposium on Foundations of Computer Science
. . McGraw-Hill, New York, NY, 1998 Scalable Parallel Computing [HX98] K. Hwang and Z. Xu.
. . Sams, 1999 Java Thread Programming [Hyd99] P. Hyde.
, IEEE Transactions on Computers . Parallel recognition and parsing on the hypercube [IPS91] O. H. Ibarra, T. C. Pong, and S. M. Sohn.
. 40(6):764–770, June 1991
A parallel searching scheme for multiprocessor systems and its application to [IYF79] M. Imai, Y. Yoshida, and T. Fukumura.
. , 416–418, 1979 Proceedings of the International Joint Conference on Artificial Intelligence . In combinatorial problems
. . Addison-Wesley, Reading, MA, 1992 An Introduction to Parallel Algorithms [Jaj92] J. Jaja.
Randomized parallel algorithms for Prolog programs and backtracking [JAM87] V. K. Janakiram, D. P. Agrawal, and R. Mehrotra.
. , 278–281, 1987 Proceedings of the 1987 International Conference on Parallel Processing . In applications
IEEE Transactions on . A randomized parallel backtracking algorithm [JAM88] V. K. Janakiram, D. P. Agrawal, and R. Mehrotra.
, C-37(12), 1988. Computers
, . MIT Press, Cambridge The Characteristics of Parallel Algorithms [JGD87] L. H. Jamieson, D. B. Gannon, and R. J. Douglass, editors.
. MA, 1987
SIAM Journal on Matrix . Matrix transposition on Boolean n-cube configured ensemble architectures [JH88] S. L. Johnsson and C.-T. Ho.
. , 9(3):419–454, July 1988 Analysis and Applications
IEEE Transactions on . Optimum broadcasting and personalized communication in hypercubes [JH89] S. L. Johnsson and C.-T. Ho.
. , 38(9):1249– 1268, September 1989 Computers
Sixth . In Optimal all-to-all personalized communication with minimum span on Boolean cubes [JH91] S. L. Johnsson and C.-T. Ho.
. , 299–304, 1991 Distributed Memory Computing Conference Proceedings
g . Technical report, Thinkin A radix-2 FFT on the connection machine [JKFM89] S. L. Johnsson, R. Krawitz, R. Frye, and D. McDonald.
Machines Corporation, Cambridge, MA, 1989.
. Technical report, School of Progress in integer programming: An exposition [JNS97] L. Johnson, G. Nemhauser, and M. Savelsbergh.
Industrial and Systems Engineering, Georgia Institute of Technology, 1997. Available from
. http://akula.isye.gatech.edu/mwps/mwps.html
. , 24(1):1–13, March 1977 Journal of the ACM . Efficient algorithms for shortest paths in sparse networks [Joh77] D. B. Johnson.
Proceedings of International Conference on . In -cube n Combining parallel and sequential sorting on a boolean [Joh84] S. L. Johnsson.
, 1984. Parallel Processing
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Journal of Parallel and . Communication efficient basic linear algebra computations on hypercube architectures [Joh87] S. L. Johnsson.
. , 4(2):133–172, April 1987 Distributed Computing
, VLSI and Parallel Computation . In R. Suaya and G. Birtwistle, editors, Communication in network architectures [Joh90] S. L. Johnsson.
. 223–389. Morgan Kaufmann, San Mateo, CA, 1990
. , 14:654–669, 1993 SIAM Journal on Scientific Computing . A parallel graph coloring heuristic [JP93] M. T. Jones and P. E. Plassmann.
Proceedings of the 1987 International . In All pairs shortest paths on a hypercube multiprocessor [JS87] J.-F. Jenq and S. Sahni.
. , 713–716, 1987 Conference on Parallel Processing
A . Technical Report RIACS TR 88.18, NAS Fast Fourier transform algorithm design and tradeoffs [KA88] R. A. Kamin and G. B. Adams.
Ames Research Center, Moffet Field, CA, 1988.
Journal of Parallel and . Benchmarking and comparison of the task graph scheduling algorithms [KA99a] Y.-K. Kwok and I. Ahmad.
. , 59:381–422, 1999 Distributed Computing
ACM Computing . Static scheduling algorithms for allocating directed task graphs to multiprocessors [KA99b] Y.-K. Kwok and I. Ahmad.
. , 31(4):406– 471, 1999 Surveys
, , 25:53–76 Econometrica . Assignment problems and the location of economic activities [KB57] T. C. Koopmans and M. J. Beckmann.
. 1957
. . Waltham, MA. 1990 KSR-1 Overview [Ken90] Kendall Square Research Corporation.
. , 33(5):539–543, 1990 Communications of the ACM . Measuring parallel processor performance [KF90] A. H. Karp and H. P. Flatt.
Journal of Parallel and Distributed . Analyzing scalability of parallel algorithms and architectures [KG94] V. Kumar and A. Gupta.
, , 22(3):379–391, 1994. Also available as Technical Report TR 91-18, Department of Computer Science Department Computing
. University of Minnesota, Minneapolis, MN
. Parallel Algorithms for Machine Intelligence and Vision [KGK90] V. Kumar, P. S. Gopalakrishnan, and L. N. Kanal, editors.
. Springer-Verlag, New York, NY, 1990
Journal of Parallel and . Scalable load balancing techniques for parallel computers [KGR94] V. Kumar, A. Grama, and V. N. Rao.
. , 22(1):60– 79, July 1994 Distributed Computing
. , 15:693–718, 1967 SIAM Journal of Applied Math . Finite state processes and dynamic programming [KH67] R. M. Karp and M. H. Held.
An efficient implementation of Batcher's odd-even merge algorithm and its application in parallel [KH83] M. Kumar and D. S. Hirschberg.
. , C–32, March 1983 IEEE Transactions on Computers . sorting schemes
, , 3(4):267–286 Computer Networks . Virtual cut-through: A new communication switching technique [KK79] P. Kermani and L. Kleinrock.
. 1979
A general branch-and-bound formulation for understanding and synthesizing and/or tree search [KK83] V. Kumar and L. N. Kanal.
. , 21:179–198, 1983 Artificial Intelligence . procedures
IEEE Transactions on Pattern Analysis . Parallel branch-and-bound formulations for and/or tree search [KK84] V. Kumar and L. N. Kanal.
. , PAMI–6:768–778, 1984 and Machine Intelligence
. . Springer-Verlag, New York, NY, 1988 Search in Artificial Intelligence [KK88a] L. N. Kanal and V. Kumar.
. The CDP: A unifying formulation for heuristic search, dynamic programming, and branch-and-bound [KK88b] V. Kumar and L. N. Kanal.
. , 1–27. Springer-Verlag, New York, NY, 1988 Search in Artificial Intelligence In L. N. Kanal and V. Kumar, editors,
Proceedings of 7th International . In Efficient Parallel Mappings of a Dynamic Programming Algorithm [KK93] G. Karypis and V. Kumar.
. , number 563–568, 1993 Parallel Processing Symposium
, Journal of Parallel and Distributed Computing . Unstructured tree search on simd parallel computers [KK94] G. Karypis and V. Kumar.
. 22(3):379–391, September 1994
. , 41(2):278–300, 1999 SIAM Review . -way partitioning for irregular graphs k Parallel multilevel [KK99] G. Karypis and V. Kumar.
, . North-Holland Parallel Processing for Artificial Intelligence [KKKS94] L. N. Kanal, V. Kumar, H. Kitano, and C. Suttner, editors.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. Amsterdam, The Netherlands, 1994
Sixth Distributed Memory . In Probabilistic analysis of the efficiency of the dynamic load distribution [KN91] K. Kimura and I. Nobuyuki.
, 1991. Computing Conference Proceedings
. . Addison-Wesley, Reading, MA, 1973 The Art of Computer Programming: Sorting and Searching [Knu73] D. E. Knuth.
Proceedings of the International Joint Conference on . In The use of parallelism to implement a heuristic search [Kor81] W. Kornfeld.
. , 575–580, 1981 Artificial Intelligence
. . Kluwer Academic Publishers, Boston, MA, 1988 Parallel Computation and Computers for Artificial Intelligence [Kow88] J. S. Kowalik.
. In Branch-and-bound and backtrack search on mesh-connected arrays of processors [KP92] C. Kaklamanis and G. Persiano.
. , 118–126, 1992 Proceedings of Fourth Annual Symposium on Parallel Algorithms and Architectures
Journal of Parallel and Distributed . Array processor with multiple broadcasting [KR87a] V. K. P. Kumar and C. S. Raghavendra.
. , 173–190, 1987 Computing
, International Journal of Parallel Programming . Parallel depth-first search, part II: Analysis [KR87b] V. Kumar and V. N. Rao.
. 16(6):501–519, 1987
, . Technical Report 408 A survey of complexity of algorithms for shared-memory machines [KR88] R. M. Karp and V. Ramachandran.
University of California, Berkeley, 1988.
Proceedings of the Fourth Conference on . In Load balancing on the hypercube architecture [KR89] V. Kumar and V. N. Rao.
. , 603–608, 1989 Hypercubes, Concurrent Computers, and Applications
Proceedings of . In Parallel best-first search of state-space graphs: A summary of results [KRR88] V. Kumar, K. Ramesh, and V. N. Rao.
. , 122–126, 1988 the 1988 National Conference on Artificial Intelligence
. . Technical Report RC13572, IBM T. J A complexity theory of efficient parallel algorithms [KRS88] C. P. Kruskal, L. Rudolph, and M. Snir.
Watson Research Center, Yorktown Heights, NY, 1988.
, Proceedings of the AMS . In On the shortest spanning subtree of a graph and the traveling salesman problem [Kru56] J. B. Kruskal.
. volume 7, 48–50, 1956
Proceedings of Supercomputing . In The horizon supercomputing system: architecture and software [KS88] J. Kuehn and B. Smith.
. , 28–34, 1988 Conference
Sixth . In Efficient parallel execution of IDA* on shared and distributed-memory multiprocessors [KS91a] L. V. Kale and V. Saletore.
, 1991. Distributed Memory Computing Conference Proceedings
Journal of Parallel and . Scalability of parallel algorithms for the all-pairs shortest path problem [KS91b] V. Kumar and V. Singh.
Proceedings of the International Conference on , 13(2):124–138, October 1991. A short version appears in the Distributed Computing
, 1990. Parallel Processing
. . SunSoft Press, Mountainview, CA, 1995 Programming with Threads [KSS95] S. Kleiman, D. Shah, and B. Smaalders.
Proceedings of 18th ACM . In y Parallel hashing – an efficient implementation of shared memor [KU86] A. R. Karlin and E. Upfal.
. , 160–168, 1986 Conference on Theory of Computing
n , 73–74. Academic Press, Sa Advances in Computing . In M. Yovits, editor, The structure of parallel algorithms [Kun80] J. T. Kung.
. Diego, CA, 1980
Proceedings of the 1986 IEEE Symposium on . In Memory requirements for balanced computer architectures [Kun86] H. T. Kung.
. , 49–54, 1986 Computer Architecture
, . Technical Report UCF-CS-92-28 Comparison of meshes vs. hypercubes for data rearrangement [KV92] K. Kreeger and N. R. Vempaty.
Department of Computer Science, University of Central Florida, Orlando, FL, 1992.
Proceedings of the ACM Annual Symposium on . In A randomized parallel branch-and-bound procedure [KZ88] R. M. Karp and Y. Zhang.
. , 290–300, 1988 Theory of Computing
. , C-24(1):1145–1155, 1975 IEEE Transactions on Computers . Access and alignment of data in an array processor [Law75] D. H. Lawrie.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
, . Prentice Hall PTR/Sun Microsystems Press Threads Primer: A Guide to Multithreaded Programming [LB95a] B. Lewis and D. J. Berg.
1995.
Research report RC 20238 . Limits on the performance benefits of multithreading and prefetching [LB95b] B.-H. Lim and R. Bianchini.
. , IBM T. J. Watson Research Center, Yorktown Heights, NY, October 1995 (89547)
. . Prentice Hall PTR/Sun Microsystems Press, 1997 Multithreaded Programming with Pthreads [LB97] B. Lewis and D. J. Berg.
. . Sun Microsystems Press / Prentice Hall, 1998 Multithreaded Programming with PThreads [LB98] T. G. Lewis and D. Berg.
SIAM Journal on Scientific and Statistical . A parallel triangular solver for a hypercube multiprocessor [LC88] G.-J. Li and T. Coleman.
. , 9:485–502, 1988 Computing
. A new method for solving triangular systems on distributed memory message passing multiprocessors [LC89] G.-J. Li and T. Coleman.
. , 10:382–396, 1989 SIAM Journal on Scientific and Statistical Computing
, . McGraw-Hill Analysis and Design of Parallel Algorithms: Arithmetic and Matrix Problems [LD90] S. Lakshmivarahan and S. K. Dhall.
. New York, NY, 1990
Concurrency: Practice and . Multiprogramming a distributed-memory multiprocessor [LDP89] M. R. Leuze, L. W. Dowdy, and K. H. Park.
. , 1(1):19–33, September 1989 Experience
. . Addison-Wesley, 1999 Concurrent Programming in Java, Second Edition: Design Principles and Patterns [Lea99] D. Lea.
Proceedings of International Workshop on Graph-Theoretic Concepts . In Parallel computation using mesh of trees [Lei83] F. T. Leighton.
, 1983. in Computer Science
l , C–34(4):344–354, Apri IEEE Transactions on Computers . Tight bounds on the complexity of parallel sorting [Lei85a] F. T. Leighton.
. 1985
Proceedings of the 1985 International . In Fat-trees: Universal networks for hardware efficient supercomputing [Lei85b] C. E. Leiserson.
. , 393–402, 1985 Conference on Parallel Processing
. . Morgan Kaufmann, San Mateo, CA, 1992 Introduction to Parallel Algorithms and Architectures [Lei92] F. T. Leighton.
. . Prentice-Hall, Englewood Cliffs, NJ, 1992 Introduction to Parallel Computing [LER92] T. G. Lewis and H. El-Rewini.
. . Prentice-Hall, Englewood Cliffs, NJ, 1993 The Art of Parallel Programming [Les93] B. P. Lester.
. In L. H. Jamieson, D. B. Gannon, Measuring communications structures in parallel architectures and algorithms [Lev87] S. P. Levitan.
. . MIT Press, Cambridge, MA, 1987 The Characteristics of Parallel Algorithms and R. J. Douglass, editors,
,  O'Reilly & Associates Posix Programmer's Guide: Writing Portable Unix Programs with the Posix. 1 Standard. [Lew91] D. A. Lewine.
1991.
SC '98, High Performance Networking and . In OpenMP on networks of workstations [LHZ98] H. Lu, C. Hu, and W. Zwaenepoel.
, Orlando, Florida, 1998. Computing Conference
. . IEEE Computer Society Press, Los Alamitos, CA, 1992 Architectural Alternatives for Exploiting Parallelism [Lil92] D. J. Lilja.
e . Technical Report 83-101, Computer Scienc The key node method: A highly parallel alpha-beta algorithm [Lin83] G. Lindstrom.
Department, University of Utah, Salt Lake City, UT, 1983.
, International Journal of Parallel Programming . A distributed fair polling scheme applied to or-parallel logic programming [Lin92] Z. Lin.
20(4), August 1992.
, , 15(9):789–801 Communications of the ACM . Cellular arrays for the solution of graph problems [LK72] K. N. Levitt and W. T. Kautz.
. September 1972
Proceedings of the . In A hybrid SSS*/alpha-beta algorithm for parallel search of game trees [LK85] D. B. Leifker and L. N. Kanal.
. , 1044–1046, 1985 International Joint Conference on Artificial Intelligence
The Stanford dash 92] D. Lenoski, J. Laudon, K. Gharachorloo, W. D. Weber, A. Gupta, J. L. Hennessy, M. Horowitz, and M. Lam. + [LLG
. , 63–79, March 1992 IEEE Computer . multiprocessor
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. In Computational experience of an interior-point algorithm in a parallel branch-and-cut framework [LM97] E. K. Lee and J. E. Mitchell.
, 1997. Proceedings for SIAM Conference on Parallel Processing for Scientific Computing
29th Annual Symposium on Foundations of . In Universal packet routing algorithms [LMR88] F. T. Leighton, B. Maggs, and S. K. Rao.
. , 256–271, 1988 Computer Science
. . SIAM, Philadelphia, PA, 1992 Computational Frameworks for the Fast Fourier Transform [Loa92] C. V. Loan.
Advances in . In P. M. Pardalos, editor, Parallel algorithms for the quadratic assignment problem [LP92] Y. Li and P. M. Pardalos.
. , 177–189. North-Holland, Amsterdam, The Netherlands, 1992 Optimization and Parallel Computing
Information . A probabilistic simulation of PRAMs on a bounded degree network [LPP88] F. Luccio, A. Pietracaprina, and G. Pucci.
. , 28:141–147, July 1988 Processing Letters
SIAM Journal of . A new scheme for deterministic simulation of PRAMs in VLSI [LPP89] F. Luccio, A. Pietracaprina, and G. Pucci.
, 1989. Computing
Proceedings of the Fifth ACM . In Cilk: An efficient multithreaded runtime system [LRZ95] C. Leiserson, K. Randall, and Y. Zhou.
. , Santa Barbara, CA, 1995 SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)
. , 594–602, 1984 Communications of the ACM . Anomalies in parallel branch and bound algorithms [LS84] T. H. Lai and S. Sahni.
, C-34(10), IEEE Transactions on Computers . Performance of parallel branch-and-bound algorithms [LS85] T. H. Lai and A. Sprague.
October 1985.
. A note on anomalies in parallel branch-and-bound algorithms with one-to-one bounding functions [LS86] T. H. Lai and A. Sprague.
. , 23:119–122, October 1986 Information Processing Letters
Journal of Parallel and Distributed . A hypercube algorithm for the 0/1 knapsack problem [LSS88] J. Lee, E. Shragowitz, and S. Sahni.
. , (5):438–456, 1988 Computing
, , 15(4):1036–1053 SIAM Journal on Computing . A simple parallel algorithm for the maximal independent set problem [Lub86] M. Luby.
. 1986
. , 14, 1966 Operations Research . Branch-and-bound methods: A survey [LW66] E. L. Lawler and D. Woods.
Proceedings of the . In Computational efficiency of parallel approximate branch-and-bound algorithms [LW84] G.-J. Li and B. W. Wah.
. , 473–480, 1984 1984 International Conference on Parallel Processing
, , 81–89 Proceedings of COMPSAC 85 . In Parallel processing of serial dynamic programming problems [LW85] G.-J. Li and B. W. Wah.
. 1985
, C-35, IEEE Transactions on Computers . Coping with anomalies in parallel branch-and-bound algorithms [LW86] G.-J. Li and B. W. Wah.
June 1986.
. . Morgan Kaufmann, San Mateo, CA, 1995 Scalable Shared-Memory Multiprocessing [LW95] D. Lenoski and W. D. Weber.
Proceedings of 18th ACM Conference on Theory of . In New lower bounds for parallel computations [LY86] M. Li and Y. Yesha.
. , 177–187, 1986 Computing
. , 14:533–551, 1982 Computing Surveys . Parallel search of strongly ordered game trees [MC82] T. A. Marsland and M. S. Campbell.
. , 1992 Artificial Intelligence . SIMD parallel heuristic search [MD92] A. Mahanti and C. Daniels.
Proceedings of the Fifth IEEE . In Scalable duplicate pruning strategies for parallel A* graph search [MD93] N. R. Mahapatra and S. Dutt.
, 1993. Symposium on Parallel and Distributed Processing
SIAM Journal on Scientific and Statistical . Hypercube algorithms and implementations [MdV87] O. A. McBryan and E. F. V. de Velde.
. , 8(2):s227–s287, March 1987 Computing
y . Ma http://www.mpi-forum.org . Available at MPI: A Message-Passing Interface Standard [Mes94] Message Passing Interface Forum.
1994.
. http://www.mpi-forum.org . Available at MPI-2: Extensions to the Message-Passing Interface [Mes97] Message Passing Interface Forum.
July 1997.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. In V. Parallel game tree search by dynamic tree decomposition [MFMV90] B. Monien, R. Feldmann, P. Mysliwietz, and O. Vornberger.
w . Springer-Verlag, Ne Parallel Algorithms for Machine Intelligence and Vision Kumar, P. S. Gopalakrishnan, and L. N. Kanal, editors,
. York, NY, 1990
. , 30:97–102, 1989 Information Processing Letters . A fast parallel quicksort algorithm [MG89] C. U. Martel and D. Q. Gusfield.
Proceedings of the Workshop on Parallel Computing of . In Exact distributed algorithms for travelling salesman problem [Mil91] D. Miller.
, 1991. Discrete Optimization Problems
. . John Wiley & Sons, 1999 Concurrency: State Models and Java Programs [MK99] J. Magee and J. Kramer.
Proceedings of MIT Conference . In Meshes with reconfigurable buses [MKRS88] R. Miller, V. K. P. Kumar, D. I. Reisis, and Q. F. Stout.
. , 163–178, 1988 on Advanced Research in VLSI
Proceedings of the . In From dynamic programming to search algorithms with functional costs [MM73] A. Martelli and U. Montanari.
. , 345–349, 1973 International Joint Conference on Artifi cial Intelligence
. . Wiley, Chichester, UK, 1991 Practical Parallel Computing: Status and Prospects [MM91] P. Messina and A. Murli, editors.
. A parallel depth first search branch and bound for the quadratic assignment problem [MMR95] B. Mans, T. Mautor, and C. Roucairol.
. ational Research, 81(3):617–628, 1995 European Journal of Oper
. . Oxford University Press, Oxford, UK, 1988 Parallel Algorithms and Matrix Computation [Mod88] J. J. Modi.
Proceedings of the 1983 . In Experience with two parallel programs solving the traveling salesman problem [Moh83] J. Mohan.
. , 191–193, 1983 International Conference on Parallel Processing
, Hypercube Multiprocessors 1986 . In M. T. Heath, editor, Matrix computation on distributed-memory multiprocessors [Mol86] C. B. Moler.
. 181–195. SIAM, Philadelphia, PA, 1986
. . Technical Report TN-02-0587-0288, Intel Scientific Computers, 1987 Another look at Amdahl's law [Mol87] C. B. Moler.
. . Morgan Kaufmann, San Mateo, CA, 1993 Parallel Processing: From Applications to Systems [Mol93] D. I. Moldovan.
, IEEE Transactions on Pattern Analysis and Machine Intelligence . Parallel game tree search [MP85] T. A. Marsland and F. Popowich.
. PAMI-7(4):442–452, July 1985
ORSA Journal . The role of performance metrics for parallel mathematical programming algorithms [MP93] D. L. Miller and J. F. Pekny.
, 5(1), 1993. on Computing
CAPO Report . Technical Report CSD-TR-1011, On high level characterization of parallelism [MR] D. C. Marinescu and J. R. Rice.
Journal of Parallel and Distributed , Computer Science Department, Purdue University, West Lafayette, IN. Also published in CER-90-32
, 1993. Computing
Advanced Topics in Computer , 111–150. Parallel Branch-and-Bound [MRSR92] G. P. McKeown, V. J. Rayward-Smith, and S. A. Rush.
. . Blackwell Scientific Publications, Oxford, UK, 1992 Science
Proceedings of the 1988 International Conference . In Downward scalability of parallel architectures [MS88] Y. W. E. Ma and D. G. Shea.
. , 109–120, 1988 on Supercomputing
Proceedings of . In Probabilistic performance analysis of heuristic search using parallel hash tables [MS90] G. Manzini and M. Somalvico.
, 1990. the International Symposium on Artificial Intelligence and Mathematics
. . MIT Press, Cambridge, MA, 1996 Parallel Algorithms for Regular Architectures [MS96] R. Miller and Q. F. Stout.
Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity [MV84] K. Mehlhorn and U. Vishkin.
. , 21(4):339–374, November 1984 Acta Informatica . of parallel memories
Computers . Technical report, University of Paderborn, Germany, 1985. Also in The ring machine [MV85] B. Monien and O. Vornberger.
, 3(1987). and Artificial Intelligence
Proceedings of International Workshop on . In Parallel processing of combinatorial search trees [MV87] B. Monien and O. Vornberger.
, 1987. Parallel Algorithms and Architectures
y . Technical Report 30, Universit Superlinear speedup for parallel backtracking [MVS86] B. Monien, O. Vornberger, and E. Spekenmeyer.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
of Paderborn, Germany, 1986.
. , 34(3):57–61, 1991 Communications of the ACM . Scalability of parallel machines [NA91] D. Nussbaum and A. Agarwal.
5th International . In ) time crew pram merge sort algorithm n  (log O Investigating the practical value of Cole's [Nat90] L. Natvig.
. , October 1990 Symposium on Computing and Information Sciences
. . O'Reilly & Associates, Newton, MA 02164, 1996 Pthreads Programming [NBF96] B. Nichols, B. Buttlar, and J. P. Farrell.
. . Beaverton, OR, 1990 nCUBE 6400 Processor Manual [nCU90] nCUBE Corporation.
. . Hewlett-Packard professional books Thread time: the multithreaded programming guide [ND96] S. J. Norton and M. D. DiPasquale.
. Prentice-Hall, Englewood Cliffs, NJ 07632, 1996
Proceedings of 1991 International Conference for Young Computer . In A layered classification of parallel computers [Ni91] L. M. Ni.
. , 28–33, 1991 Scientists
f IEEE Digest o . In The design of the MasPar MP-1: A cost-effective massively parallel computer [Nic90] J. R. Nickolls.
. , 25–28. IEEE Computer Society Press, Los Alamitos, CA, 1990 m Papers—Comco
, 26(2), February IEEE Computer . A survey of wormhole routing techniques in direct connect networks [NM93] L. M. Ni and McKinley.
1993.
IEEE [NMB83] D. Nath, S. N. Maheshwari, and P. C. P. Bhatt. Efficient VLSI networks for parallel processing based on orthogonal trees .
. , C–32:21–23, June 1983 Transactions on Computers
, , C–28(1) IEEE Transactions on Computers . Bitonic sort on a mesh connected parallel computer [NS79] D. Nassimi and S. Sahni.
. January 1979
SIAM Journal of . Finding connected components and connected ones on a mesh-connected computer [NS80] D. Nassimi and S. Sahni.
. , 9(4):744–757, November 1980 Computing
Parallelization and performance analysis of the Cooley-Tukey FFT algorithm for shared memory [NS87] A. Norton and A. J. Silberger.
. , C-36(5):581–591, 1987 IEEE Transactions on Computers . architectures
7th International Parallel Processing . In  reconfigurable meshes with buses n  x n  numbers on n Sorting [NS93] M. Nigam and S. Sahni.
. , 174–181, 1993 Symposium
. A Report by the Committee on Physical, Mathematical Grand Challenges: High Performance Computing and Communications [NSF91]
. and Engineering Sciences, NSF/CISE, 1800 G Street NW, Washington, DC, 20550, 1991
Proceedings of the Third Conference on Hypercubes, . In The iPSC/2 direct-connect communications technology [Nug88] S. F. Nugent.
. , 51–60, 1988 Concurrent Computers, and Applications
. . Springer-Verlag, New York, NY, 1982 Fast Fourier Transform and Convolution Algorithms [Nus82] H. J. Nussbaumer.
Journal of Parallel and Distributed . Problem size, parallel architecture, and optimal speedup [NW88] D. M. Nicol and F. H. Willard.
. , 5:404–420, 1988 Computing
. Committee on Innovations in Computing and Funding a Revolution: Government Support for Computing Research [GOV99]
. Communications. National Academy Press, 1999
. , 7:149–162, 1988 Parallel Computing .  forms of factorization methods II: Parallel systems ijk The [OR88] J. M. Ortega and C. H. Romine.
. . Plenum Press, New York, NY, 1988 Introduction to Parallel and Vector Solution of Linear Systems [Ort88] J. M. Ortega.
, Communications of the ACM . Data-flow algorithms for parallel matrix computations [OS85] D. P. O'Leary and G. W. Stewart.
. 28:840–853, 1985
, Linear Algebra and its Applications . Assignment and scheduling in parallel matrix factorization [OS86] D. P. O'Leary and G. W. Stewart.
. 77:275–299, 1986
. . Morgan Kaufmann, 1998 Parallel Programming with MPI [Pac98] P. Pacheco.
. 85] G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norlton, and J + [PBG
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Proceedings of 1985 International . In The IBM research parallel processor prototype (RP3): Introduction and architecture Weiss.
. , 764– 771, 1985 Conference on Parallel Processing
, Supercomputing '89 Proceedings . In A parallel algorithm for the quadratic assignment problem [PC89] P. M. Pardalos and J. Crouse.
. 351–360. ACM Press, New York, NY, 1989
, International Journal of Parallel Processing . Dynamic partitioning of multiprocessor systems [PD89] K. H. Park and L. W. Dowdy.
. 18(2):91–120, 1989
. . Addison-Wesley, Reading, MA, 1984 g Heuristics—Intelligent Search Strategies for Computer Problem Solvin [Pea84] J. Pearl.
. . Addison-Wesley, Reading, MA, 1987 Parallel Programming [Per87] R. Perrott.
. . Prentice Hall, Englewood Cliffs, NJ, 1998. 2nd Edition In Search of Clusters [Pfi98] G. F. Pfister.
. . In V. Kumar, P. S. Gopalakrishnan, and L. N Parallel heuristic search: Two approaches [PFK90] C. Powley, C. Ferguson, and R. Korf.
. . Springer-Verlag, New York, NY, 1990 Parallel Algorithms for Machine Intelligence and Vision Kanal, editors,
. . Prentice Hall, 1998 Multithreaded Programming with Win32 [PG98] T. Q. Pham and P. K. Garg.
. . Morgan Kaufmann, San Mateo, CA, 1990 Computer Architecture: A Quantitative Approach [PH90] D. A. Patterson and J. L. Hennessy.
n . Morgan Kaufmann, Sa Computer Architecture: A Quantitative Approach, 2nd edition [PH96] D. A. Patterson and J. L. Hennessy.
. Mateo, CA, 1996
Proceedings of 1989 International Conference on . In Parallel algorithms for shortest path problems [PK89] R. C. Paige and C. P. Kruskal.
. , 14– 19, 1989 Parallel Processing
IEEE . k An inherently parallel method for heuristic problem-solving: Part I – general framewor [PK95] I. Pramanick and J. G. Kuhl.
, 6(10), October 1995. Transactions on Parallel and Distributed Systems
. , 1992 Artificial Intelligence . IDA* on the connection machine [PKF92] C. Powley, R. Korf, and C. Ferguson.
Proceedings of the 1989 ACM Symposium on Parallel . In Load balancing, selection and sorting on the hypercube [Pla89] C. C. Plaxton.
. , 64–73, 1989 Algorithms and Architectures
Annals of . Lower bounds for the quadratic assignment problem [PLRR94] P. M. Pardalos, Y. Li, K. Ramakrishna, and M. Resende.
. , 50:387–411, 1994. Special Volume on Applications of Combinatorial Optimization Operations Research
, Annual ACM Symposium on Theory of Computing . In 17th Efficient parallel solution of linear systems [PR85] V. Pan and J. H. Reif.
. 143–152, 1985
. In Parallel branch-and-bound algorithms for unconstrainted quadratic zero-one programming [PR89] P. M. Pardalos and G. P. Rodgers.
e , 131–143. North-Holland, Amsterdam, Th Impacts of Recent Computer Advances on Operations Research R. Sharda et al., editors,
. Netherlands, 1989
Parallel branch-and-bound algorithms for quadratic zero-one programming on a hypercube [PR90] P. M. Pardalos and G. P. Rodgers.
. , 22:271–292, 1990 Annals of Operations Research . architecture
. [PR91] M. Padberg and G. Rinaldi. A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems
. , 33:60– 100, 1991 SIAM Review
. , 36:1389–1401, 1957 Bell Systems Technical Journal . Shortest connection network and some generalizations [Pri57] R. C. Prim.
. In Algorithm PR2 for the parallel size reduction of the 0/1 multiknapsack problem [PRV88] G. Plateau, C. Roucairol, and I. Valabregue.
. , number 811, 1988 INRIA Rapports de Recherche
, . Prentice-Hall, Englewood Cliffs, NJ Combinatorial Optimization: Algorithms and Complexity [PS82] C. H. Papadimitriou and K. Steiglitz.
1982.
Proceedings of the 14th Princeton . In Area-time optimal VLSI networks for matrix multiplication [PV80] F. P. Preparata and J. Vuillemin.
. , 300–309, 1980 Conference on Information Science and Systems
Proceedings of . In Towards an architecture independent analysis of parallel algorithms [PY88] C. H. Papadimitriou and M. Yannakakis.
. , 510–513, 1988 20th ACM Symposium on Theory of Computing
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. , 26(1), March 1986 BIT . An upper bound for the speedup of parallel branch-and-bound algorithms [QD86] M. J. Quinn and N. Deo.
. , 6:349–357, 1988 Parallel Computing . Parallel sorting algorithms for tightly coupled multiprocessors [Qui88] M. J. Quinn.
IEEE Transactions on . Analysis and implementation of branch-and-bound algorithms on a hypercube multicomputer [Qui89] M. J. Quinn.
, 1989. Computers
. . McGraw-Hill, New York, NY, 1994 Parallel Computing: Theory and Practice [Qui94] M. J. Quinn.
Foundations of Software . In Qsm: A general purpose shared-memory model for parallel computation [Ram97] V. Ramachandran.
. , 1–5, 1997 Technology and Theoretical Computer Science
, . Ph.D. Thesis, Department of Computer Science, Yale University, New Haven, CT Fluent Parallel Computation [Ran89] A. G. Ranade.
1989.
Proceedings of the Third ACM Symposium on . In Optimal speedup for backtrack search on a butterfly network [Ran91] A. G. Ranade.
, 1991. Parallel Algorithms and Architectures
.  Ph.D. Thesis, University of Texas, Austin, TX, 1990 Parallel Processing of Heuristic Search. [Rao90] V. N. Rao.
, . Ph.D. Thesis, Carnegie-Mellon University, Pittsburgh, PA Performance Evaluation of Multiple Processor Systems [Ras78] L. Raskin.
1978.
IEEE Transactions on Acoustics, Speech and Signal . A new principle for Fast fourier transform [RB76] C. M. Rader and N. M. Brenner.
. , 24:264–265, 1976 Processing
, . Technical Report E3646 Analytical and heuristic modeling of distributed algorithms [RDK89] C. Renolet, M. Diamond, and J. Kimbel.
FMC Corporation, Advanced Systems Center, Minneapolis, MN, 1989.
. , 396–409, 1981 SIAM Journal of Computing . Probabilistic algorithms for sorting and selection [Rei81] R. Reischuk.
. . MIT Press, Cambridge, MA, 1989 Multicomputer Networks: Message-Based Parallel Processing [RF89] D. A. Reed and R. M. Fujimoto.
An efficient termination detection and abortion algorithm for [RICN88] K. Rokusawa, N. Ichiyoshi, T. Chikayama, and H. Nakashima.
. , 18–22, 1988 Proceedings of 1988 International Conference on Parallel Processing: Vol. I . In distributed processing systems
, International Journal of Parallel Programming . Parallel depth-first search, part I: Implementation [RK87] V. N. Rao and V. Kumar.
. 16(6):479–499, 1987
. , C–37 (12), 1988 IEEE Transactions on Computers . Concurrent access of priority queues [RK88a] V. N. Rao and V. Kumar.
Proceedings of the 1988 Foundation of Software . In Superlinear speedup in state-space search [RK88b] V. N. Rao and V. Kumar.
, , number 338, 161–174. Springer-Verlag Series Lecture Notes in Computer Science Technology and Theoretical Computer Science
. 1988
, IEEE Transactions on Parallel and Distributed Systems . On the efficicency of parallel backtracking [RK93] V. N. Rao and V. Kumar.
. , Computer Science Department, University of Minnesota technical report TR 90-55 4(4):427–437, April 1993. available as a
Proceedings of the National . In A parallel implementation of iterative-deepening-A* [RKR87] V. N. Rao, V. Kumar, and K. Ramesh.
. , 878–882, 1987 Conference on Artificial Intelligence (AAAI-87)
, . Prentice-Hall, Englewood Cliffs, NJ Combinatorial Algorithms: Theory and Practice [RND77] E. M. Reingold, J. Nievergelt, and N. Deo.
1977.
. , 6:109–114, 1988 Parallel Computing . Parallel solution of triangular systems of equations [RO88] C. H. Romine and J. M. Ortega.
. , 21–39 Algorithms and Complexity: New Directions and Recent Results . In J. Traub, editor, Probabilistic algorithms [Rob75] M. O. Robin.
. Academic Press, San Diego, CA, 1975
. . John Wiley and Sons, New York, NY, 1990 The Impact of Vector and Parallel Architectures on Gaussian Elimination [Rob90] Y. Robert.
Hypercube Multiprocessors . In M. T. Heath, editor, The parallel solution of triangular systems on a hypercube [Rom87] C. H. Romine.
. , 552–559. SIAM, Philadelphia, PA, 1987 1987
, Discrete Applied Mathematics . A parallel branch-and-bound algorithm for the quadratic assignment problem [Rou87] C. Roucairol.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. 18:211–225, 1987
Proceedings of the Workshop On Parallel . In Parallel branch-and-bound on shared-memory multiprocessors [Rou91] C. Roucairol.
, 1991. Computing of Discrete Optimization Problems
Practical UNIX Programming: A Guide to Concurrency, [RRRR96] K. A. Robbins, S. Robbins, K. R. Robbins, and S. Robbins.
. . Prentice Hall, 1996 Communication, and Multithreading
International Conference . In The tera computer system [RS90a] A. P. R. Alverson, D. Callahan, D. Cummings, B. Koblenz and B. Smith.
. , 1–6, 1990 on Supercomputing
,  Pattern Recognition. Springer-Verlag, New York, NY Hypercube Algorithms for Image Processing and [RS90b] S. Ranka and S. Sahni.
1990.
. , 34(1):60–76, January 1987 Journal of the ACM . A logarithmic time sort for linear size networks [RV87] J. H. Reif and L. G. Valiant.
. , 59:297–307, 1988 Theoretical Computer Science . Efficient parallel computations for dynamic programming [Ryt88] W. Rytter.
. . Wiley, Chichester, UK, 1989 Parallel Processing and Artificial Intelligence [RZ89] M. Reeve and S. E. Zenith, editors.
Linear Algebra and its . Communication complexity of the Gaussian elimination algorithm on multiprocessors [Saa86] Y. Saad.
. , 77:315–340, 1986 Applications
Proceedings of Fourth . In A large scale, homogeneous, fully distributed parallel machine [SB77] H. Sullivan and T. R. Bashkow.
. , 105–124, March 1977 Symposium on Computer Architecture
. Report Analysis of multithreaded architectures for parallel computing [SBCV90] R. H. Saavedra-Barrera, D. E. Culler, and T. Von Eiken.
. UCB/CSD 90/569, University of California, Berkeley, Computer Science Division, Berkeley, CA, April 1990
. , 2:484–521, October 1980 ACM Transactions on Programming Languages and Systems . Ultracomputers [Sch80] J. T. Schwartz.
. , 21(10):847–857, 1978 Communications of the ACM . Implementing quicksort programs [Sed78] R. Sedgewick.
. , 28(1):22–33, 1985 Communications of the ACM . The cosmic cube [Sei85] C. L. Seitz.
Proceedings of the . In Circuit-switched vs. store-and-forward solutions to symmetric communication problems [Sei89] S. R. Seidel.
. , 253–255, 1989 Fourth Conference on Hypercubes, Concurrent Computers, and Applications
, . Technical report, California Institute of Technology, Pasadena Mosaic C: An experimental fine-grain multicomputer [Sei92] C. L. Seitz.
CA, 1992.
Proceedings of the Third Conference on . In -port communication d Binsorting on hypercube with [SG88] S. R. Seidel and W. L. George.
. , 1455–1461, January 1988 Hypercube Concurrent Computers
. , 17:1093–1109, December 1991 Parallel Computing . Toward a better parallel performance metric [SG91] X.-H. Sun and J. L. Gustafson.
. Also available as Technical Report IS-5053, UC-32, Ames Laboratory, Iowa State University, Ames, IA
. . Ellis Horwood, Chichester, UK, 1985 Data-Flow Computing [Sha85] J. A. Sharp.
. , 2(7):30–32, July 1959 Communications of the ACM . A high-speed sorting procedure [She59] D. L. Shell.
IEEE . Scaling parallel programs for multiprocessors: Methodology and examples [SHG93] J. P. Singh, J. L. Hennessy, and A. Gupta.
. , 26(7):42–50, 1993 Computer
Proceedings of the 4th Annual . In The universality of various types of SIMD machine interconnection networks [Sie77] H. J. Siegel.
.  , 23–25, 1977 Symposium on Computer Architecture
. . D. C. Heath, Lexington, MA, 1985 Interconnection Networks for Large-Scale Parallel Processing [Sie85] H. J. Siegel.
, , 10(4):682–690 SIAM Journal of Computing . Fast, efficient parallel algorithms for some graph problems [SJ81] C. Savage and J. Jaja.
. November 1981
Proceedings of Supercomputing . In A dynamic scheduling strategy for the chare-kernel system [SK89] W. Shu and L. V. Kale.
. , 389–398, 1989 Conference
Proceedings of the 1990 . In Consistent linear speedup to a first solution in parallel state-space search [SK90] V. Saletore and L. V. Kale.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. , 227–233, August 1990 National Conference on Artificial Intelligence
International . Efficient algorithms for parallel sorting on mesh multicomputers [SKAT91a] V. Singh, V. Kumar, G. Agha, and C. Tomlinson.
. , 20(2):95–131, 1991 Journal of Parallel Programming
International Journal . Scalability of parallel sorting on mesh multicomputers [SKAT91b] V. Singh, V. Kumar, G. Agha, and C. Tomlinson.
, 20(2), 1991. of Parallel Programming
, Journal of Parallel and Distributed Computing . The DADO production system machine [SM86] S. J. Stolfo and D. P. Miranker.
. 3:269–296, June 1986
, 31(1), 1984. Journal of the ACM . Random trees and the analysis of branch and bound procedures [Smi84] D. R. Smith.
. , 324–333, 1990 Supercomputing '90 Proceedings . In Another view of parallel speedup [SN90] X.-H. Sun and L. M. Ni.
, Journal of Parallel and Distributed Computing . Scalable problems and memory-bounded speedup [SN93] X.-H. Sun and L. M. Ni.
. 19:27–37, September 1993
. , 242–253, 1982 Proceedings of Principles of Distributed Computing . In On parallel search [Sni82] M. Snir.
. , 14(3):688–708, August 1985 SIAM Journal of Computing . On parallel searching [Sni85] M. Snir.
, Annual Review of Computer Science . Type architectures, shared-memory and the corollary of modest potential [Sny86] L. Snyder.
. 1:289–317, 1986
, . MIT Press MPI: The Complete Reference 96] M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. Dongarra. + [SOHL
. Cambridge, MA, 1996
Introduction to The Design and Analysis of . In S. Goodman and S. Hedetniemi, editors, An algorithm attributed to Sollin [Sol77] M. Sollin.
. . McGraw-Hill, Cambridge, MA, 1977 Algorithms
, . Technical Report IS-5057, Ames Laboratory Scalability of parallel algorithm-machine combinations [SR91] X.-H. Sun and D. T. Rover.
. IEEE Transactions on Parallel and Distributed Systems Iowa State University, Ames, IA, 1991. Also published in
. , 37:867–872, 1988 IEEE Transactions on Computers . Topological properties of hypercubes [SS88] Y. Saad and M. H. Schultz.
, , 6:115–135 Journal of Parallel and Distributed Computing . Data communication in hypercubes [SS89a] Y. Saad and M. H. Schultz.
, 1989. Also available as Technical Report YALEU/DCS/RR-428 from the Department of Computer Science, Yale University, New Haven
. CT
. , 11:131–150, 1989 Parallel Computing . Data communication in parallel architectures [SS89b] Y. Saad and M. H. Schultz.
. , 14:361–372, 1990 Journal of Parallel and Distributed Computing . Parallel sorting by regular sampling [SS90] H. Shi and J. Schaeffer.
Solving the traveling salesman problem with a distributed branch-and-bound algorithm on a 1024 [ST95] B. M. S. Tschvke, R. Lling.
. , 182– 189, Santa Barbara, CA, April 1995 Proceedings of the 9th International Parallel Processing Symposium . In processor network
. , C-20(2):153–161, 1971 IEEE Transactions on Computers . Parallel processing with the perfect shuffle [Sto71] H. S. Stone.
. . Addison-Wesley, Reading, MA, 1993 High-Performance Computer Architecture: Third Edition [Sto93] H. S. Stone.
. . SunSoft Press, Mountainview, CA, 1995 Solaris multithreaded programming guide [Sun95] SunSoft.
. . Beaverton, OR, 1991 Paragon XP/S Product Overview [Sup91] Supercomputer Systems Division, Intel Corporation.
, Journal of Algorithms . Finding the maximum, merging and sorting in a parallel computation model [SV81] Y. Shiloach and U. Vishkin.
. 88–102, 1981
. , 3:128–146, 1982 Journal of Algorithms . ) parallel max-flow algorithm n  log 2 n  ( O An [SV82] Y. Shiloach and U. Vishkin.
Hypercube Multiprocessors . In M. T. Heath, editor, Passing messages in link-bound hypercubes [SW87] Q. F. Stout and B. A. Wagar.
. , 251–257. SIAM, Philadelphia, PA, 1987 1987
. , 5:197–210, 1987 Parallel Computing . Multiprocessor FFTs [Swa87] P. N. Swarztrauber.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
IEEE Trans. on Parallel and Distributed . Performance considerations of shared virtual memory machines [SZ96] X.-H. Sun and J. P. Zhu.
. , 6(11):1185– 1194, 1996 Systems
. . Prentice-Hall, Englewood Cliffs, NJ, 1990 Multiprocessors [Tab90] D. Tabak.
. . McGraw-Hill, New York, NY, 1991 Advanced Multiprocessors [Tab91] D. Tabak.
35th Information Processing Society . In An optimal routing method of all-to-all communication on hypercube networks [Tak87] R. Take.
. , 1987 of Japan
Proceedings of the . In Simultaneous multithreading: Maximizing on-chip parallelism [TEL95] D. M. Tullsen, S. Eggers, and H. M. Levy.
, 1995. 22nd Annual International Symposium on Computer Architecture
, , 8:400–406 Journal of Parallel and Distributed Computing . Adaptive parallel algorithms for integral knapsack problems [Ten90] S. Teng.
. 1990
. . Cambridge, MA. 1990 The CM-2 Technical Summary [Thi90] Thinking Machines Corporation.
. . Cambridge, MA. 1991 The CM-5 Technical Summary [Thi91] Thinking Machines Corporation.
. , C-32(11):1047–1057, 1983 IBM Journal of Research and Development . Fourier transforms in VLSI [Tho83] C. D. Thompson.
. , 32(5):108–109, May 1999 Computer . Standards: OpenMP: Shared-memory parallelism from the ashes [Thr99] J. Throop.
r . Technical Report RIACS TR 88.41, Research Institute fo Parallel matrix multiplication on the connection machine [Tic88] W. F. Tichy.
Advanced Computer Science, NASA Ames Research Center, Moffet Field, CA, 1988.
, , 21:263–271 Communications of the ACM . Sorting on a mesh-connected parallel computer [TK77] C. D. Thompson and H. T. Kung.
. 1977
Proceedings of the 1990 International Conference on . In Optimal granularity of grid iteration problems [TL90] Z. Tang and G.-J. Li.
. , I111–I118, 1990 Parallel Processing
. . Ph.D. Thesis, University of Washington, Seattle, WA, 1996 Simultaneous multithreading [Tul96] D. M. Tullsen.
r , 14(4):862–874, Novembe SIAM Journal on Computing . An efficient parallel biconnectivity algorithm [TV85] R. E. Tarjan and U. Vishkin.
. 1985
The superthreaded architecture: Thread pipelining with run-time data dependence checking and control [TY96] J.-Y. Tsai and P.-C. Yew.
. , 35–46, 1996 Proceedings of the International Conference on Parallel Architectures and Compilation Techniques . In speculation
Proceedings of the 16th ACM . In A probabilistic relation between desirable and feasible models of parallel computation [Upf84] E. Upfal.
. , 258–265, 1984 Conference on Theory of Computing
Proceedings of the 25th Annual Symposium on the . In How to share memory in a distributed system [UW84] E. Upfal and A. Widgerson.
. , 171–180, 1984 Foundation of Computer Science
. , 4(3):348–355, September 1975 SIAM Journal of Computing . Parallelism in comparison problems [Val75] L. G. Valiant.
. , 11:350–361, 1982 SIAM Journal on Computing . A scheme for fast parallel communication [Val82] L. G. Valiant.
. , 33(8), 1990 Communications of the ACM . A bridging model for parallel computation [Val90a] L. G. Valiant.
. , 1990 Handbook of Theoretical Computer Science . General purpose parallel architectures [Val90b] L. G. Valiant.
. , 2:413–423, 1989 SIAM Journal on Discrete Mathematics . Gaussian elimination with pivoting is P-complete [Vav89] S. A. Vavasis.
Proceedings of the 13th ACM Symposium on . In Universal schemes for parallel communication [VB81] L. G. Valiant and G. J. Brebner.
. , 263–277, 1981 Theory of Computation
International Journal . Towards a general model for evaluating the relative performance of computer systems [VC89] F. A. Van-Catledge.
. , 3(2):100–108, 1989 of Supercomputer Applications
, . Technical Report 29, University of Paderborn, Germany Implementing branch-and-bound in a ring of processors [Vor86] O. Vornberger.
1986.
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Proceedings of the 1987 International Conference on . In The personal supercomputer: A network of transputers [Vor87a] O. Vornberger.
, 1987. Supercomputing
Proceedings of the Second International Workshop on Distributed . In Load balancing in a network of transputers [Vor87b] O. Vornberger.
, 1987. Parallel Algorithms
. New classes for parallel complexity: A study of unification and other complete problems for P [VS86] J. S. Vitter and R. A. Simmons.
, May 1986. IEEE Transactions on Computers
SIAM . Fast parallel computation of polynomials using few processors [VSBR83] L. G. Valiant, S. Skyum, S. Berkowitz, and C. Rackoff.
. , 12(4):641–644, 1983 Journal of Computing
Parallel Programming: Techniques and Applications Using Networked Workstations and Parallel [WA98] B. Wilkinson and C. M. Allen.
. . Prentice Hall, 1998 Computers
. . Prentice-Hall, NJ, 1999 Parallel Programming [WA99] B. Wilkinson and M. Allen.
Proceedings of the Second Conference on Hypercube . In Hyperquicksort: A fast sorting algorithm for hypercubes [Wag87] B. A. Wagar.
. , 292– 299, 1987 Multiprocessors
. . Prentice-Hall, Englewood Cliffs, NJ, 1991 Parallel Processing and Ada [Wal91] Y. Wallach.
. , 765–777, 1972 Proceedings of AFIPS Conference . In r C.mmp—a multimicroprocesso [WB72] W. A. Wulf and C. G. Bell.
A . Technical Report TR-97-037, International Computer Science Institute, Berkeley, C Active threads manual [Wei97] B. Weissman.
. 94704, 1997
t , 669–702, Augus IEEE Transactions on Computers . On a class of multistage interconnection networks [WF80] C. L. Wu and T. Y. Feng.
. 1980
, . IEEE Computer Society Press Interconnection Networks for Parallel and Distributed Processing [WF84] C. L. Wu and T. Y. Feng.
. Washington, DC, 1984
Proceedings of International . In A distributed shortest path algorithm and its mapping on the Multi-PSI [WI89] K. Wada and N. Ichiyoshi.
, 1989. Conference of Parallel Processing
. . Prentice-Hall, NJ, 1986 Algorithms and Complexity [Wil86] H. S. Wilf.
. . MIT Press, Cambridge, MA, 1995 Practical Parallel Programming [Wil95] G. V. Wilson.
. . The Coriolis Group, 2000 Windows 2000 Systems Programming Black Book [Wil00] A. Williams.
, IEEE International Conference on Acoustics, Speech and Signal Processing . In A new method for computing DFT [Win77] S. Winograd.
. 366–368, 1977
, Circuits, Systems, and Signal Processing . Systolic processing for dynamic programming problems [WL88] B. W. Wah and G.-J. Li.
. 7(2):119–149, 1988
l The status of MANIP—a multicomputer architecture for solving combinatoria [WLY84] B. W. Wah, G.-J. Li, and C. F. Yu.
. , 56–63, 1984 Proceedings of 11th Annual International Symposium on Computer Architecture . In s extremum-search problem
IEEE . s MANIP—a multicomputer architecture for solving combinatorial extremum-search problem [WM84] B. W. Wah and Y. W. E. Ma.
. , C–33, May 1984 Transactions on Computers
. . North-Holland, Amsterdam, The Netherlands, 1986 Fifth Generation Computer Architectures [Woo86] J. V. Woods, editor.
, . Ph.D. Thesis, Stanford University Information Requirements and the Implications for Parallel Computation [Wor88] P. H. Worley.
. Department of Computer Science, Palo Alto, CA, 1988
, SIAM Journal on Scientific and Statistical Computing . The effect of time constraints on scaled speedup [Wor90] P. H. Worley.
. 11(5):838–858, 1990
SIAM Journal on Scientific and Statistical . Limits on parallelism in the numerical solution of linear PDEs [Wor91] P. H. Worley.
. , 12:1–35, January 1991 Computing
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
. , 2:435–448, 1988 Journal of Supercomputing . A balanced bin sort for hypercube multiprocessors [WS88] Y. Won and S. Sahni.
. , 3:209–234, 1989 Journal of Supercomputing . Hypercube computing: Connected components [WS89] J. Woo and S. Sahni.
, June 1991. Also Journal of Supercomputing . Computing biconnected components on a hypercube [WS91] J. Woo and S. Sahni.
. available as Technical Report TR 89-7 from the Department of Computer Science, University of Minnesota, Minneapolis, MN
IEEE Transactions on . Stochastic modeling of branch-and-bound algorithms with best-first search [WY85] B. W. Wah and C. F. Yu.
, SE-11, September 1985. Software Engineering
, , 32(8):1014–5 Communications of the ACM . Bridging the gap between Amdahl's law and Sandia laboratory's result [Zho89] X. Zhou.
. 1989
. . McGraw-Hill, 1996 Parallel and Distributed Computing Handbook [Zom96] A. Zomaya, editor.
Supercomputing '89 . In Measuring the scalability of parallel computer systems [ZRV89] J. R. Zorbas, D. J. Reble, and R. E. VanKooten.
. , 832–841, 1989 Proceedings
[ Team LiB ]
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks
Brought to You by
Like the book? Buy it!
. This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks